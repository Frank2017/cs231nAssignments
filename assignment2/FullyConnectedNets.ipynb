{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as we move to bigger models. Ideally we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures.\n",
    "\n",
    "In this exercise we will implement fully-connected networks using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "\n",
    "In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch Normalization as a tool to more efficiently optimize deep networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: foward\n",
    "Open the file `cs231n/layers.py` and implement the `affine_forward` function.\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76985004799e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "# print 'x',x\n",
    "# print 'w',w\n",
    "# print 'b',b\n",
    "out, _ = affine_forward(x, w, b)\n",
    "# print cache[0].shape\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print 'Testing affine_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  4.10540563894e-10\n",
      "dw error:  1.31744415149e-10\n",
      "db error:  8.34154426541e-12\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print 'Testing affine_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print 'Testing relu_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27559925923e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print 'Testing relu_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `cs231n/layer_utils.py`.\n",
    "\n",
    "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_relu_forward:\n",
      "dx error:  5.65292290521e-10\n",
      "dw error:  5.44961999572e-10\n",
      "db error:  2.54799933861e-11\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "print 'Testing affine_relu_forward:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layers: Softmax and SVM\n",
    "You implemented these loss functions in the last assignment, so we'll give them to you for free here. You should still make sure you understand how they work by looking at the implementations in `cs231n/layers.py`.\n",
    "\n",
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing svm_loss:\n",
      "loss:  8.99915790428\n",
      "dx error:  8.18289447289e-10\n",
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.30250133799\n",
      "dx error:  9.94193846257e-09\n"
     ]
    }
   ],
   "source": [
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be 1e-9\n",
    "print 'Testing svm_loss:'\n",
    "print 'loss: ', loss\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
    "print '\\nTesting softmax_loss:'\n",
    "print 'loss: ', loss\n",
    "print 'dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer network\n",
    "In the previous assignment you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
    "\n",
    "Open the file `cs231n/classifiers/fc_net.py` and complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check with reg =  0.0\n",
      "W1 relative error: 1.22e-08\n",
      "W2 relative error: 3.34e-10\n",
      "b1 relative error: 4.73e-09\n",
      "b2 relative error: 4.33e-10\n",
      "Running numeric gradient check with reg =  0.7\n",
      "W1 relative error: 2.53e-07\n",
      "W2 relative error: 1.37e-07\n",
      "b1 relative error: 1.56e-08\n",
      "b2 relative error: 9.09e-10\n"
     ]
    }
   ],
   "source": [
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-2\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print 'Testing initialization ... '\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print 'Testing test-time forward pass ... '\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print 'Testing training loss (no regularization)'\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "for reg in [0.0, 0.7]:\n",
    "  print 'Running numeric gradient check with reg = ', reg\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print '%s relative error: %.2e' % (name, rel_error(grad_num, grads[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n",
    "\n",
    "Open the file `cs231n/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 th try:\n",
      "learning rate is  0.0392933105819\n",
      "regularization strengths is  0.149665965627\n",
      "weight scale is  0.012394573039\n",
      "hidden size is  107\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 8.497434\n",
      "(Epoch 0 / 10) train acc: 0.162000; val_acc: 0.147000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/layers.py:593: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 101 / 4900) loss: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/layers.py:590: RuntimeWarning: overflow encountered in subtract\n",
      "cs231n/layers.py:590: RuntimeWarning: invalid value encountered in subtract\n",
      "cs231n/layers.py:117: RuntimeWarning: invalid value encountered in greater\n",
      "  dx = dout * (cache > 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.089000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.122000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "validataion accuracy  0.147\n",
      "\n",
      "1 th try:\n",
      "learning rate is  1.45458729931e-05\n",
      "regularization strengths is  0.0384029804967\n",
      "weight scale is  0.00378258957834\n",
      "hidden size is  298\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.819468\n",
      "(Epoch 0 / 10) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 101 / 4900) loss: 2.409301\n",
      "(Iteration 201 / 4900) loss: 2.352570\n",
      "(Iteration 301 / 4900) loss: 2.134109\n",
      "(Iteration 401 / 4900) loss: 2.177619\n",
      "(Epoch 1 / 10) train acc: 0.287000; val_acc: 0.302000\n",
      "(Iteration 501 / 4900) loss: 2.222513\n",
      "(Iteration 601 / 4900) loss: 2.085622\n",
      "(Iteration 701 / 4900) loss: 2.210913\n",
      "(Iteration 801 / 4900) loss: 2.132147\n",
      "(Iteration 901 / 4900) loss: 2.199756\n",
      "(Epoch 2 / 10) train acc: 0.336000; val_acc: 0.330000\n",
      "(Iteration 1001 / 4900) loss: 2.159534\n",
      "(Iteration 1101 / 4900) loss: 1.939062\n",
      "(Iteration 1201 / 4900) loss: 1.985746\n",
      "(Iteration 1301 / 4900) loss: 2.128655\n",
      "(Iteration 1401 / 4900) loss: 2.166066\n",
      "(Epoch 3 / 10) train acc: 0.358000; val_acc: 0.360000\n",
      "(Iteration 1501 / 4900) loss: 1.993949\n",
      "(Iteration 1601 / 4900) loss: 2.079192\n",
      "(Iteration 1701 / 4900) loss: 2.144341\n",
      "(Iteration 1801 / 4900) loss: 2.075083\n",
      "(Iteration 1901 / 4900) loss: 1.988919\n",
      "(Epoch 4 / 10) train acc: 0.365000; val_acc: 0.372000\n",
      "(Iteration 2001 / 4900) loss: 1.912384\n",
      "(Iteration 2101 / 4900) loss: 1.955039\n",
      "(Iteration 2201 / 4900) loss: 1.919053\n",
      "(Iteration 2301 / 4900) loss: 2.167778\n",
      "(Iteration 2401 / 4900) loss: 2.082689\n",
      "(Epoch 5 / 10) train acc: 0.386000; val_acc: 0.380000\n",
      "(Iteration 2501 / 4900) loss: 2.110938\n",
      "(Iteration 2601 / 4900) loss: 2.112466\n",
      "(Iteration 2701 / 4900) loss: 1.982226\n",
      "(Iteration 2801 / 4900) loss: 1.911972\n",
      "(Iteration 2901 / 4900) loss: 2.076805\n",
      "(Epoch 6 / 10) train acc: 0.382000; val_acc: 0.389000\n",
      "(Iteration 3001 / 4900) loss: 1.882079\n",
      "(Iteration 3101 / 4900) loss: 2.137529\n",
      "(Iteration 3201 / 4900) loss: 1.951711\n",
      "(Iteration 3301 / 4900) loss: 2.051460\n",
      "(Iteration 3401 / 4900) loss: 1.951925\n",
      "(Epoch 7 / 10) train acc: 0.407000; val_acc: 0.388000\n",
      "(Iteration 3501 / 4900) loss: 2.136153\n",
      "(Iteration 3601 / 4900) loss: 1.893594\n",
      "(Iteration 3701 / 4900) loss: 1.886078\n",
      "(Iteration 3801 / 4900) loss: 1.912595\n",
      "(Iteration 3901 / 4900) loss: 2.005738\n",
      "(Epoch 8 / 10) train acc: 0.401000; val_acc: 0.393000\n",
      "(Iteration 4001 / 4900) loss: 1.918731\n",
      "(Iteration 4101 / 4900) loss: 2.093103\n",
      "(Iteration 4201 / 4900) loss: 1.983851\n",
      "(Iteration 4301 / 4900) loss: 2.101046\n",
      "(Iteration 4401 / 4900) loss: 1.815263\n",
      "(Epoch 9 / 10) train acc: 0.426000; val_acc: 0.400000\n",
      "(Iteration 4501 / 4900) loss: 1.975818\n",
      "(Iteration 4601 / 4900) loss: 1.926760\n",
      "(Iteration 4701 / 4900) loss: 2.044153\n",
      "(Iteration 4801 / 4900) loss: 1.929980\n",
      "(Epoch 10 / 10) train acc: 0.397000; val_acc: 0.409000\n",
      "validataion accuracy  0.409\n",
      "\n",
      "2 th try:\n",
      "learning rate is  3.91492623592e-05\n",
      "regularization strengths is  0.016638695787\n",
      "weight scale is  0.0864857140571\n",
      "hidden size is  73\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: inf\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.142000\n",
      "(Iteration 101 / 4900) loss: 117.331054\n",
      "(Iteration 201 / 4900) loss: 87.708546\n",
      "(Iteration 301 / 4900) loss: 74.698059\n",
      "(Iteration 401 / 4900) loss: 59.691188\n",
      "(Epoch 1 / 10) train acc: 0.218000; val_acc: 0.228000\n",
      "(Iteration 501 / 4900) loss: 68.607190\n",
      "(Iteration 601 / 4900) loss: 61.785909\n",
      "(Iteration 701 / 4900) loss: 55.621992\n",
      "(Iteration 801 / 4900) loss: 43.548225\n",
      "(Iteration 901 / 4900) loss: 39.185854\n",
      "(Epoch 2 / 10) train acc: 0.276000; val_acc: 0.235000\n",
      "(Iteration 1001 / 4900) loss: 35.909090\n",
      "(Iteration 1101 / 4900) loss: 33.988195\n",
      "(Iteration 1201 / 4900) loss: 30.680988\n",
      "(Iteration 1301 / 4900) loss: 26.418076\n",
      "(Iteration 1401 / 4900) loss: 24.911152\n",
      "(Epoch 3 / 10) train acc: 0.293000; val_acc: 0.241000\n",
      "(Iteration 1501 / 4900) loss: 22.058566\n",
      "(Iteration 1601 / 4900) loss: 20.886499\n",
      "(Iteration 1701 / 4900) loss: 20.488322\n",
      "(Iteration 1801 / 4900) loss: 18.586339\n",
      "(Iteration 1901 / 4900) loss: 17.499020\n",
      "(Epoch 4 / 10) train acc: 0.233000; val_acc: 0.222000\n",
      "(Iteration 2001 / 4900) loss: 17.832639\n",
      "(Iteration 2101 / 4900) loss: 16.951240\n",
      "(Iteration 2201 / 4900) loss: 17.550781\n",
      "(Iteration 2301 / 4900) loss: 17.171686\n",
      "(Iteration 2401 / 4900) loss: 17.169552\n",
      "(Epoch 5 / 10) train acc: 0.227000; val_acc: 0.232000\n",
      "(Iteration 2501 / 4900) loss: 16.749070\n",
      "(Iteration 2601 / 4900) loss: 16.702242\n",
      "(Iteration 2701 / 4900) loss: 17.139347\n",
      "(Iteration 2801 / 4900) loss: 17.122828\n",
      "(Iteration 2901 / 4900) loss: 17.130032\n",
      "(Epoch 6 / 10) train acc: 0.211000; val_acc: 0.212000\n",
      "(Iteration 3001 / 4900) loss: 16.628735\n",
      "(Iteration 3101 / 4900) loss: 16.490931\n",
      "(Iteration 3201 / 4900) loss: 16.772262\n",
      "(Iteration 3301 / 4900) loss: 16.016777\n",
      "(Iteration 3401 / 4900) loss: 16.402892\n",
      "(Epoch 7 / 10) train acc: 0.245000; val_acc: 0.248000\n",
      "(Iteration 3501 / 4900) loss: 16.105441\n",
      "(Iteration 3601 / 4900) loss: 16.573060\n",
      "(Iteration 3701 / 4900) loss: 16.020089\n",
      "(Iteration 3801 / 4900) loss: 16.283354\n",
      "(Iteration 3901 / 4900) loss: 16.696817\n",
      "(Epoch 8 / 10) train acc: 0.249000; val_acc: 0.258000\n",
      "(Iteration 4001 / 4900) loss: 16.009558\n",
      "(Iteration 4101 / 4900) loss: 15.930215\n",
      "(Iteration 4201 / 4900) loss: 15.884484\n",
      "(Iteration 4301 / 4900) loss: 15.697058\n",
      "(Iteration 4401 / 4900) loss: 16.139086\n",
      "(Epoch 9 / 10) train acc: 0.239000; val_acc: 0.254000\n",
      "(Iteration 4501 / 4900) loss: 16.461056\n",
      "(Iteration 4601 / 4900) loss: 15.821308\n",
      "(Iteration 4701 / 4900) loss: 15.811169\n",
      "(Iteration 4801 / 4900) loss: 16.335025\n",
      "(Epoch 10 / 10) train acc: 0.269000; val_acc: 0.268000\n",
      "validataion accuracy  0.268\n",
      "\n",
      "3 th try:\n",
      "learning rate is  0.00225705657124\n",
      "regularization strengths is  0.00108363449928\n",
      "weight scale is  0.00221488384014\n",
      "hidden size is  250\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.350458\n",
      "(Epoch 0 / 10) train acc: 0.174000; val_acc: 0.193000\n",
      "(Iteration 101 / 4900) loss: 1.769416\n",
      "(Iteration 201 / 4900) loss: 1.531792\n",
      "(Iteration 301 / 4900) loss: 1.962423\n",
      "(Iteration 401 / 4900) loss: 1.871174\n",
      "(Epoch 1 / 10) train acc: 0.448000; val_acc: 0.449000\n",
      "(Iteration 501 / 4900) loss: 1.770383\n",
      "(Iteration 601 / 4900) loss: 2.584829\n",
      "(Iteration 701 / 4900) loss: 1.535029\n",
      "(Iteration 801 / 4900) loss: 2.093955\n",
      "(Iteration 901 / 4900) loss: 1.640059\n",
      "(Epoch 2 / 10) train acc: 0.467000; val_acc: 0.430000\n",
      "(Iteration 1001 / 4900) loss: 1.617566\n",
      "(Iteration 1101 / 4900) loss: 2.092653\n",
      "(Iteration 1201 / 4900) loss: 1.687134\n",
      "(Iteration 1301 / 4900) loss: 1.641043\n",
      "(Iteration 1401 / 4900) loss: 1.640863\n",
      "(Epoch 3 / 10) train acc: 0.423000; val_acc: 0.424000\n",
      "(Iteration 1501 / 4900) loss: 1.990552\n",
      "(Iteration 1601 / 4900) loss: 2.216241\n",
      "(Iteration 1701 / 4900) loss: 1.909368\n",
      "(Iteration 1801 / 4900) loss: 1.821911\n",
      "(Iteration 1901 / 4900) loss: 1.673021\n",
      "(Epoch 4 / 10) train acc: 0.521000; val_acc: 0.483000\n",
      "(Iteration 2001 / 4900) loss: 1.371848\n",
      "(Iteration 2101 / 4900) loss: 1.302765\n",
      "(Iteration 2201 / 4900) loss: 1.672175\n",
      "(Iteration 2301 / 4900) loss: 1.752568\n",
      "(Iteration 2401 / 4900) loss: 1.863824\n",
      "(Epoch 5 / 10) train acc: 0.483000; val_acc: 0.419000\n",
      "(Iteration 2501 / 4900) loss: 1.666777\n",
      "(Iteration 2601 / 4900) loss: 1.357458\n",
      "(Iteration 2701 / 4900) loss: 1.756363\n",
      "(Iteration 2801 / 4900) loss: 1.455330\n",
      "(Iteration 2901 / 4900) loss: 1.877031\n",
      "(Epoch 6 / 10) train acc: 0.488000; val_acc: 0.410000\n",
      "(Iteration 3001 / 4900) loss: 1.147033\n",
      "(Iteration 3101 / 4900) loss: 1.704442\n",
      "(Iteration 3201 / 4900) loss: 1.202712\n",
      "(Iteration 3301 / 4900) loss: 1.945463\n",
      "(Iteration 3401 / 4900) loss: 1.398552\n",
      "(Epoch 7 / 10) train acc: 0.534000; val_acc: 0.492000\n",
      "(Iteration 3501 / 4900) loss: 1.388186\n",
      "(Iteration 3601 / 4900) loss: 1.247529\n",
      "(Iteration 3701 / 4900) loss: 1.597907\n",
      "(Iteration 3801 / 4900) loss: 1.879245\n",
      "(Iteration 3901 / 4900) loss: 1.641584\n",
      "(Epoch 8 / 10) train acc: 0.494000; val_acc: 0.435000\n",
      "(Iteration 4001 / 4900) loss: 1.383589\n",
      "(Iteration 4101 / 4900) loss: 1.326232\n",
      "(Iteration 4201 / 4900) loss: 1.599640\n",
      "(Iteration 4301 / 4900) loss: 1.427810\n",
      "(Iteration 4401 / 4900) loss: 1.337302\n",
      "(Epoch 9 / 10) train acc: 0.588000; val_acc: 0.484000\n",
      "(Iteration 4501 / 4900) loss: 1.508433\n",
      "(Iteration 4601 / 4900) loss: 1.322909\n",
      "(Iteration 4701 / 4900) loss: 1.300371\n",
      "(Iteration 4801 / 4900) loss: 1.398220\n",
      "(Epoch 10 / 10) train acc: 0.581000; val_acc: 0.466000\n",
      "validataion accuracy  0.492\n",
      "\n",
      "4 th try:\n",
      "learning rate is  3.3856788237e-05\n",
      "regularization strengths is  0.250572658124\n",
      "weight scale is  0.013894471251\n",
      "hidden size is  55\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 8.767210\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.097000\n",
      "(Iteration 101 / 4900) loss: 7.248890\n",
      "(Iteration 201 / 4900) loss: 6.845991\n",
      "(Iteration 301 / 4900) loss: 6.781729\n",
      "(Iteration 401 / 4900) loss: 6.625119\n",
      "(Epoch 1 / 10) train acc: 0.241000; val_acc: 0.252000\n",
      "(Iteration 501 / 4900) loss: 6.403872\n",
      "(Iteration 601 / 4900) loss: 6.250963\n",
      "(Iteration 701 / 4900) loss: 6.147799\n",
      "(Iteration 801 / 4900) loss: 6.070744\n",
      "(Iteration 901 / 4900) loss: 5.868317\n",
      "(Epoch 2 / 10) train acc: 0.322000; val_acc: 0.307000\n",
      "(Iteration 1001 / 4900) loss: 5.989058\n",
      "(Iteration 1101 / 4900) loss: 6.098567\n",
      "(Iteration 1201 / 4900) loss: 6.059340\n",
      "(Iteration 1301 / 4900) loss: 5.948571\n",
      "(Iteration 1401 / 4900) loss: 5.837000\n",
      "(Epoch 3 / 10) train acc: 0.352000; val_acc: 0.330000\n",
      "(Iteration 1501 / 4900) loss: 5.633695\n",
      "(Iteration 1601 / 4900) loss: 5.777250\n",
      "(Iteration 1701 / 4900) loss: 5.941166\n",
      "(Iteration 1801 / 4900) loss: 5.944372\n",
      "(Iteration 1901 / 4900) loss: 5.909305\n",
      "(Epoch 4 / 10) train acc: 0.364000; val_acc: 0.331000\n",
      "(Iteration 2001 / 4900) loss: 5.792320\n",
      "(Iteration 2101 / 4900) loss: 5.682434\n",
      "(Iteration 2201 / 4900) loss: 5.776804\n",
      "(Iteration 2301 / 4900) loss: 5.838396\n",
      "(Iteration 2401 / 4900) loss: 5.562143\n",
      "(Epoch 5 / 10) train acc: 0.334000; val_acc: 0.344000\n",
      "(Iteration 2501 / 4900) loss: 5.600130\n",
      "(Iteration 2601 / 4900) loss: 5.825621\n",
      "(Iteration 2701 / 4900) loss: 5.677201\n",
      "(Iteration 2801 / 4900) loss: 5.579660\n",
      "(Iteration 2901 / 4900) loss: 5.758939\n",
      "(Epoch 6 / 10) train acc: 0.380000; val_acc: 0.355000\n",
      "(Iteration 3001 / 4900) loss: 5.662293\n",
      "(Iteration 3101 / 4900) loss: 5.793433\n",
      "(Iteration 3201 / 4900) loss: 5.712893\n",
      "(Iteration 3301 / 4900) loss: 5.633942\n",
      "(Iteration 3401 / 4900) loss: 5.638758\n",
      "(Epoch 7 / 10) train acc: 0.380000; val_acc: 0.366000\n",
      "(Iteration 3501 / 4900) loss: 5.471045\n",
      "(Iteration 3601 / 4900) loss: 5.749576\n",
      "(Iteration 3701 / 4900) loss: 5.458980\n",
      "(Iteration 3801 / 4900) loss: 5.890822\n",
      "(Iteration 3901 / 4900) loss: 5.632479\n",
      "(Epoch 8 / 10) train acc: 0.373000; val_acc: 0.363000\n",
      "(Iteration 4001 / 4900) loss: 5.513119\n",
      "(Iteration 4101 / 4900) loss: 5.642413\n",
      "(Iteration 4201 / 4900) loss: 5.596210\n",
      "(Iteration 4301 / 4900) loss: 5.688317\n",
      "(Iteration 4401 / 4900) loss: 5.498178\n",
      "(Epoch 9 / 10) train acc: 0.374000; val_acc: 0.368000\n",
      "(Iteration 4501 / 4900) loss: 5.667592\n",
      "(Iteration 4601 / 4900) loss: 5.655102\n",
      "(Iteration 4701 / 4900) loss: 5.656054\n",
      "(Iteration 4801 / 4900) loss: 5.727850\n",
      "(Epoch 10 / 10) train acc: 0.345000; val_acc: 0.372000\n",
      "validataion accuracy  0.372\n",
      "\n",
      "5 th try:\n",
      "learning rate is  0.0419150102144\n",
      "regularization strengths is  0.00159621941648\n",
      "weight scale is  0.00965430283057\n",
      "hidden size is  177\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 5.415696\n",
      "(Epoch 0 / 10) train acc: 0.115000; val_acc: 0.121000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.088000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.101000; val_acc: 0.087000\n",
      "validataion accuracy  0.121\n",
      "\n",
      "6 th try:\n",
      "learning rate is  9.67631407161e-06\n",
      "regularization strengths is  0.0104538112579\n",
      "weight scale is  0.0970071237195\n",
      "hidden size is  88\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: inf\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.083000\n",
      "(Iteration 101 / 4900) loss: 207.063618\n",
      "(Iteration 201 / 4900) loss: 181.376304\n",
      "(Iteration 301 / 4900) loss: 161.089531\n",
      "(Iteration 401 / 4900) loss: 164.564360\n",
      "(Epoch 1 / 10) train acc: 0.206000; val_acc: 0.194000\n",
      "(Iteration 501 / 4900) loss: 146.392106\n",
      "(Iteration 601 / 4900) loss: 142.716342\n",
      "(Iteration 701 / 4900) loss: 131.623917\n",
      "(Iteration 801 / 4900) loss: 129.029920\n",
      "(Iteration 901 / 4900) loss: 124.666696\n",
      "(Epoch 2 / 10) train acc: 0.202000; val_acc: 0.221000\n",
      "(Iteration 1001 / 4900) loss: 106.695897\n",
      "(Iteration 1101 / 4900) loss: 113.430451\n",
      "(Iteration 1201 / 4900) loss: 128.415356\n",
      "(Iteration 1301 / 4900) loss: 110.906697\n",
      "(Iteration 1401 / 4900) loss: 97.422521\n",
      "(Epoch 3 / 10) train acc: 0.241000; val_acc: 0.235000\n",
      "(Iteration 1501 / 4900) loss: 96.994911\n",
      "(Iteration 1601 / 4900) loss: 99.088591\n",
      "(Iteration 1701 / 4900) loss: 110.580036\n",
      "(Iteration 1801 / 4900) loss: 94.063662\n",
      "(Iteration 1901 / 4900) loss: 79.608179\n",
      "(Epoch 4 / 10) train acc: 0.217000; val_acc: 0.253000\n",
      "(Iteration 2001 / 4900) loss: 98.364891\n",
      "(Iteration 2101 / 4900) loss: 90.381752\n",
      "(Iteration 2201 / 4900) loss: 81.779614\n",
      "(Iteration 2301 / 4900) loss: 77.296554\n",
      "(Iteration 2401 / 4900) loss: 82.323215\n",
      "(Epoch 5 / 10) train acc: 0.248000; val_acc: 0.251000\n",
      "(Iteration 2501 / 4900) loss: 73.055788\n",
      "(Iteration 2601 / 4900) loss: 81.222125\n",
      "(Iteration 2701 / 4900) loss: 80.959511\n",
      "(Iteration 2801 / 4900) loss: 69.668277\n",
      "(Iteration 2901 / 4900) loss: 68.404494\n",
      "(Epoch 6 / 10) train acc: 0.246000; val_acc: 0.253000\n",
      "(Iteration 3001 / 4900) loss: 71.262578\n",
      "(Iteration 3101 / 4900) loss: 57.560162\n",
      "(Iteration 3201 / 4900) loss: 82.305488\n",
      "(Iteration 3301 / 4900) loss: 70.367808\n",
      "(Iteration 3401 / 4900) loss: 76.033434\n",
      "(Epoch 7 / 10) train acc: 0.249000; val_acc: 0.261000\n",
      "(Iteration 3501 / 4900) loss: 74.512136\n",
      "(Iteration 3601 / 4900) loss: 70.641472\n",
      "(Iteration 3701 / 4900) loss: 74.310723\n",
      "(Iteration 3801 / 4900) loss: 68.029747\n",
      "(Iteration 3901 / 4900) loss: 70.516668\n",
      "(Epoch 8 / 10) train acc: 0.253000; val_acc: 0.262000\n",
      "(Iteration 4001 / 4900) loss: 71.716338\n",
      "(Iteration 4101 / 4900) loss: 56.343813\n",
      "(Iteration 4201 / 4900) loss: 53.378908\n",
      "(Iteration 4301 / 4900) loss: 61.997726\n",
      "(Iteration 4401 / 4900) loss: 57.316585\n",
      "(Epoch 9 / 10) train acc: 0.275000; val_acc: 0.264000\n",
      "(Iteration 4501 / 4900) loss: 54.408119\n",
      "(Iteration 4601 / 4900) loss: 65.017487\n",
      "(Iteration 4701 / 4900) loss: 49.557260\n",
      "(Iteration 4801 / 4900) loss: 46.469815\n",
      "(Epoch 10 / 10) train acc: 0.249000; val_acc: 0.260000\n",
      "validataion accuracy  0.264\n",
      "\n",
      "7 th try:\n",
      "learning rate is  2.05725326426e-06\n",
      "regularization strengths is  0.00226569209138\n",
      "weight scale is  0.0216466755746\n",
      "hidden size is  143\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 22.480969\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.074000\n",
      "(Iteration 101 / 4900) loss: 17.748116\n",
      "(Iteration 201 / 4900) loss: 14.689215\n",
      "(Iteration 301 / 4900) loss: 12.290928\n",
      "(Iteration 401 / 4900) loss: 11.586812\n",
      "(Epoch 1 / 10) train acc: 0.158000; val_acc: 0.130000\n",
      "(Iteration 501 / 4900) loss: 12.068141\n",
      "(Iteration 601 / 4900) loss: 10.259041\n",
      "(Iteration 701 / 4900) loss: 12.044465\n",
      "(Iteration 801 / 4900) loss: 12.001934\n",
      "(Iteration 901 / 4900) loss: 11.703581\n",
      "(Epoch 2 / 10) train acc: 0.192000; val_acc: 0.153000\n",
      "(Iteration 1001 / 4900) loss: 10.451445\n",
      "(Iteration 1101 / 4900) loss: 10.596725\n",
      "(Iteration 1201 / 4900) loss: 10.078710\n",
      "(Iteration 1301 / 4900) loss: 8.513340\n",
      "(Iteration 1401 / 4900) loss: 9.231268\n",
      "(Epoch 3 / 10) train acc: 0.181000; val_acc: 0.168000\n",
      "(Iteration 1501 / 4900) loss: 9.482863\n",
      "(Iteration 1601 / 4900) loss: 9.687570\n",
      "(Iteration 1701 / 4900) loss: 9.044937\n",
      "(Iteration 1801 / 4900) loss: 8.252351\n",
      "(Iteration 1901 / 4900) loss: 10.338596\n",
      "(Epoch 4 / 10) train acc: 0.197000; val_acc: 0.188000\n",
      "(Iteration 2001 / 4900) loss: 9.249672\n",
      "(Iteration 2101 / 4900) loss: 9.229508\n",
      "(Iteration 2201 / 4900) loss: 7.908254\n",
      "(Iteration 2301 / 4900) loss: 10.017269\n",
      "(Iteration 2401 / 4900) loss: 8.882202\n",
      "(Epoch 5 / 10) train acc: 0.181000; val_acc: 0.192000\n",
      "(Iteration 2501 / 4900) loss: 9.692164\n",
      "(Iteration 2601 / 4900) loss: 6.977857\n",
      "(Iteration 2701 / 4900) loss: 7.744875\n",
      "(Iteration 2801 / 4900) loss: 8.195830\n",
      "(Iteration 2901 / 4900) loss: 9.187141\n",
      "(Epoch 6 / 10) train acc: 0.187000; val_acc: 0.195000\n",
      "(Iteration 3001 / 4900) loss: 7.139399\n",
      "(Iteration 3101 / 4900) loss: 8.541596\n",
      "(Iteration 3201 / 4900) loss: 8.542843\n",
      "(Iteration 3301 / 4900) loss: 9.633896\n",
      "(Iteration 3401 / 4900) loss: 8.324645\n",
      "(Epoch 7 / 10) train acc: 0.214000; val_acc: 0.204000\n",
      "(Iteration 3501 / 4900) loss: 7.588366\n",
      "(Iteration 3601 / 4900) loss: 7.587526\n",
      "(Iteration 3701 / 4900) loss: 8.406121\n",
      "(Iteration 3801 / 4900) loss: 6.719433\n",
      "(Iteration 3901 / 4900) loss: 8.470664\n",
      "(Epoch 8 / 10) train acc: 0.216000; val_acc: 0.218000\n",
      "(Iteration 4001 / 4900) loss: 8.156260\n",
      "(Iteration 4101 / 4900) loss: 6.529121\n",
      "(Iteration 4201 / 4900) loss: 7.735936\n",
      "(Iteration 4301 / 4900) loss: 7.779884\n",
      "(Iteration 4401 / 4900) loss: 8.317743\n",
      "(Epoch 9 / 10) train acc: 0.198000; val_acc: 0.224000\n",
      "(Iteration 4501 / 4900) loss: 7.390630\n",
      "(Iteration 4601 / 4900) loss: 7.738427\n",
      "(Iteration 4701 / 4900) loss: 6.354022\n",
      "(Iteration 4801 / 4900) loss: 6.670303\n",
      "(Epoch 10 / 10) train acc: 0.222000; val_acc: 0.223000\n",
      "validataion accuracy  0.224\n",
      "\n",
      "8 th try:\n",
      "learning rate is  0.00724978333904\n",
      "regularization strengths is  0.00180929766269\n",
      "weight scale is  0.0333176860741\n",
      "hidden size is  97\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 39.876585\n",
      "(Epoch 0 / 10) train acc: 0.148000; val_acc: 0.146000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.088000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "validataion accuracy  0.146\n",
      "\n",
      "9 th try:\n",
      "learning rate is  0.00427102526243\n",
      "regularization strengths is  0.00165272969837\n",
      "weight scale is  0.0419908566284\n",
      "hidden size is  111\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 61.060494\n",
      "(Epoch 0 / 10) train acc: 0.169000; val_acc: 0.158000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.104000; val_acc: 0.110000\n",
      "(Iteration 501 / 4900) loss: inf\n",
      "(Iteration 601 / 4900) loss: inf\n",
      "(Iteration 701 / 4900) loss: inf\n",
      "(Iteration 801 / 4900) loss: inf\n",
      "(Iteration 901 / 4900) loss: inf\n",
      "(Epoch 2 / 10) train acc: 0.100000; val_acc: 0.092000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.089000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.130000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.093000; val_acc: 0.087000\n",
      "validataion accuracy  0.158\n",
      "\n",
      "10 th try:\n",
      "learning rate is  0.000416886326786\n",
      "regularization strengths is  0.0049285923384\n",
      "weight scale is  0.0627745284235\n",
      "hidden size is  150\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 181.751626\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.127000\n",
      "(Iteration 101 / 4900) loss: 42.538042\n",
      "(Iteration 201 / 4900) loss: 28.168844\n",
      "(Iteration 301 / 4900) loss: 29.870429\n",
      "(Iteration 401 / 4900) loss: 32.137044\n",
      "(Epoch 1 / 10) train acc: 0.266000; val_acc: 0.264000\n",
      "(Iteration 501 / 4900) loss: 17.561176\n",
      "(Iteration 601 / 4900) loss: 24.464294\n",
      "(Iteration 701 / 4900) loss: 19.237175\n",
      "(Iteration 801 / 4900) loss: 21.993128\n",
      "(Iteration 901 / 4900) loss: 20.792311\n",
      "(Epoch 2 / 10) train acc: 0.263000; val_acc: 0.277000\n",
      "(Iteration 1001 / 4900) loss: 13.338611\n",
      "(Iteration 1101 / 4900) loss: 24.981486\n",
      "(Iteration 1201 / 4900) loss: 17.834962\n",
      "(Iteration 1301 / 4900) loss: 18.401776\n",
      "(Iteration 1401 / 4900) loss: 13.467617\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.262000\n",
      "(Iteration 1501 / 4900) loss: 19.267116\n",
      "(Iteration 1601 / 4900) loss: 19.181691\n",
      "(Iteration 1701 / 4900) loss: 18.789953\n",
      "(Iteration 1801 / 4900) loss: 11.442109\n",
      "(Iteration 1901 / 4900) loss: 19.457762\n",
      "(Epoch 4 / 10) train acc: 0.282000; val_acc: 0.279000\n",
      "(Iteration 2001 / 4900) loss: 12.121798\n",
      "(Iteration 2101 / 4900) loss: 13.817175\n",
      "(Iteration 2201 / 4900) loss: 12.079222\n",
      "(Iteration 2301 / 4900) loss: 18.110146\n",
      "(Iteration 2401 / 4900) loss: 14.388564\n",
      "(Epoch 5 / 10) train acc: 0.243000; val_acc: 0.278000\n",
      "(Iteration 2501 / 4900) loss: 12.405200\n",
      "(Iteration 2601 / 4900) loss: 11.681683\n",
      "(Iteration 2701 / 4900) loss: 8.928104\n",
      "(Iteration 2801 / 4900) loss: 10.721491\n",
      "(Iteration 2901 / 4900) loss: 13.259029\n",
      "(Epoch 6 / 10) train acc: 0.224000; val_acc: 0.215000\n",
      "(Iteration 3001 / 4900) loss: 10.514596\n",
      "(Iteration 3101 / 4900) loss: 16.703408\n",
      "(Iteration 3201 / 4900) loss: 12.190635\n",
      "(Iteration 3301 / 4900) loss: 13.311901\n",
      "(Iteration 3401 / 4900) loss: 14.610516\n",
      "(Epoch 7 / 10) train acc: 0.318000; val_acc: 0.325000\n",
      "(Iteration 3501 / 4900) loss: 12.740979\n",
      "(Iteration 3601 / 4900) loss: 10.564159\n",
      "(Iteration 3701 / 4900) loss: 12.198887\n",
      "(Iteration 3801 / 4900) loss: 9.599832\n",
      "(Iteration 3901 / 4900) loss: 10.641855\n",
      "(Epoch 8 / 10) train acc: 0.311000; val_acc: 0.314000\n",
      "(Iteration 4001 / 4900) loss: 8.071399\n",
      "(Iteration 4101 / 4900) loss: 10.218645\n",
      "(Iteration 4201 / 4900) loss: 7.240735\n",
      "(Iteration 4301 / 4900) loss: 9.876970\n",
      "(Iteration 4401 / 4900) loss: 13.717028\n",
      "(Epoch 9 / 10) train acc: 0.252000; val_acc: 0.281000\n",
      "(Iteration 4501 / 4900) loss: 9.015898\n",
      "(Iteration 4601 / 4900) loss: 10.746890\n",
      "(Iteration 4701 / 4900) loss: 11.729786\n",
      "(Iteration 4801 / 4900) loss: 10.255484\n",
      "(Epoch 10 / 10) train acc: 0.251000; val_acc: 0.248000\n",
      "validataion accuracy  0.325\n",
      "\n",
      "11 th try:\n",
      "learning rate is  8.36117636768e-05\n",
      "regularization strengths is  0.00267621728662\n",
      "weight scale is  0.00181054244807\n",
      "hidden size is  61\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.301082\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.113000\n",
      "(Iteration 101 / 4900) loss: 2.245356\n",
      "(Iteration 201 / 4900) loss: 2.097445\n",
      "(Iteration 301 / 4900) loss: 1.960508\n",
      "(Iteration 401 / 4900) loss: 1.895431\n",
      "(Epoch 1 / 10) train acc: 0.307000; val_acc: 0.309000\n",
      "(Iteration 501 / 4900) loss: 1.919087\n",
      "(Iteration 601 / 4900) loss: 1.860544\n",
      "(Iteration 701 / 4900) loss: 1.826999\n",
      "(Iteration 801 / 4900) loss: 1.702126\n",
      "(Iteration 901 / 4900) loss: 1.765760\n",
      "(Epoch 2 / 10) train acc: 0.344000; val_acc: 0.359000\n",
      "(Iteration 1001 / 4900) loss: 1.843498\n",
      "(Iteration 1101 / 4900) loss: 1.832159\n",
      "(Iteration 1201 / 4900) loss: 1.726609\n",
      "(Iteration 1301 / 4900) loss: 1.622527\n",
      "(Iteration 1401 / 4900) loss: 1.771084\n",
      "(Epoch 3 / 10) train acc: 0.396000; val_acc: 0.390000\n",
      "(Iteration 1501 / 4900) loss: 1.722343\n",
      "(Iteration 1601 / 4900) loss: 1.685345\n",
      "(Iteration 1701 / 4900) loss: 1.630686\n",
      "(Iteration 1801 / 4900) loss: 1.723726\n",
      "(Iteration 1901 / 4900) loss: 1.657211\n",
      "(Epoch 4 / 10) train acc: 0.410000; val_acc: 0.410000\n",
      "(Iteration 2001 / 4900) loss: 1.867826\n",
      "(Iteration 2101 / 4900) loss: 1.681095\n",
      "(Iteration 2201 / 4900) loss: 1.678352\n",
      "(Iteration 2301 / 4900) loss: 1.682107\n",
      "(Iteration 2401 / 4900) loss: 1.752936\n",
      "(Epoch 5 / 10) train acc: 0.422000; val_acc: 0.441000\n",
      "(Iteration 2501 / 4900) loss: 1.689274\n",
      "(Iteration 2601 / 4900) loss: 1.664786\n",
      "(Iteration 2701 / 4900) loss: 1.514197\n",
      "(Iteration 2801 / 4900) loss: 1.762594\n",
      "(Iteration 2901 / 4900) loss: 1.603475\n",
      "(Epoch 6 / 10) train acc: 0.432000; val_acc: 0.436000\n",
      "(Iteration 3001 / 4900) loss: 1.610141\n",
      "(Iteration 3101 / 4900) loss: 1.385215\n",
      "(Iteration 3201 / 4900) loss: 1.592929\n",
      "(Iteration 3301 / 4900) loss: 1.569749\n",
      "(Iteration 3401 / 4900) loss: 1.490170\n",
      "(Epoch 7 / 10) train acc: 0.456000; val_acc: 0.451000\n",
      "(Iteration 3501 / 4900) loss: 1.646866\n",
      "(Iteration 3601 / 4900) loss: 1.497065\n",
      "(Iteration 3701 / 4900) loss: 1.617932\n",
      "(Iteration 3801 / 4900) loss: 1.697920\n",
      "(Iteration 3901 / 4900) loss: 1.501055\n",
      "(Epoch 8 / 10) train acc: 0.463000; val_acc: 0.460000\n",
      "(Iteration 4001 / 4900) loss: 1.664968\n",
      "(Iteration 4101 / 4900) loss: 1.686302\n",
      "(Iteration 4201 / 4900) loss: 1.629010\n",
      "(Iteration 4301 / 4900) loss: 1.442042\n",
      "(Iteration 4401 / 4900) loss: 1.434737\n",
      "(Epoch 9 / 10) train acc: 0.434000; val_acc: 0.470000\n",
      "(Iteration 4501 / 4900) loss: 1.550989\n",
      "(Iteration 4601 / 4900) loss: 1.540435\n",
      "(Iteration 4701 / 4900) loss: 1.669421\n",
      "(Iteration 4801 / 4900) loss: 1.581057\n",
      "(Epoch 10 / 10) train acc: 0.449000; val_acc: 0.469000\n",
      "validataion accuracy  0.47\n",
      "\n",
      "12 th try:\n",
      "learning rate is  0.000371856203725\n",
      "regularization strengths is  0.00198663788273\n",
      "weight scale is  0.00233726508236\n",
      "hidden size is  232\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.318088\n",
      "(Epoch 0 / 10) train acc: 0.115000; val_acc: 0.094000\n",
      "(Iteration 101 / 4900) loss: 1.832804\n",
      "(Iteration 201 / 4900) loss: 1.753404\n",
      "(Iteration 301 / 4900) loss: 1.773972\n",
      "(Iteration 401 / 4900) loss: 1.621462\n",
      "(Epoch 1 / 10) train acc: 0.470000; val_acc: 0.434000\n",
      "(Iteration 501 / 4900) loss: 1.508816\n",
      "(Iteration 601 / 4900) loss: 1.535748\n",
      "(Iteration 701 / 4900) loss: 1.670490\n",
      "(Iteration 801 / 4900) loss: 1.516777\n",
      "(Iteration 901 / 4900) loss: 1.743056\n",
      "(Epoch 2 / 10) train acc: 0.456000; val_acc: 0.462000\n",
      "(Iteration 1001 / 4900) loss: 1.396851\n",
      "(Iteration 1101 / 4900) loss: 1.479519\n",
      "(Iteration 1201 / 4900) loss: 1.569754\n",
      "(Iteration 1301 / 4900) loss: 1.504847\n",
      "(Iteration 1401 / 4900) loss: 1.392096\n",
      "(Epoch 3 / 10) train acc: 0.520000; val_acc: 0.491000\n",
      "(Iteration 1501 / 4900) loss: 1.452558\n",
      "(Iteration 1601 / 4900) loss: 1.396624\n",
      "(Iteration 1701 / 4900) loss: 1.346559\n",
      "(Iteration 1801 / 4900) loss: 1.639111\n",
      "(Iteration 1901 / 4900) loss: 1.296190\n",
      "(Epoch 4 / 10) train acc: 0.549000; val_acc: 0.492000\n",
      "(Iteration 2001 / 4900) loss: 1.139472\n",
      "(Iteration 2101 / 4900) loss: 1.465419\n",
      "(Iteration 2201 / 4900) loss: 1.386342\n",
      "(Iteration 2301 / 4900) loss: 1.434336\n",
      "(Iteration 2401 / 4900) loss: 1.347400\n",
      "(Epoch 5 / 10) train acc: 0.557000; val_acc: 0.509000\n",
      "(Iteration 2501 / 4900) loss: 1.356145\n",
      "(Iteration 2601 / 4900) loss: 1.292597\n",
      "(Iteration 2701 / 4900) loss: 1.243285\n",
      "(Iteration 2801 / 4900) loss: 1.211886\n",
      "(Iteration 2901 / 4900) loss: 1.147411\n",
      "(Epoch 6 / 10) train acc: 0.582000; val_acc: 0.499000\n",
      "(Iteration 3001 / 4900) loss: 1.134942\n",
      "(Iteration 3101 / 4900) loss: 1.204991\n",
      "(Iteration 3201 / 4900) loss: 1.294445\n",
      "(Iteration 3301 / 4900) loss: 1.298470\n",
      "(Iteration 3401 / 4900) loss: 1.076229\n",
      "(Epoch 7 / 10) train acc: 0.609000; val_acc: 0.513000\n",
      "(Iteration 3501 / 4900) loss: 1.194803\n",
      "(Iteration 3601 / 4900) loss: 0.910576\n",
      "(Iteration 3701 / 4900) loss: 1.253867\n",
      "(Iteration 3801 / 4900) loss: 1.169617\n",
      "(Iteration 3901 / 4900) loss: 1.256394\n",
      "(Epoch 8 / 10) train acc: 0.591000; val_acc: 0.516000\n",
      "(Iteration 4001 / 4900) loss: 1.100629\n",
      "(Iteration 4101 / 4900) loss: 1.199833\n",
      "(Iteration 4201 / 4900) loss: 1.051104\n",
      "(Iteration 4301 / 4900) loss: 1.259735\n",
      "(Iteration 4401 / 4900) loss: 1.034571\n",
      "(Epoch 9 / 10) train acc: 0.609000; val_acc: 0.528000\n",
      "(Iteration 4501 / 4900) loss: 1.226965\n",
      "(Iteration 4601 / 4900) loss: 1.011534\n",
      "(Iteration 4701 / 4900) loss: 1.108035\n",
      "(Iteration 4801 / 4900) loss: 1.183960\n",
      "(Epoch 10 / 10) train acc: 0.602000; val_acc: 0.526000\n",
      "validataion accuracy  0.528\n",
      "\n",
      "13 th try:\n",
      "learning rate is  0.0489986405179\n",
      "regularization strengths is  0.0132037931366\n",
      "weight scale is  0.0518084495603\n",
      "hidden size is  131\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 96.554332\n",
      "(Epoch 0 / 10) train acc: 0.093000; val_acc: 0.099000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.100000; val_acc: 0.087000\n",
      "validataion accuracy  0.099\n",
      "\n",
      "14 th try:\n",
      "learning rate is  9.43501245682e-06\n",
      "regularization strengths is  0.0541409519466\n",
      "weight scale is  0.00493221272975\n",
      "hidden size is  220\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.939662\n",
      "(Epoch 0 / 10) train acc: 0.120000; val_acc: 0.109000\n",
      "(Iteration 101 / 4900) loss: 2.813237\n",
      "(Iteration 201 / 4900) loss: 2.686087\n",
      "(Iteration 301 / 4900) loss: 2.729197\n",
      "(Iteration 401 / 4900) loss: 2.524324\n",
      "(Epoch 1 / 10) train acc: 0.251000; val_acc: 0.264000\n",
      "(Iteration 501 / 4900) loss: 2.468965\n",
      "(Iteration 601 / 4900) loss: 2.517858\n",
      "(Iteration 701 / 4900) loss: 2.534096\n",
      "(Iteration 801 / 4900) loss: 2.487092\n",
      "(Iteration 901 / 4900) loss: 2.492147\n",
      "(Epoch 2 / 10) train acc: 0.304000; val_acc: 0.312000\n",
      "(Iteration 1001 / 4900) loss: 2.495115\n",
      "(Iteration 1101 / 4900) loss: 2.483299\n",
      "(Iteration 1201 / 4900) loss: 2.244624\n",
      "(Iteration 1301 / 4900) loss: 2.458152\n",
      "(Iteration 1401 / 4900) loss: 2.280820\n",
      "(Epoch 3 / 10) train acc: 0.351000; val_acc: 0.337000\n",
      "(Iteration 1501 / 4900) loss: 2.336555\n",
      "(Iteration 1601 / 4900) loss: 2.451512\n",
      "(Iteration 1701 / 4900) loss: 2.352248\n",
      "(Iteration 1801 / 4900) loss: 2.254631\n",
      "(Iteration 1901 / 4900) loss: 2.220175\n",
      "(Epoch 4 / 10) train acc: 0.327000; val_acc: 0.367000\n",
      "(Iteration 2001 / 4900) loss: 2.334620\n",
      "(Iteration 2101 / 4900) loss: 2.420724\n",
      "(Iteration 2201 / 4900) loss: 2.329854\n",
      "(Iteration 2301 / 4900) loss: 2.243388\n",
      "(Iteration 2401 / 4900) loss: 2.223849\n",
      "(Epoch 5 / 10) train acc: 0.346000; val_acc: 0.365000\n",
      "(Iteration 2501 / 4900) loss: 2.140602\n",
      "(Iteration 2601 / 4900) loss: 2.280052\n",
      "(Iteration 2701 / 4900) loss: 2.147134\n",
      "(Iteration 2801 / 4900) loss: 2.413544\n",
      "(Iteration 2901 / 4900) loss: 2.075974\n",
      "(Epoch 6 / 10) train acc: 0.339000; val_acc: 0.375000\n",
      "(Iteration 3001 / 4900) loss: 2.107151\n",
      "(Iteration 3101 / 4900) loss: 2.227779\n",
      "(Iteration 3201 / 4900) loss: 2.215682\n",
      "(Iteration 3301 / 4900) loss: 2.374010\n",
      "(Iteration 3401 / 4900) loss: 2.176115\n",
      "(Epoch 7 / 10) train acc: 0.367000; val_acc: 0.377000\n",
      "(Iteration 3501 / 4900) loss: 2.155896\n",
      "(Iteration 3601 / 4900) loss: 2.078881\n",
      "(Iteration 3701 / 4900) loss: 2.216753\n",
      "(Iteration 3801 / 4900) loss: 2.189140\n",
      "(Iteration 3901 / 4900) loss: 2.290087\n",
      "(Epoch 8 / 10) train acc: 0.362000; val_acc: 0.378000\n",
      "(Iteration 4001 / 4900) loss: 2.215318\n",
      "(Iteration 4101 / 4900) loss: 2.176991\n",
      "(Iteration 4201 / 4900) loss: 2.053427\n",
      "(Iteration 4301 / 4900) loss: 2.152093\n",
      "(Iteration 4401 / 4900) loss: 2.231888\n",
      "(Epoch 9 / 10) train acc: 0.390000; val_acc: 0.380000\n",
      "(Iteration 4501 / 4900) loss: 2.178306\n",
      "(Iteration 4601 / 4900) loss: 2.286038\n",
      "(Iteration 4701 / 4900) loss: 2.284710\n",
      "(Iteration 4801 / 4900) loss: 2.010443\n",
      "(Epoch 10 / 10) train acc: 0.393000; val_acc: 0.378000\n",
      "validataion accuracy  0.38\n",
      "\n",
      "15 th try:\n",
      "learning rate is  0.086532415874\n",
      "regularization strengths is  0.0473647825318\n",
      "weight scale is  0.0204129143439\n",
      "hidden size is  208\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 27.220497\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.096000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.088000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.085000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.093000; val_acc: 0.087000\n",
      "validataion accuracy  0.096\n",
      "\n",
      "16 th try:\n",
      "learning rate is  1.49875657283e-05\n",
      "regularization strengths is  0.00577768344223\n",
      "weight scale is  0.00121679051426\n",
      "hidden size is  130\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.301229\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.100000\n",
      "(Iteration 101 / 4900) loss: 2.281008\n",
      "(Iteration 201 / 4900) loss: 2.281681\n",
      "(Iteration 301 / 4900) loss: 2.253878\n",
      "(Iteration 401 / 4900) loss: 2.229083\n",
      "(Epoch 1 / 10) train acc: 0.220000; val_acc: 0.251000\n",
      "(Iteration 501 / 4900) loss: 2.199613\n",
      "(Iteration 601 / 4900) loss: 2.213048\n",
      "(Iteration 701 / 4900) loss: 2.162419\n",
      "(Iteration 801 / 4900) loss: 2.183359\n",
      "(Iteration 901 / 4900) loss: 2.112509\n",
      "(Epoch 2 / 10) train acc: 0.250000; val_acc: 0.273000\n",
      "(Iteration 1001 / 4900) loss: 2.129513\n",
      "(Iteration 1101 / 4900) loss: 2.120700\n",
      "(Iteration 1201 / 4900) loss: 2.003954\n",
      "(Iteration 1301 / 4900) loss: 2.019891\n",
      "(Iteration 1401 / 4900) loss: 2.069475\n",
      "(Epoch 3 / 10) train acc: 0.267000; val_acc: 0.281000\n",
      "(Iteration 1501 / 4900) loss: 2.124646\n",
      "(Iteration 1601 / 4900) loss: 2.064852\n",
      "(Iteration 1701 / 4900) loss: 2.082581\n",
      "(Iteration 1801 / 4900) loss: 1.910539\n",
      "(Iteration 1901 / 4900) loss: 1.999507\n",
      "(Epoch 4 / 10) train acc: 0.282000; val_acc: 0.301000\n",
      "(Iteration 2001 / 4900) loss: 2.054320\n",
      "(Iteration 2101 / 4900) loss: 1.927413\n",
      "(Iteration 2201 / 4900) loss: 1.961595\n",
      "(Iteration 2301 / 4900) loss: 1.954444\n",
      "(Iteration 2401 / 4900) loss: 2.030384\n",
      "(Epoch 5 / 10) train acc: 0.313000; val_acc: 0.307000\n",
      "(Iteration 2501 / 4900) loss: 2.079007\n",
      "(Iteration 2601 / 4900) loss: 1.918757\n",
      "(Iteration 2701 / 4900) loss: 1.957779\n",
      "(Iteration 2801 / 4900) loss: 1.889206\n",
      "(Iteration 2901 / 4900) loss: 1.964800\n",
      "(Epoch 6 / 10) train acc: 0.324000; val_acc: 0.312000\n",
      "(Iteration 3001 / 4900) loss: 1.883586\n",
      "(Iteration 3101 / 4900) loss: 1.947878\n",
      "(Iteration 3201 / 4900) loss: 1.952926\n",
      "(Iteration 3301 / 4900) loss: 1.852769\n",
      "(Iteration 3401 / 4900) loss: 1.929711\n",
      "(Epoch 7 / 10) train acc: 0.302000; val_acc: 0.321000\n",
      "(Iteration 3501 / 4900) loss: 1.843947\n",
      "(Iteration 3601 / 4900) loss: 1.862557\n",
      "(Iteration 3701 / 4900) loss: 1.863642\n",
      "(Iteration 3801 / 4900) loss: 1.988010\n",
      "(Iteration 3901 / 4900) loss: 1.803419\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.333000\n",
      "(Iteration 4001 / 4900) loss: 1.912189\n",
      "(Iteration 4101 / 4900) loss: 1.910503\n",
      "(Iteration 4201 / 4900) loss: 1.941000\n",
      "(Iteration 4301 / 4900) loss: 1.960994\n",
      "(Iteration 4401 / 4900) loss: 1.894217\n",
      "(Epoch 9 / 10) train acc: 0.350000; val_acc: 0.338000\n",
      "(Iteration 4501 / 4900) loss: 1.944695\n",
      "(Iteration 4601 / 4900) loss: 1.909418\n",
      "(Iteration 4701 / 4900) loss: 1.839521\n",
      "(Iteration 4801 / 4900) loss: 1.920485\n",
      "(Epoch 10 / 10) train acc: 0.334000; val_acc: 0.346000\n",
      "validataion accuracy  0.346\n",
      "\n",
      "17 th try:\n",
      "learning rate is  2.62961384617e-05\n",
      "regularization strengths is  0.208078342516\n",
      "weight scale is  0.011754109858\n",
      "hidden size is  51\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 6.654268\n",
      "(Epoch 0 / 10) train acc: 0.079000; val_acc: 0.074000\n",
      "(Iteration 101 / 4900) loss: 5.051797\n",
      "(Iteration 201 / 4900) loss: 4.907791\n",
      "(Iteration 301 / 4900) loss: 4.535728\n",
      "(Iteration 401 / 4900) loss: 4.417794\n",
      "(Epoch 1 / 10) train acc: 0.247000; val_acc: 0.236000\n",
      "(Iteration 501 / 4900) loss: 4.571193\n",
      "(Iteration 601 / 4900) loss: 4.358172\n",
      "(Iteration 701 / 4900) loss: 4.546777\n",
      "(Iteration 801 / 4900) loss: 4.332548\n",
      "(Iteration 901 / 4900) loss: 4.239049\n",
      "(Epoch 2 / 10) train acc: 0.291000; val_acc: 0.288000\n",
      "(Iteration 1001 / 4900) loss: 4.202318\n",
      "(Iteration 1101 / 4900) loss: 4.225257\n",
      "(Iteration 1201 / 4900) loss: 4.177001\n",
      "(Iteration 1301 / 4900) loss: 4.285307\n",
      "(Iteration 1401 / 4900) loss: 4.099064\n",
      "(Epoch 3 / 10) train acc: 0.290000; val_acc: 0.317000\n",
      "(Iteration 1501 / 4900) loss: 4.339217\n",
      "(Iteration 1601 / 4900) loss: 4.227171\n",
      "(Iteration 1701 / 4900) loss: 3.982459\n",
      "(Iteration 1801 / 4900) loss: 4.056678\n",
      "(Iteration 1901 / 4900) loss: 3.970955\n",
      "(Epoch 4 / 10) train acc: 0.335000; val_acc: 0.332000\n",
      "(Iteration 2001 / 4900) loss: 4.120495\n",
      "(Iteration 2101 / 4900) loss: 4.042176\n",
      "(Iteration 2201 / 4900) loss: 4.066357\n",
      "(Iteration 2301 / 4900) loss: 4.076914\n",
      "(Iteration 2401 / 4900) loss: 3.905603\n",
      "(Epoch 5 / 10) train acc: 0.329000; val_acc: 0.341000\n",
      "(Iteration 2501 / 4900) loss: 4.038589\n",
      "(Iteration 2601 / 4900) loss: 4.207934\n",
      "(Iteration 2701 / 4900) loss: 4.087270\n",
      "(Iteration 2801 / 4900) loss: 4.191749\n",
      "(Iteration 2901 / 4900) loss: 3.884805\n",
      "(Epoch 6 / 10) train acc: 0.325000; val_acc: 0.355000\n",
      "(Iteration 3001 / 4900) loss: 4.004633\n",
      "(Iteration 3101 / 4900) loss: 3.906855\n",
      "(Iteration 3201 / 4900) loss: 4.044873\n",
      "(Iteration 3301 / 4900) loss: 3.973172\n",
      "(Iteration 3401 / 4900) loss: 3.927471\n",
      "(Epoch 7 / 10) train acc: 0.334000; val_acc: 0.358000\n",
      "(Iteration 3501 / 4900) loss: 3.954715\n",
      "(Iteration 3601 / 4900) loss: 4.158343\n",
      "(Iteration 3701 / 4900) loss: 3.973530\n",
      "(Iteration 3801 / 4900) loss: 4.192606\n",
      "(Iteration 3901 / 4900) loss: 3.867275\n",
      "(Epoch 8 / 10) train acc: 0.371000; val_acc: 0.362000\n",
      "(Iteration 4001 / 4900) loss: 4.128524\n",
      "(Iteration 4101 / 4900) loss: 3.878956\n",
      "(Iteration 4201 / 4900) loss: 4.027331\n",
      "(Iteration 4301 / 4900) loss: 3.986835\n",
      "(Iteration 4401 / 4900) loss: 4.106486\n",
      "(Epoch 9 / 10) train acc: 0.379000; val_acc: 0.368000\n",
      "(Iteration 4501 / 4900) loss: 4.160244\n",
      "(Iteration 4601 / 4900) loss: 4.013420\n",
      "(Iteration 4701 / 4900) loss: 4.035984\n",
      "(Iteration 4801 / 4900) loss: 3.753210\n",
      "(Epoch 10 / 10) train acc: 0.386000; val_acc: 0.367000\n",
      "validataion accuracy  0.368\n",
      "\n",
      "18 th try:\n",
      "learning rate is  3.743211195e-05\n",
      "regularization strengths is  0.00560995905681\n",
      "weight scale is  0.00279662905804\n",
      "hidden size is  303\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.388815\n",
      "(Epoch 0 / 10) train acc: 0.094000; val_acc: 0.107000\n",
      "(Iteration 101 / 4900) loss: 2.151805\n",
      "(Iteration 201 / 4900) loss: 2.023224\n",
      "(Iteration 301 / 4900) loss: 1.933101\n",
      "(Iteration 401 / 4900) loss: 1.917286\n",
      "(Epoch 1 / 10) train acc: 0.327000; val_acc: 0.338000\n",
      "(Iteration 501 / 4900) loss: 1.927794\n",
      "(Iteration 601 / 4900) loss: 1.878212\n",
      "(Iteration 701 / 4900) loss: 1.839835\n",
      "(Iteration 801 / 4900) loss: 1.799921\n",
      "(Iteration 901 / 4900) loss: 1.724570\n",
      "(Epoch 2 / 10) train acc: 0.396000; val_acc: 0.368000\n",
      "(Iteration 1001 / 4900) loss: 1.659821\n",
      "(Iteration 1101 / 4900) loss: 1.862922\n",
      "(Iteration 1201 / 4900) loss: 1.828118\n",
      "(Iteration 1301 / 4900) loss: 1.761034\n",
      "(Iteration 1401 / 4900) loss: 1.652213\n",
      "(Epoch 3 / 10) train acc: 0.382000; val_acc: 0.398000\n",
      "(Iteration 1501 / 4900) loss: 1.768571\n",
      "(Iteration 1601 / 4900) loss: 1.880185\n",
      "(Iteration 1701 / 4900) loss: 1.640662\n",
      "(Iteration 1801 / 4900) loss: 1.731526\n",
      "(Iteration 1901 / 4900) loss: 1.846368\n",
      "(Epoch 4 / 10) train acc: 0.397000; val_acc: 0.422000\n",
      "(Iteration 2001 / 4900) loss: 1.639702\n",
      "(Iteration 2101 / 4900) loss: 1.710649\n",
      "(Iteration 2201 / 4900) loss: 1.670017\n",
      "(Iteration 2301 / 4900) loss: 2.011243\n",
      "(Iteration 2401 / 4900) loss: 1.794516\n",
      "(Epoch 5 / 10) train acc: 0.423000; val_acc: 0.421000\n",
      "(Iteration 2501 / 4900) loss: 1.676322\n",
      "(Iteration 2601 / 4900) loss: 1.710822\n",
      "(Iteration 2701 / 4900) loss: 1.731990\n",
      "(Iteration 2801 / 4900) loss: 1.725829\n",
      "(Iteration 2901 / 4900) loss: 1.683054\n",
      "(Epoch 6 / 10) train acc: 0.443000; val_acc: 0.436000\n",
      "(Iteration 3001 / 4900) loss: 1.626056\n",
      "(Iteration 3101 / 4900) loss: 1.511213\n",
      "(Iteration 3201 / 4900) loss: 1.706016\n",
      "(Iteration 3301 / 4900) loss: 1.639223\n",
      "(Iteration 3401 / 4900) loss: 1.550607\n",
      "(Epoch 7 / 10) train acc: 0.452000; val_acc: 0.442000\n",
      "(Iteration 3501 / 4900) loss: 1.649726\n",
      "(Iteration 3601 / 4900) loss: 1.589523\n",
      "(Iteration 3701 / 4900) loss: 1.607343\n",
      "(Iteration 3801 / 4900) loss: 1.562276\n",
      "(Iteration 3901 / 4900) loss: 1.510812\n",
      "(Epoch 8 / 10) train acc: 0.419000; val_acc: 0.448000\n",
      "(Iteration 4001 / 4900) loss: 1.795212\n",
      "(Iteration 4101 / 4900) loss: 1.603171\n",
      "(Iteration 4201 / 4900) loss: 1.641496\n",
      "(Iteration 4301 / 4900) loss: 1.664908\n",
      "(Iteration 4401 / 4900) loss: 1.715697\n",
      "(Epoch 9 / 10) train acc: 0.442000; val_acc: 0.454000\n",
      "(Iteration 4501 / 4900) loss: 1.571944\n",
      "(Iteration 4601 / 4900) loss: 1.494398\n",
      "(Iteration 4701 / 4900) loss: 1.698373\n",
      "(Iteration 4801 / 4900) loss: 1.587491\n",
      "(Epoch 10 / 10) train acc: 0.452000; val_acc: 0.458000\n",
      "validataion accuracy  0.458\n",
      "\n",
      "19 th try:\n",
      "learning rate is  0.0328867321196\n",
      "regularization strengths is  0.00627436110114\n",
      "weight scale is  0.00746395454217\n",
      "hidden size is  160\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 3.306267\n",
      "(Epoch 0 / 10) train acc: 0.107000; val_acc: 0.113000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.077000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.089000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.085000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.093000; val_acc: 0.087000\n",
      "validataion accuracy  0.113\n",
      "\n",
      "20 th try:\n",
      "learning rate is  0.000437434697076\n",
      "regularization strengths is  0.916943138151\n",
      "weight scale is  0.0786643960695\n",
      "hidden size is  185\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: inf\n",
      "(Epoch 0 / 10) train acc: 0.151000; val_acc: 0.137000\n",
      "(Iteration 101 / 4900) loss: 1560.897665\n",
      "(Iteration 201 / 4900) loss: 1417.732961\n",
      "(Iteration 301 / 4900) loss: 1285.713368\n",
      "(Iteration 401 / 4900) loss: 1201.520400\n",
      "(Epoch 1 / 10) train acc: 0.257000; val_acc: 0.270000\n",
      "(Iteration 501 / 4900) loss: 1097.996849\n",
      "(Iteration 601 / 4900) loss: 1016.920627\n",
      "(Iteration 701 / 4900) loss: 939.153560\n",
      "(Iteration 801 / 4900) loss: 864.677402\n",
      "(Iteration 901 / 4900) loss: 805.037773\n",
      "(Epoch 2 / 10) train acc: 0.273000; val_acc: 0.291000\n",
      "(Iteration 1001 / 4900) loss: 749.480251\n",
      "(Iteration 1101 / 4900) loss: 689.864367\n",
      "(Iteration 1201 / 4900) loss: 644.833793\n",
      "(Iteration 1301 / 4900) loss: 595.020841\n",
      "(Iteration 1401 / 4900) loss: 555.508310\n",
      "(Epoch 3 / 10) train acc: 0.254000; val_acc: 0.301000\n",
      "(Iteration 1501 / 4900) loss: 514.317297\n",
      "(Iteration 1601 / 4900) loss: 482.082698\n",
      "(Iteration 1701 / 4900) loss: 446.605939\n",
      "(Iteration 1801 / 4900) loss: 418.015490\n",
      "(Iteration 1901 / 4900) loss: 388.197499\n",
      "(Epoch 4 / 10) train acc: 0.264000; val_acc: 0.293000\n",
      "(Iteration 2001 / 4900) loss: 362.612026\n",
      "(Iteration 2101 / 4900) loss: 341.002149\n",
      "(Iteration 2201 / 4900) loss: 318.600415\n",
      "(Iteration 2301 / 4900) loss: 298.853435\n",
      "(Iteration 2401 / 4900) loss: 280.586007\n",
      "(Epoch 5 / 10) train acc: 0.358000; val_acc: 0.332000\n",
      "(Iteration 2501 / 4900) loss: 262.651109\n",
      "(Iteration 2601 / 4900) loss: 246.503002\n",
      "(Iteration 2701 / 4900) loss: 231.684306\n",
      "(Iteration 2801 / 4900) loss: 217.875705\n",
      "(Iteration 2901 / 4900) loss: 205.151171\n",
      "(Epoch 6 / 10) train acc: 0.317000; val_acc: 0.321000\n",
      "(Iteration 3001 / 4900) loss: 192.467690\n",
      "(Iteration 3101 / 4900) loss: 181.917422\n",
      "(Iteration 3201 / 4900) loss: 171.082875\n",
      "(Iteration 3301 / 4900) loss: 161.542038\n",
      "(Iteration 3401 / 4900) loss: 152.171977\n",
      "(Epoch 7 / 10) train acc: 0.360000; val_acc: 0.358000\n",
      "(Iteration 3501 / 4900) loss: 143.743790\n",
      "(Iteration 3601 / 4900) loss: 135.938292\n",
      "(Iteration 3701 / 4900) loss: 128.542464\n",
      "(Iteration 3801 / 4900) loss: 121.886652\n",
      "(Iteration 3901 / 4900) loss: 115.257666\n",
      "(Epoch 8 / 10) train acc: 0.457000; val_acc: 0.438000\n",
      "(Iteration 4001 / 4900) loss: 109.262680\n",
      "(Iteration 4101 / 4900) loss: 103.612419\n",
      "(Iteration 4201 / 4900) loss: 98.262311\n",
      "(Iteration 4301 / 4900) loss: 93.607619\n",
      "(Iteration 4401 / 4900) loss: 88.488960\n",
      "(Epoch 9 / 10) train acc: 0.462000; val_acc: 0.448000\n",
      "(Iteration 4501 / 4900) loss: 84.160364\n",
      "(Iteration 4601 / 4900) loss: 80.138140\n",
      "(Iteration 4701 / 4900) loss: 76.296202\n",
      "(Iteration 4801 / 4900) loss: 72.480833\n",
      "(Epoch 10 / 10) train acc: 0.452000; val_acc: 0.442000\n",
      "validataion accuracy  0.448\n",
      "\n",
      "21 th try:\n",
      "learning rate is  0.0228682175575\n",
      "regularization strengths is  0.00483814934706\n",
      "weight scale is  0.00210706749955\n",
      "hidden size is  129\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.319902\n",
      "(Epoch 0 / 10) train acc: 0.099000; val_acc: 0.078000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.116000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.089000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.088000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.113000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "validataion accuracy  0.087\n",
      "\n",
      "22 th try:\n",
      "learning rate is  6.28398748254e-06\n",
      "regularization strengths is  0.00126663273947\n",
      "weight scale is  0.0215308198561\n",
      "hidden size is  58\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 13.289520\n",
      "(Epoch 0 / 10) train acc: 0.083000; val_acc: 0.074000\n",
      "(Iteration 101 / 4900) loss: 8.911373\n",
      "(Iteration 201 / 4900) loss: 6.937282\n",
      "(Iteration 301 / 4900) loss: 7.269728\n",
      "(Iteration 401 / 4900) loss: 6.345247\n",
      "(Epoch 1 / 10) train acc: 0.163000; val_acc: 0.143000\n",
      "(Iteration 501 / 4900) loss: 6.003052\n",
      "(Iteration 601 / 4900) loss: 6.567491\n",
      "(Iteration 701 / 4900) loss: 6.829751\n",
      "(Iteration 801 / 4900) loss: 5.431444\n",
      "(Iteration 901 / 4900) loss: 4.878863\n",
      "(Epoch 2 / 10) train acc: 0.187000; val_acc: 0.180000\n",
      "(Iteration 1001 / 4900) loss: 5.006521\n",
      "(Iteration 1101 / 4900) loss: 3.892920\n",
      "(Iteration 1201 / 4900) loss: 4.105297\n",
      "(Iteration 1301 / 4900) loss: 4.293265\n",
      "(Iteration 1401 / 4900) loss: 4.256764\n",
      "(Epoch 3 / 10) train acc: 0.202000; val_acc: 0.201000\n",
      "(Iteration 1501 / 4900) loss: 4.403187\n",
      "(Iteration 1601 / 4900) loss: 4.443268\n",
      "(Iteration 1701 / 4900) loss: 4.413395\n",
      "(Iteration 1801 / 4900) loss: 4.443246\n",
      "(Iteration 1901 / 4900) loss: 4.358168\n",
      "(Epoch 4 / 10) train acc: 0.217000; val_acc: 0.216000\n",
      "(Iteration 2001 / 4900) loss: 3.922512\n",
      "(Iteration 2101 / 4900) loss: 4.718540\n",
      "(Iteration 2201 / 4900) loss: 3.488683\n",
      "(Iteration 2301 / 4900) loss: 3.566372\n",
      "(Iteration 2401 / 4900) loss: 3.955041\n",
      "(Epoch 5 / 10) train acc: 0.217000; val_acc: 0.224000\n",
      "(Iteration 2501 / 4900) loss: 4.003514\n",
      "(Iteration 2601 / 4900) loss: 3.595182\n",
      "(Iteration 2701 / 4900) loss: 3.770351\n",
      "(Iteration 2801 / 4900) loss: 3.470851\n",
      "(Iteration 2901 / 4900) loss: 3.112857\n",
      "(Epoch 6 / 10) train acc: 0.222000; val_acc: 0.239000\n",
      "(Iteration 3001 / 4900) loss: 3.635873\n",
      "(Iteration 3101 / 4900) loss: 2.990996\n",
      "(Iteration 3201 / 4900) loss: 3.712538\n",
      "(Iteration 3301 / 4900) loss: 3.127279\n",
      "(Iteration 3401 / 4900) loss: 2.923664\n",
      "(Epoch 7 / 10) train acc: 0.221000; val_acc: 0.249000\n",
      "(Iteration 3501 / 4900) loss: 3.415598\n",
      "(Iteration 3601 / 4900) loss: 2.937271\n",
      "(Iteration 3701 / 4900) loss: 2.877096\n",
      "(Iteration 3801 / 4900) loss: 3.055613\n",
      "(Iteration 3901 / 4900) loss: 2.598286\n",
      "(Epoch 8 / 10) train acc: 0.232000; val_acc: 0.257000\n",
      "(Iteration 4001 / 4900) loss: 3.058547\n",
      "(Iteration 4101 / 4900) loss: 3.069957\n",
      "(Iteration 4201 / 4900) loss: 3.407711\n",
      "(Iteration 4301 / 4900) loss: 3.311776\n",
      "(Iteration 4401 / 4900) loss: 2.942217\n",
      "(Epoch 9 / 10) train acc: 0.270000; val_acc: 0.266000\n",
      "(Iteration 4501 / 4900) loss: 2.894418\n",
      "(Iteration 4601 / 4900) loss: 3.074717\n",
      "(Iteration 4701 / 4900) loss: 3.199721\n",
      "(Iteration 4801 / 4900) loss: 2.607794\n",
      "(Epoch 10 / 10) train acc: 0.241000; val_acc: 0.264000\n",
      "validataion accuracy  0.266\n",
      "\n",
      "23 th try:\n",
      "learning rate is  6.06396939248e-06\n",
      "regularization strengths is  0.00113156163495\n",
      "weight scale is  0.00350955819639\n",
      "hidden size is  101\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.394903\n",
      "(Epoch 0 / 10) train acc: 0.093000; val_acc: 0.081000\n",
      "(Iteration 101 / 4900) loss: 2.328953\n",
      "(Iteration 201 / 4900) loss: 2.290920\n",
      "(Iteration 301 / 4900) loss: 2.251555\n",
      "(Iteration 401 / 4900) loss: 2.249256\n",
      "(Epoch 1 / 10) train acc: 0.180000; val_acc: 0.198000\n",
      "(Iteration 501 / 4900) loss: 2.218715\n",
      "(Iteration 601 / 4900) loss: 2.203723\n",
      "(Iteration 701 / 4900) loss: 2.225795\n",
      "(Iteration 801 / 4900) loss: 2.139989\n",
      "(Iteration 901 / 4900) loss: 2.151767\n",
      "(Epoch 2 / 10) train acc: 0.242000; val_acc: 0.254000\n",
      "(Iteration 1001 / 4900) loss: 2.218297\n",
      "(Iteration 1101 / 4900) loss: 2.127606\n",
      "(Iteration 1201 / 4900) loss: 2.091037\n",
      "(Iteration 1301 / 4900) loss: 2.111291\n",
      "(Iteration 1401 / 4900) loss: 2.114555\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.277000\n",
      "(Iteration 1501 / 4900) loss: 2.141995\n",
      "(Iteration 1601 / 4900) loss: 2.155332\n",
      "(Iteration 1701 / 4900) loss: 2.047288\n",
      "(Iteration 1801 / 4900) loss: 2.166057\n",
      "(Iteration 1901 / 4900) loss: 2.042534\n",
      "(Epoch 4 / 10) train acc: 0.267000; val_acc: 0.277000\n",
      "(Iteration 2001 / 4900) loss: 2.083186\n",
      "(Iteration 2101 / 4900) loss: 2.041222\n",
      "(Iteration 2201 / 4900) loss: 1.994931\n",
      "(Iteration 2301 / 4900) loss: 2.080003\n",
      "(Iteration 2401 / 4900) loss: 2.138207\n",
      "(Epoch 5 / 10) train acc: 0.290000; val_acc: 0.281000\n",
      "(Iteration 2501 / 4900) loss: 1.970335\n",
      "(Iteration 2601 / 4900) loss: 2.060515\n",
      "(Iteration 2701 / 4900) loss: 1.897342\n",
      "(Iteration 2801 / 4900) loss: 2.070759\n",
      "(Iteration 2901 / 4900) loss: 1.972818\n",
      "(Epoch 6 / 10) train acc: 0.281000; val_acc: 0.292000\n",
      "(Iteration 3001 / 4900) loss: 2.088697\n",
      "(Iteration 3101 / 4900) loss: 1.960439\n",
      "(Iteration 3201 / 4900) loss: 2.017914\n",
      "(Iteration 3301 / 4900) loss: 1.865030\n",
      "(Iteration 3401 / 4900) loss: 2.054823\n",
      "(Epoch 7 / 10) train acc: 0.305000; val_acc: 0.300000\n",
      "(Iteration 3501 / 4900) loss: 1.953533\n",
      "(Iteration 3601 / 4900) loss: 2.042656\n",
      "(Iteration 3701 / 4900) loss: 1.985727\n",
      "(Iteration 3801 / 4900) loss: 1.953291\n",
      "(Iteration 3901 / 4900) loss: 1.936317\n",
      "(Epoch 8 / 10) train acc: 0.319000; val_acc: 0.306000\n",
      "(Iteration 4001 / 4900) loss: 1.992749\n",
      "(Iteration 4101 / 4900) loss: 2.030352\n",
      "(Iteration 4201 / 4900) loss: 1.953842\n",
      "(Iteration 4301 / 4900) loss: 1.972200\n",
      "(Iteration 4401 / 4900) loss: 1.987521\n",
      "(Epoch 9 / 10) train acc: 0.300000; val_acc: 0.313000\n",
      "(Iteration 4501 / 4900) loss: 1.975661\n",
      "(Iteration 4601 / 4900) loss: 1.998988\n",
      "(Iteration 4701 / 4900) loss: 1.915211\n",
      "(Iteration 4801 / 4900) loss: 1.871380\n",
      "(Epoch 10 / 10) train acc: 0.343000; val_acc: 0.315000\n",
      "validataion accuracy  0.315\n",
      "\n",
      "24 th try:\n",
      "learning rate is  0.000178107074933\n",
      "regularization strengths is  0.682094379062\n",
      "weight scale is  0.00339353936085\n",
      "hidden size is  96\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 3.507484\n",
      "(Epoch 0 / 10) train acc: 0.139000; val_acc: 0.128000\n",
      "(Iteration 101 / 4900) loss: 3.139616\n",
      "(Iteration 201 / 4900) loss: 2.918484\n",
      "(Iteration 301 / 4900) loss: 2.818038\n",
      "(Iteration 401 / 4900) loss: 2.754695\n",
      "(Epoch 1 / 10) train acc: 0.382000; val_acc: 0.395000\n",
      "(Iteration 501 / 4900) loss: 2.714024\n",
      "(Iteration 601 / 4900) loss: 2.895235\n",
      "(Iteration 701 / 4900) loss: 2.548117\n",
      "(Iteration 801 / 4900) loss: 2.648999\n",
      "(Iteration 901 / 4900) loss: 2.745222\n",
      "(Epoch 2 / 10) train acc: 0.443000; val_acc: 0.414000\n",
      "(Iteration 1001 / 4900) loss: 2.615128\n",
      "(Iteration 1101 / 4900) loss: 2.585712\n",
      "(Iteration 1201 / 4900) loss: 2.501288\n",
      "(Iteration 1301 / 4900) loss: 2.442906\n",
      "(Iteration 1401 / 4900) loss: 2.321842\n",
      "(Epoch 3 / 10) train acc: 0.456000; val_acc: 0.447000\n",
      "(Iteration 1501 / 4900) loss: 2.589019\n",
      "(Iteration 1601 / 4900) loss: 2.469827\n",
      "(Iteration 1701 / 4900) loss: 2.453985\n",
      "(Iteration 1801 / 4900) loss: 2.266950\n",
      "(Iteration 1901 / 4900) loss: 2.422940\n",
      "(Epoch 4 / 10) train acc: 0.497000; val_acc: 0.461000\n",
      "(Iteration 2001 / 4900) loss: 2.256830\n",
      "(Iteration 2101 / 4900) loss: 2.356116\n",
      "(Iteration 2201 / 4900) loss: 2.362842\n",
      "(Iteration 2301 / 4900) loss: 2.242288\n",
      "(Iteration 2401 / 4900) loss: 2.235145\n",
      "(Epoch 5 / 10) train acc: 0.465000; val_acc: 0.460000\n",
      "(Iteration 2501 / 4900) loss: 2.173442\n",
      "(Iteration 2601 / 4900) loss: 2.216105\n",
      "(Iteration 2701 / 4900) loss: 2.237004\n",
      "(Iteration 2801 / 4900) loss: 2.157987\n",
      "(Iteration 2901 / 4900) loss: 2.075452\n",
      "(Epoch 6 / 10) train acc: 0.498000; val_acc: 0.481000\n",
      "(Iteration 3001 / 4900) loss: 2.081917\n",
      "(Iteration 3101 / 4900) loss: 1.987313\n",
      "(Iteration 3201 / 4900) loss: 2.150960\n",
      "(Iteration 3301 / 4900) loss: 2.117649\n",
      "(Iteration 3401 / 4900) loss: 1.999657\n",
      "(Epoch 7 / 10) train acc: 0.500000; val_acc: 0.485000\n",
      "(Iteration 3501 / 4900) loss: 2.036212\n",
      "(Iteration 3601 / 4900) loss: 2.022846\n",
      "(Iteration 3701 / 4900) loss: 1.991145\n",
      "(Iteration 3801 / 4900) loss: 1.986187\n",
      "(Iteration 3901 / 4900) loss: 1.842089\n",
      "(Epoch 8 / 10) train acc: 0.471000; val_acc: 0.486000\n",
      "(Iteration 4001 / 4900) loss: 1.890314\n",
      "(Iteration 4101 / 4900) loss: 1.897549\n",
      "(Iteration 4201 / 4900) loss: 1.906693\n",
      "(Iteration 4301 / 4900) loss: 2.032820\n",
      "(Iteration 4401 / 4900) loss: 2.076392\n",
      "(Epoch 9 / 10) train acc: 0.508000; val_acc: 0.509000\n",
      "(Iteration 4501 / 4900) loss: 1.962413\n",
      "(Iteration 4601 / 4900) loss: 2.066870\n",
      "(Iteration 4701 / 4900) loss: 1.889890\n",
      "(Iteration 4801 / 4900) loss: 1.894900\n",
      "(Epoch 10 / 10) train acc: 0.536000; val_acc: 0.500000\n",
      "validataion accuracy  0.509\n",
      "\n",
      "25 th try:\n",
      "learning rate is  8.48765487447e-06\n",
      "regularization strengths is  0.193405962186\n",
      "weight scale is  0.0513053044166\n",
      "hidden size is  306\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 393.375062\n",
      "(Epoch 0 / 10) train acc: 0.087000; val_acc: 0.086000\n",
      "(Iteration 101 / 4900) loss: 343.847950\n",
      "(Iteration 201 / 4900) loss: 331.208417\n",
      "(Iteration 301 / 4900) loss: 303.223061\n",
      "(Iteration 401 / 4900) loss: 315.702954\n",
      "(Epoch 1 / 10) train acc: 0.221000; val_acc: 0.194000\n",
      "(Iteration 501 / 4900) loss: 300.930759\n",
      "(Iteration 601 / 4900) loss: 310.816122\n",
      "(Iteration 701 / 4900) loss: 300.803347\n",
      "(Iteration 801 / 4900) loss: 304.622845\n",
      "(Iteration 901 / 4900) loss: 287.680813\n",
      "(Epoch 2 / 10) train acc: 0.244000; val_acc: 0.237000\n",
      "(Iteration 1001 / 4900) loss: 294.757756\n",
      "(Iteration 1101 / 4900) loss: 294.515270\n",
      "(Iteration 1201 / 4900) loss: 290.103684\n",
      "(Iteration 1301 / 4900) loss: 280.547134\n",
      "(Iteration 1401 / 4900) loss: 281.323430\n",
      "(Epoch 3 / 10) train acc: 0.212000; val_acc: 0.257000\n",
      "(Iteration 1501 / 4900) loss: 298.024722\n",
      "(Iteration 1601 / 4900) loss: 288.143740\n",
      "(Iteration 1701 / 4900) loss: 293.392838\n",
      "(Iteration 1801 / 4900) loss: 279.997018\n",
      "(Iteration 1901 / 4900) loss: 277.865739\n",
      "(Epoch 4 / 10) train acc: 0.277000; val_acc: 0.255000\n",
      "(Iteration 2001 / 4900) loss: 286.814187\n",
      "(Iteration 2101 / 4900) loss: 284.127123\n",
      "(Iteration 2201 / 4900) loss: 278.721221\n",
      "(Iteration 2301 / 4900) loss: 281.310094\n",
      "(Iteration 2401 / 4900) loss: 287.845577\n",
      "(Epoch 5 / 10) train acc: 0.305000; val_acc: 0.253000\n",
      "(Iteration 2501 / 4900) loss: 289.566225\n",
      "(Iteration 2601 / 4900) loss: 288.173724\n",
      "(Iteration 2701 / 4900) loss: 272.320197\n",
      "(Iteration 2801 / 4900) loss: 277.687991\n",
      "(Iteration 2901 / 4900) loss: 278.270348\n",
      "(Epoch 6 / 10) train acc: 0.277000; val_acc: 0.270000\n",
      "(Iteration 3001 / 4900) loss: 272.427038\n",
      "(Iteration 3101 / 4900) loss: 273.139611\n",
      "(Iteration 3201 / 4900) loss: 275.255023\n",
      "(Iteration 3301 / 4900) loss: 271.849079\n",
      "(Iteration 3401 / 4900) loss: 272.502679\n",
      "(Epoch 7 / 10) train acc: 0.290000; val_acc: 0.268000\n",
      "(Iteration 3501 / 4900) loss: 272.939729\n",
      "(Iteration 3601 / 4900) loss: 273.423228\n",
      "(Iteration 3701 / 4900) loss: 276.303314\n",
      "(Iteration 3801 / 4900) loss: 268.902139\n",
      "(Iteration 3901 / 4900) loss: 269.525185\n",
      "(Epoch 8 / 10) train acc: 0.310000; val_acc: 0.277000\n",
      "(Iteration 4001 / 4900) loss: 273.703355\n",
      "(Iteration 4101 / 4900) loss: 260.367531\n",
      "(Iteration 4201 / 4900) loss: 266.589603\n",
      "(Iteration 4301 / 4900) loss: 265.114359\n",
      "(Iteration 4401 / 4900) loss: 268.730818\n",
      "(Epoch 9 / 10) train acc: 0.298000; val_acc: 0.275000\n",
      "(Iteration 4501 / 4900) loss: 268.561541\n",
      "(Iteration 4601 / 4900) loss: 269.416641\n",
      "(Iteration 4701 / 4900) loss: 271.471852\n",
      "(Iteration 4801 / 4900) loss: 271.438509\n",
      "(Epoch 10 / 10) train acc: 0.303000; val_acc: 0.290000\n",
      "validataion accuracy  0.29\n",
      "\n",
      "26 th try:\n",
      "learning rate is  9.89361823665e-06\n",
      "regularization strengths is  0.0650896321797\n",
      "weight scale is  0.00253124181366\n",
      "hidden size is  90\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.400568\n",
      "(Epoch 0 / 10) train acc: 0.086000; val_acc: 0.092000\n",
      "(Iteration 101 / 4900) loss: 2.361140\n",
      "(Iteration 201 / 4900) loss: 2.332564\n",
      "(Iteration 301 / 4900) loss: 2.326727\n",
      "(Iteration 401 / 4900) loss: 2.268985\n",
      "(Epoch 1 / 10) train acc: 0.223000; val_acc: 0.224000\n",
      "(Iteration 501 / 4900) loss: 2.260723\n",
      "(Iteration 601 / 4900) loss: 2.269695\n",
      "(Iteration 701 / 4900) loss: 2.190898\n",
      "(Iteration 801 / 4900) loss: 2.191340\n",
      "(Iteration 901 / 4900) loss: 2.215559\n",
      "(Epoch 2 / 10) train acc: 0.269000; val_acc: 0.272000\n",
      "(Iteration 1001 / 4900) loss: 2.196794\n",
      "(Iteration 1101 / 4900) loss: 2.142387\n",
      "(Iteration 1201 / 4900) loss: 2.125870\n",
      "(Iteration 1301 / 4900) loss: 2.190925\n",
      "(Iteration 1401 / 4900) loss: 2.074644\n",
      "(Epoch 3 / 10) train acc: 0.265000; val_acc: 0.296000\n",
      "(Iteration 1501 / 4900) loss: 1.998818\n",
      "(Iteration 1601 / 4900) loss: 2.102768\n",
      "(Iteration 1701 / 4900) loss: 2.170537\n",
      "(Iteration 1801 / 4900) loss: 2.073720\n",
      "(Iteration 1901 / 4900) loss: 2.137418\n",
      "(Epoch 4 / 10) train acc: 0.280000; val_acc: 0.305000\n",
      "(Iteration 2001 / 4900) loss: 2.138946\n",
      "(Iteration 2101 / 4900) loss: 2.123283\n",
      "(Iteration 2201 / 4900) loss: 2.012350\n",
      "(Iteration 2301 / 4900) loss: 2.120731\n",
      "(Iteration 2401 / 4900) loss: 2.153772\n",
      "(Epoch 5 / 10) train acc: 0.290000; val_acc: 0.315000\n",
      "(Iteration 2501 / 4900) loss: 2.024599\n",
      "(Iteration 2601 / 4900) loss: 2.143938\n",
      "(Iteration 2701 / 4900) loss: 2.074313\n",
      "(Iteration 2801 / 4900) loss: 2.178573\n",
      "(Iteration 2901 / 4900) loss: 1.944345\n",
      "(Epoch 6 / 10) train acc: 0.304000; val_acc: 0.324000\n",
      "(Iteration 3001 / 4900) loss: 2.014952\n",
      "(Iteration 3101 / 4900) loss: 1.946186\n",
      "(Iteration 3201 / 4900) loss: 1.990101\n",
      "(Iteration 3301 / 4900) loss: 1.976440\n",
      "(Iteration 3401 / 4900) loss: 1.980731\n",
      "(Epoch 7 / 10) train acc: 0.308000; val_acc: 0.336000\n",
      "(Iteration 3501 / 4900) loss: 2.094595\n",
      "(Iteration 3601 / 4900) loss: 1.969428\n",
      "(Iteration 3701 / 4900) loss: 2.003097\n",
      "(Iteration 3801 / 4900) loss: 2.094566\n",
      "(Iteration 3901 / 4900) loss: 1.972756\n",
      "(Epoch 8 / 10) train acc: 0.329000; val_acc: 0.342000\n",
      "(Iteration 4001 / 4900) loss: 1.982635\n",
      "(Iteration 4101 / 4900) loss: 2.067822\n",
      "(Iteration 4201 / 4900) loss: 1.989607\n",
      "(Iteration 4301 / 4900) loss: 1.924716\n",
      "(Iteration 4401 / 4900) loss: 2.014720\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.350000\n",
      "(Iteration 4501 / 4900) loss: 2.073547\n",
      "(Iteration 4601 / 4900) loss: 2.084910\n",
      "(Iteration 4701 / 4900) loss: 1.980761\n",
      "(Iteration 4801 / 4900) loss: 1.901346\n",
      "(Epoch 10 / 10) train acc: 0.330000; val_acc: 0.351000\n",
      "validataion accuracy  0.351\n",
      "\n",
      "27 th try:\n",
      "learning rate is  2.37545177401e-05\n",
      "regularization strengths is  0.0453688530632\n",
      "weight scale is  0.00678592764635\n",
      "hidden size is  173\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 3.827964\n",
      "(Epoch 0 / 10) train acc: 0.093000; val_acc: 0.088000\n",
      "(Iteration 101 / 4900) loss: 2.747442\n",
      "(Iteration 201 / 4900) loss: 2.607770\n",
      "(Iteration 301 / 4900) loss: 2.693319\n",
      "(Iteration 401 / 4900) loss: 2.679258\n",
      "(Epoch 1 / 10) train acc: 0.280000; val_acc: 0.298000\n",
      "(Iteration 501 / 4900) loss: 2.479265\n",
      "(Iteration 601 / 4900) loss: 2.689774\n",
      "(Iteration 701 / 4900) loss: 2.446231\n",
      "(Iteration 801 / 4900) loss: 2.621533\n",
      "(Iteration 901 / 4900) loss: 2.563474\n",
      "(Epoch 2 / 10) train acc: 0.350000; val_acc: 0.340000\n",
      "(Iteration 1001 / 4900) loss: 2.478266\n",
      "(Iteration 1101 / 4900) loss: 2.296961\n",
      "(Iteration 1201 / 4900) loss: 2.506430\n",
      "(Iteration 1301 / 4900) loss: 2.298094\n",
      "(Iteration 1401 / 4900) loss: 2.394735\n",
      "(Epoch 3 / 10) train acc: 0.319000; val_acc: 0.375000\n",
      "(Iteration 1501 / 4900) loss: 2.373995\n",
      "(Iteration 1601 / 4900) loss: 2.482032\n",
      "(Iteration 1701 / 4900) loss: 2.385964\n",
      "(Iteration 1801 / 4900) loss: 2.288632\n",
      "(Iteration 1901 / 4900) loss: 2.332985\n",
      "(Epoch 4 / 10) train acc: 0.354000; val_acc: 0.389000\n",
      "(Iteration 2001 / 4900) loss: 2.244514\n",
      "(Iteration 2101 / 4900) loss: 2.472785\n",
      "(Iteration 2201 / 4900) loss: 2.181289\n",
      "(Iteration 2301 / 4900) loss: 2.356709\n",
      "(Iteration 2401 / 4900) loss: 2.339296\n",
      "(Epoch 5 / 10) train acc: 0.401000; val_acc: 0.392000\n",
      "(Iteration 2501 / 4900) loss: 2.291314\n",
      "(Iteration 2601 / 4900) loss: 2.431415\n",
      "(Iteration 2701 / 4900) loss: 2.318639\n",
      "(Iteration 2801 / 4900) loss: 2.063382\n",
      "(Iteration 2901 / 4900) loss: 2.304562\n",
      "(Epoch 6 / 10) train acc: 0.399000; val_acc: 0.397000\n",
      "(Iteration 3001 / 4900) loss: 2.087270\n",
      "(Iteration 3101 / 4900) loss: 2.255101\n",
      "(Iteration 3201 / 4900) loss: 2.291383\n",
      "(Iteration 3301 / 4900) loss: 2.179283\n",
      "(Iteration 3401 / 4900) loss: 2.265214\n",
      "(Epoch 7 / 10) train acc: 0.406000; val_acc: 0.402000\n",
      "(Iteration 3501 / 4900) loss: 2.206536\n",
      "(Iteration 3601 / 4900) loss: 2.140863\n",
      "(Iteration 3701 / 4900) loss: 2.250796\n",
      "(Iteration 3801 / 4900) loss: 2.136640\n",
      "(Iteration 3901 / 4900) loss: 2.371044\n",
      "(Epoch 8 / 10) train acc: 0.398000; val_acc: 0.407000\n",
      "(Iteration 4001 / 4900) loss: 2.361423\n",
      "(Iteration 4101 / 4900) loss: 2.198510\n",
      "(Iteration 4201 / 4900) loss: 2.258296\n",
      "(Iteration 4301 / 4900) loss: 2.303688\n",
      "(Iteration 4401 / 4900) loss: 2.037426\n",
      "(Epoch 9 / 10) train acc: 0.400000; val_acc: 0.406000\n",
      "(Iteration 4501 / 4900) loss: 2.200255\n",
      "(Iteration 4601 / 4900) loss: 2.389878\n",
      "(Iteration 4701 / 4900) loss: 2.491053\n",
      "(Iteration 4801 / 4900) loss: 2.260716\n",
      "(Epoch 10 / 10) train acc: 0.390000; val_acc: 0.416000\n",
      "validataion accuracy  0.416\n",
      "\n",
      "28 th try:\n",
      "learning rate is  6.66047557797e-05\n",
      "regularization strengths is  0.00336913087059\n",
      "weight scale is  0.00212422887035\n",
      "hidden size is  236\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.343106\n",
      "(Epoch 0 / 10) train acc: 0.082000; val_acc: 0.092000\n",
      "(Iteration 101 / 4900) loss: 2.100524\n",
      "(Iteration 201 / 4900) loss: 2.053591\n",
      "(Iteration 301 / 4900) loss: 2.032768\n",
      "(Iteration 401 / 4900) loss: 1.973835\n",
      "(Epoch 1 / 10) train acc: 0.348000; val_acc: 0.352000\n",
      "(Iteration 501 / 4900) loss: 1.872141\n",
      "(Iteration 601 / 4900) loss: 1.925353\n",
      "(Iteration 701 / 4900) loss: 1.800892\n",
      "(Iteration 801 / 4900) loss: 1.858254\n",
      "(Iteration 901 / 4900) loss: 1.704663\n",
      "(Epoch 2 / 10) train acc: 0.407000; val_acc: 0.387000\n",
      "(Iteration 1001 / 4900) loss: 1.620970\n",
      "(Iteration 1101 / 4900) loss: 1.813514\n",
      "(Iteration 1201 / 4900) loss: 1.695900\n",
      "(Iteration 1301 / 4900) loss: 1.706437\n",
      "(Iteration 1401 / 4900) loss: 1.699002\n",
      "(Epoch 3 / 10) train acc: 0.397000; val_acc: 0.429000\n",
      "(Iteration 1501 / 4900) loss: 1.638982\n",
      "(Iteration 1601 / 4900) loss: 1.743750\n",
      "(Iteration 1701 / 4900) loss: 1.562515\n",
      "(Iteration 1801 / 4900) loss: 1.629909\n",
      "(Iteration 1901 / 4900) loss: 1.623754\n",
      "(Epoch 4 / 10) train acc: 0.398000; val_acc: 0.437000\n",
      "(Iteration 2001 / 4900) loss: 1.574217\n",
      "(Iteration 2101 / 4900) loss: 1.625797\n",
      "(Iteration 2201 / 4900) loss: 1.497728\n",
      "(Iteration 2301 / 4900) loss: 1.477433\n",
      "(Iteration 2401 / 4900) loss: 1.742508\n",
      "(Epoch 5 / 10) train acc: 0.419000; val_acc: 0.440000\n",
      "(Iteration 2501 / 4900) loss: 1.454779\n",
      "(Iteration 2601 / 4900) loss: 1.624716\n",
      "(Iteration 2701 / 4900) loss: 1.574534\n",
      "(Iteration 2801 / 4900) loss: 1.681888\n",
      "(Iteration 2901 / 4900) loss: 1.655942\n",
      "(Epoch 6 / 10) train acc: 0.458000; val_acc: 0.454000\n",
      "(Iteration 3001 / 4900) loss: 1.693755\n",
      "(Iteration 3101 / 4900) loss: 1.529490\n",
      "(Iteration 3201 / 4900) loss: 1.662348\n",
      "(Iteration 3301 / 4900) loss: 1.495105\n",
      "(Iteration 3401 / 4900) loss: 1.548562\n",
      "(Epoch 7 / 10) train acc: 0.475000; val_acc: 0.465000\n",
      "(Iteration 3501 / 4900) loss: 1.460867\n",
      "(Iteration 3601 / 4900) loss: 1.493457\n",
      "(Iteration 3701 / 4900) loss: 1.703461\n",
      "(Iteration 3801 / 4900) loss: 1.604450\n",
      "(Iteration 3901 / 4900) loss: 1.664343\n",
      "(Epoch 8 / 10) train acc: 0.463000; val_acc: 0.463000\n",
      "(Iteration 4001 / 4900) loss: 1.677664\n",
      "(Iteration 4101 / 4900) loss: 1.376473\n",
      "(Iteration 4201 / 4900) loss: 1.400814\n",
      "(Iteration 4301 / 4900) loss: 1.523425\n",
      "(Iteration 4401 / 4900) loss: 1.462088\n",
      "(Epoch 9 / 10) train acc: 0.456000; val_acc: 0.476000\n",
      "(Iteration 4501 / 4900) loss: 1.450565\n",
      "(Iteration 4601 / 4900) loss: 1.534941\n",
      "(Iteration 4701 / 4900) loss: 1.501515\n",
      "(Iteration 4801 / 4900) loss: 1.532373\n",
      "(Epoch 10 / 10) train acc: 0.484000; val_acc: 0.463000\n",
      "validataion accuracy  0.476\n",
      "\n",
      "29 th try:\n",
      "learning rate is  2.7508672275e-06\n",
      "regularization strengths is  0.0103763506044\n",
      "weight scale is  0.00387147321636\n",
      "hidden size is  68\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.365238\n",
      "(Epoch 0 / 10) train acc: 0.091000; val_acc: 0.075000\n",
      "(Iteration 101 / 4900) loss: 2.368654\n",
      "(Iteration 201 / 4900) loss: 2.354897\n",
      "(Iteration 301 / 4900) loss: 2.362765\n",
      "(Iteration 401 / 4900) loss: 2.294437\n",
      "(Epoch 1 / 10) train acc: 0.129000; val_acc: 0.129000\n",
      "(Iteration 501 / 4900) loss: 2.328901\n",
      "(Iteration 601 / 4900) loss: 2.319841\n",
      "(Iteration 701 / 4900) loss: 2.264837\n",
      "(Iteration 801 / 4900) loss: 2.269196\n",
      "(Iteration 901 / 4900) loss: 2.219880\n",
      "(Epoch 2 / 10) train acc: 0.173000; val_acc: 0.165000\n",
      "(Iteration 1001 / 4900) loss: 2.256652\n",
      "(Iteration 1101 / 4900) loss: 2.296945\n",
      "(Iteration 1201 / 4900) loss: 2.280735\n",
      "(Iteration 1301 / 4900) loss: 2.223099\n",
      "(Iteration 1401 / 4900) loss: 2.279495\n",
      "(Epoch 3 / 10) train acc: 0.166000; val_acc: 0.176000\n",
      "(Iteration 1501 / 4900) loss: 2.219415\n",
      "(Iteration 1601 / 4900) loss: 2.200437\n",
      "(Iteration 1701 / 4900) loss: 2.217984\n",
      "(Iteration 1801 / 4900) loss: 2.245673\n",
      "(Iteration 1901 / 4900) loss: 2.206887\n",
      "(Epoch 4 / 10) train acc: 0.234000; val_acc: 0.195000\n",
      "(Iteration 2001 / 4900) loss: 2.189002\n",
      "(Iteration 2101 / 4900) loss: 2.118968\n",
      "(Iteration 2201 / 4900) loss: 2.138634\n",
      "(Iteration 2301 / 4900) loss: 2.189890\n",
      "(Iteration 2401 / 4900) loss: 2.191540\n",
      "(Epoch 5 / 10) train acc: 0.226000; val_acc: 0.214000\n",
      "(Iteration 2501 / 4900) loss: 2.134777\n",
      "(Iteration 2601 / 4900) loss: 2.140970\n",
      "(Iteration 2701 / 4900) loss: 2.154100\n",
      "(Iteration 2801 / 4900) loss: 2.190205\n",
      "(Iteration 2901 / 4900) loss: 2.107289\n",
      "(Epoch 6 / 10) train acc: 0.222000; val_acc: 0.238000\n",
      "(Iteration 3001 / 4900) loss: 2.129902\n",
      "(Iteration 3101 / 4900) loss: 2.148898\n",
      "(Iteration 3201 / 4900) loss: 2.133578\n",
      "(Iteration 3301 / 4900) loss: 2.176796\n",
      "(Iteration 3401 / 4900) loss: 2.123744\n",
      "(Epoch 7 / 10) train acc: 0.228000; val_acc: 0.247000\n",
      "(Iteration 3501 / 4900) loss: 2.201422\n",
      "(Iteration 3601 / 4900) loss: 2.131199\n",
      "(Iteration 3701 / 4900) loss: 2.159467\n",
      "(Iteration 3801 / 4900) loss: 2.169752\n",
      "(Iteration 3901 / 4900) loss: 2.111188\n",
      "(Epoch 8 / 10) train acc: 0.249000; val_acc: 0.255000\n",
      "(Iteration 4001 / 4900) loss: 2.175469\n",
      "(Iteration 4101 / 4900) loss: 2.122106\n",
      "(Iteration 4201 / 4900) loss: 2.110539\n",
      "(Iteration 4301 / 4900) loss: 2.124739\n",
      "(Iteration 4401 / 4900) loss: 2.039011\n",
      "(Epoch 9 / 10) train acc: 0.245000; val_acc: 0.275000\n",
      "(Iteration 4501 / 4900) loss: 2.103392\n",
      "(Iteration 4601 / 4900) loss: 2.099441\n",
      "(Iteration 4701 / 4900) loss: 2.190732\n",
      "(Iteration 4801 / 4900) loss: 2.076705\n",
      "(Epoch 10 / 10) train acc: 0.246000; val_acc: 0.281000\n",
      "validataion accuracy  0.281\n",
      "\n",
      "30 th try:\n",
      "learning rate is  0.000625525018544\n",
      "regularization strengths is  0.00190032689798\n",
      "weight scale is  0.09123225812\n",
      "hidden size is  53\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 182.317945\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.111000\n",
      "(Iteration 101 / 4900) loss: 27.663659\n",
      "(Iteration 201 / 4900) loss: 25.716558\n",
      "(Iteration 301 / 4900) loss: 40.668194\n",
      "(Iteration 401 / 4900) loss: 26.496005\n",
      "(Epoch 1 / 10) train acc: 0.155000; val_acc: 0.146000\n",
      "(Iteration 501 / 4900) loss: 19.153909\n",
      "(Iteration 601 / 4900) loss: 22.805045\n",
      "(Iteration 701 / 4900) loss: 18.799821\n",
      "(Iteration 801 / 4900) loss: 28.624389\n",
      "(Iteration 901 / 4900) loss: 13.805102\n",
      "(Epoch 2 / 10) train acc: 0.190000; val_acc: 0.170000\n",
      "(Iteration 1001 / 4900) loss: 17.125231\n",
      "(Iteration 1101 / 4900) loss: 19.214424\n",
      "(Iteration 1201 / 4900) loss: 12.665159\n",
      "(Iteration 1301 / 4900) loss: 14.410064\n",
      "(Iteration 1401 / 4900) loss: 12.841141\n",
      "(Epoch 3 / 10) train acc: 0.235000; val_acc: 0.242000\n",
      "(Iteration 1501 / 4900) loss: 13.479912\n",
      "(Iteration 1601 / 4900) loss: 15.545956\n",
      "(Iteration 1701 / 4900) loss: 15.710765\n",
      "(Iteration 1801 / 4900) loss: 10.829565\n",
      "(Iteration 1901 / 4900) loss: 13.874508\n",
      "(Epoch 4 / 10) train acc: 0.202000; val_acc: 0.182000\n",
      "(Iteration 2001 / 4900) loss: 8.381933\n",
      "(Iteration 2101 / 4900) loss: 9.171650\n",
      "(Iteration 2201 / 4900) loss: 9.348901\n",
      "(Iteration 2301 / 4900) loss: 9.568913\n",
      "(Iteration 2401 / 4900) loss: 10.177118\n",
      "(Epoch 5 / 10) train acc: 0.210000; val_acc: 0.209000\n",
      "(Iteration 2501 / 4900) loss: 9.282033\n",
      "(Iteration 2601 / 4900) loss: 7.513526\n",
      "(Iteration 2701 / 4900) loss: 9.543869\n",
      "(Iteration 2801 / 4900) loss: 7.260893\n",
      "(Iteration 2901 / 4900) loss: 10.174443\n",
      "(Epoch 6 / 10) train acc: 0.238000; val_acc: 0.184000\n",
      "(Iteration 3001 / 4900) loss: 8.201913\n",
      "(Iteration 3101 / 4900) loss: 8.146391\n",
      "(Iteration 3201 / 4900) loss: 10.777028\n",
      "(Iteration 3301 / 4900) loss: 9.430290\n",
      "(Iteration 3401 / 4900) loss: 6.445475\n",
      "(Epoch 7 / 10) train acc: 0.178000; val_acc: 0.200000\n",
      "(Iteration 3501 / 4900) loss: 9.600773\n",
      "(Iteration 3601 / 4900) loss: 9.102333\n",
      "(Iteration 3701 / 4900) loss: 8.664497\n",
      "(Iteration 3801 / 4900) loss: 8.865089\n",
      "(Iteration 3901 / 4900) loss: 6.022562\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.177000\n",
      "(Iteration 4001 / 4900) loss: 8.113358\n",
      "(Iteration 4101 / 4900) loss: 6.158292\n",
      "(Iteration 4201 / 4900) loss: 6.497336\n",
      "(Iteration 4301 / 4900) loss: 7.030266\n",
      "(Iteration 4401 / 4900) loss: 7.015833\n",
      "(Epoch 9 / 10) train acc: 0.229000; val_acc: 0.213000\n",
      "(Iteration 4501 / 4900) loss: 9.727355\n",
      "(Iteration 4601 / 4900) loss: 8.480408\n",
      "(Iteration 4701 / 4900) loss: 4.774704\n",
      "(Iteration 4801 / 4900) loss: 6.880041\n",
      "(Epoch 10 / 10) train acc: 0.199000; val_acc: 0.219000\n",
      "validataion accuracy  0.242\n",
      "\n",
      "31 th try:\n",
      "learning rate is  0.0878998712653\n",
      "regularization strengths is  0.00136861836218\n",
      "weight scale is  0.00590610253467\n",
      "hidden size is  134\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 3.054837\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.107000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.112000; val_acc: 0.087000\n",
      "validataion accuracy  0.107\n",
      "\n",
      "32 th try:\n",
      "learning rate is  0.000411794648597\n",
      "regularization strengths is  0.057592625181\n",
      "weight scale is  0.00685405742213\n",
      "hidden size is  304\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 4.917969\n",
      "(Epoch 0 / 10) train acc: 0.160000; val_acc: 0.155000\n",
      "(Iteration 101 / 4900) loss: 3.215732\n",
      "(Iteration 201 / 4900) loss: 2.909739\n",
      "(Iteration 301 / 4900) loss: 2.908036\n",
      "(Iteration 401 / 4900) loss: 3.134155\n",
      "(Epoch 1 / 10) train acc: 0.428000; val_acc: 0.427000\n",
      "(Iteration 501 / 4900) loss: 2.791118\n",
      "(Iteration 601 / 4900) loss: 2.680754\n",
      "(Iteration 701 / 4900) loss: 2.883326\n",
      "(Iteration 801 / 4900) loss: 2.740203\n",
      "(Iteration 901 / 4900) loss: 2.626781\n",
      "(Epoch 2 / 10) train acc: 0.505000; val_acc: 0.460000\n",
      "(Iteration 1001 / 4900) loss: 2.663467\n",
      "(Iteration 1101 / 4900) loss: 2.495879\n",
      "(Iteration 1201 / 4900) loss: 2.644613\n",
      "(Iteration 1301 / 4900) loss: 2.524508\n",
      "(Iteration 1401 / 4900) loss: 2.437607\n",
      "(Epoch 3 / 10) train acc: 0.527000; val_acc: 0.476000\n",
      "(Iteration 1501 / 4900) loss: 2.577084\n",
      "(Iteration 1601 / 4900) loss: 2.537801\n",
      "(Iteration 1701 / 4900) loss: 2.454521\n",
      "(Iteration 1801 / 4900) loss: 2.519962\n",
      "(Iteration 1901 / 4900) loss: 2.519836\n",
      "(Epoch 4 / 10) train acc: 0.516000; val_acc: 0.468000\n",
      "(Iteration 2001 / 4900) loss: 2.520268\n",
      "(Iteration 2101 / 4900) loss: 2.366138\n",
      "(Iteration 2201 / 4900) loss: 2.376194\n",
      "(Iteration 2301 / 4900) loss: 2.388601\n",
      "(Iteration 2401 / 4900) loss: 2.485825\n",
      "(Epoch 5 / 10) train acc: 0.561000; val_acc: 0.493000\n",
      "(Iteration 2501 / 4900) loss: 2.462035\n",
      "(Iteration 2601 / 4900) loss: 2.439238\n",
      "(Iteration 2701 / 4900) loss: 2.612875\n",
      "(Iteration 2801 / 4900) loss: 2.156070\n",
      "(Iteration 2901 / 4900) loss: 2.273614\n",
      "(Epoch 6 / 10) train acc: 0.567000; val_acc: 0.490000\n",
      "(Iteration 3001 / 4900) loss: 2.460388\n",
      "(Iteration 3101 / 4900) loss: 2.618616\n",
      "(Iteration 3201 / 4900) loss: 2.342397\n",
      "(Iteration 3301 / 4900) loss: 2.336255\n",
      "(Iteration 3401 / 4900) loss: 2.170370\n",
      "(Epoch 7 / 10) train acc: 0.589000; val_acc: 0.501000\n",
      "(Iteration 3501 / 4900) loss: 2.237738\n",
      "(Iteration 3601 / 4900) loss: 2.417627\n",
      "(Iteration 3701 / 4900) loss: 2.259687\n",
      "(Iteration 3801 / 4900) loss: 2.238976\n",
      "(Iteration 3901 / 4900) loss: 2.205809\n",
      "(Epoch 8 / 10) train acc: 0.590000; val_acc: 0.515000\n",
      "(Iteration 4001 / 4900) loss: 2.079085\n",
      "(Iteration 4101 / 4900) loss: 2.336573\n",
      "(Iteration 4201 / 4900) loss: 2.181649\n",
      "(Iteration 4301 / 4900) loss: 2.161838\n",
      "(Iteration 4401 / 4900) loss: 1.984465\n",
      "(Epoch 9 / 10) train acc: 0.608000; val_acc: 0.524000\n",
      "(Iteration 4501 / 4900) loss: 2.383790\n",
      "(Iteration 4601 / 4900) loss: 2.101325\n",
      "(Iteration 4701 / 4900) loss: 2.157653\n",
      "(Iteration 4801 / 4900) loss: 2.110302\n",
      "(Epoch 10 / 10) train acc: 0.623000; val_acc: 0.514000\n",
      "validataion accuracy  0.524\n",
      "\n",
      "33 th try:\n",
      "learning rate is  2.05328471067e-05\n",
      "regularization strengths is  0.0830242735101\n",
      "weight scale is  0.00136308455751\n",
      "hidden size is  83\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.313628\n",
      "(Epoch 0 / 10) train acc: 0.107000; val_acc: 0.083000\n",
      "(Iteration 101 / 4900) loss: 2.308284\n",
      "(Iteration 201 / 4900) loss: 2.304074\n",
      "(Iteration 301 / 4900) loss: 2.292148\n",
      "(Iteration 401 / 4900) loss: 2.227894\n",
      "(Epoch 1 / 10) train acc: 0.221000; val_acc: 0.243000\n",
      "(Iteration 501 / 4900) loss: 2.207562\n",
      "(Iteration 601 / 4900) loss: 2.190156\n",
      "(Iteration 701 / 4900) loss: 2.151674\n",
      "(Iteration 801 / 4900) loss: 2.217154\n",
      "(Iteration 901 / 4900) loss: 2.142628\n",
      "(Epoch 2 / 10) train acc: 0.251000; val_acc: 0.267000\n",
      "(Iteration 1001 / 4900) loss: 2.101433\n",
      "(Iteration 1101 / 4900) loss: 2.017811\n",
      "(Iteration 1201 / 4900) loss: 2.101519\n",
      "(Iteration 1301 / 4900) loss: 2.093093\n",
      "(Iteration 1401 / 4900) loss: 2.086516\n",
      "(Epoch 3 / 10) train acc: 0.279000; val_acc: 0.287000\n",
      "(Iteration 1501 / 4900) loss: 2.063494\n",
      "(Iteration 1601 / 4900) loss: 2.012470\n",
      "(Iteration 1701 / 4900) loss: 1.888946\n",
      "(Iteration 1801 / 4900) loss: 1.996408\n",
      "(Iteration 1901 / 4900) loss: 2.009394\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.297000\n",
      "(Iteration 2001 / 4900) loss: 1.931662\n",
      "(Iteration 2101 / 4900) loss: 2.079055\n",
      "(Iteration 2201 / 4900) loss: 2.025668\n",
      "(Iteration 2301 / 4900) loss: 2.023362\n",
      "(Iteration 2401 / 4900) loss: 1.941187\n",
      "(Epoch 5 / 10) train acc: 0.338000; val_acc: 0.313000\n",
      "(Iteration 2501 / 4900) loss: 1.856556\n",
      "(Iteration 2601 / 4900) loss: 2.054472\n",
      "(Iteration 2701 / 4900) loss: 1.891828\n",
      "(Iteration 2801 / 4900) loss: 1.911497\n",
      "(Iteration 2901 / 4900) loss: 1.800893\n",
      "(Epoch 6 / 10) train acc: 0.337000; val_acc: 0.313000\n",
      "(Iteration 3001 / 4900) loss: 1.897738\n",
      "(Iteration 3101 / 4900) loss: 1.779129\n",
      "(Iteration 3201 / 4900) loss: 1.840701\n",
      "(Iteration 3301 / 4900) loss: 1.923757\n",
      "(Iteration 3401 / 4900) loss: 1.938624\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.332000\n",
      "(Iteration 3501 / 4900) loss: 1.925471\n",
      "(Iteration 3601 / 4900) loss: 1.912072\n",
      "(Iteration 3701 / 4900) loss: 2.000242\n",
      "(Iteration 3801 / 4900) loss: 1.859638\n",
      "(Iteration 3901 / 4900) loss: 1.832715\n",
      "(Epoch 8 / 10) train acc: 0.334000; val_acc: 0.337000\n",
      "(Iteration 4001 / 4900) loss: 1.762638\n",
      "(Iteration 4101 / 4900) loss: 1.780197\n",
      "(Iteration 4201 / 4900) loss: 1.985443\n",
      "(Iteration 4301 / 4900) loss: 1.933743\n",
      "(Iteration 4401 / 4900) loss: 1.944827\n",
      "(Epoch 9 / 10) train acc: 0.350000; val_acc: 0.344000\n",
      "(Iteration 4501 / 4900) loss: 1.783349\n",
      "(Iteration 4601 / 4900) loss: 1.879382\n",
      "(Iteration 4701 / 4900) loss: 1.854543\n",
      "(Iteration 4801 / 4900) loss: 1.700865\n",
      "(Epoch 10 / 10) train acc: 0.350000; val_acc: 0.350000\n",
      "validataion accuracy  0.35\n",
      "\n",
      "34 th try:\n",
      "learning rate is  0.00118188416697\n",
      "regularization strengths is  0.0276539379729\n",
      "weight scale is  0.00920649782423\n",
      "hidden size is  121\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 4.522381\n",
      "(Epoch 0 / 10) train acc: 0.142000; val_acc: 0.156000\n",
      "(Iteration 101 / 4900) loss: 2.160512\n",
      "(Iteration 201 / 4900) loss: 2.125857\n",
      "(Iteration 301 / 4900) loss: 2.212971\n",
      "(Iteration 401 / 4900) loss: 2.275876\n",
      "(Epoch 1 / 10) train acc: 0.407000; val_acc: 0.390000\n",
      "(Iteration 501 / 4900) loss: 2.156143\n",
      "(Iteration 601 / 4900) loss: 2.008524\n",
      "(Iteration 701 / 4900) loss: 1.886611\n",
      "(Iteration 801 / 4900) loss: 1.879588\n",
      "(Iteration 901 / 4900) loss: 2.080296\n",
      "(Epoch 2 / 10) train acc: 0.464000; val_acc: 0.419000\n",
      "(Iteration 1001 / 4900) loss: 2.065905\n",
      "(Iteration 1101 / 4900) loss: 1.857992\n",
      "(Iteration 1201 / 4900) loss: 1.748474\n",
      "(Iteration 1301 / 4900) loss: 2.036982\n",
      "(Iteration 1401 / 4900) loss: 1.868417\n",
      "(Epoch 3 / 10) train acc: 0.467000; val_acc: 0.442000\n",
      "(Iteration 1501 / 4900) loss: 2.087855\n",
      "(Iteration 1601 / 4900) loss: 1.826277\n",
      "(Iteration 1701 / 4900) loss: 2.002455\n",
      "(Iteration 1801 / 4900) loss: 1.682930\n",
      "(Iteration 1901 / 4900) loss: 1.941817\n",
      "(Epoch 4 / 10) train acc: 0.490000; val_acc: 0.465000\n",
      "(Iteration 2001 / 4900) loss: 1.670245\n",
      "(Iteration 2101 / 4900) loss: 1.729361\n",
      "(Iteration 2201 / 4900) loss: 1.709529\n",
      "(Iteration 2301 / 4900) loss: 1.800400\n",
      "(Iteration 2401 / 4900) loss: 1.771647\n",
      "(Epoch 5 / 10) train acc: 0.523000; val_acc: 0.457000\n",
      "(Iteration 2501 / 4900) loss: 1.682942\n",
      "(Iteration 2601 / 4900) loss: 1.850066\n",
      "(Iteration 2701 / 4900) loss: 1.700258\n",
      "(Iteration 2801 / 4900) loss: 1.588860\n",
      "(Iteration 2901 / 4900) loss: 1.703497\n",
      "(Epoch 6 / 10) train acc: 0.521000; val_acc: 0.459000\n",
      "(Iteration 3001 / 4900) loss: 1.593178\n",
      "(Iteration 3101 / 4900) loss: 1.594724\n",
      "(Iteration 3201 / 4900) loss: 1.640798\n",
      "(Iteration 3301 / 4900) loss: 1.549830\n",
      "(Iteration 3401 / 4900) loss: 1.693270\n",
      "(Epoch 7 / 10) train acc: 0.551000; val_acc: 0.465000\n",
      "(Iteration 3501 / 4900) loss: 1.557112\n",
      "(Iteration 3601 / 4900) loss: 1.818751\n",
      "(Iteration 3701 / 4900) loss: 1.603884\n",
      "(Iteration 3801 / 4900) loss: 1.551449\n",
      "(Iteration 3901 / 4900) loss: 1.611235\n",
      "(Epoch 8 / 10) train acc: 0.555000; val_acc: 0.480000\n",
      "(Iteration 4001 / 4900) loss: 1.560690\n",
      "(Iteration 4101 / 4900) loss: 1.542425\n",
      "(Iteration 4201 / 4900) loss: 1.509699\n",
      "(Iteration 4301 / 4900) loss: 1.641011\n",
      "(Iteration 4401 / 4900) loss: 1.678916\n",
      "(Epoch 9 / 10) train acc: 0.574000; val_acc: 0.507000\n",
      "(Iteration 4501 / 4900) loss: 1.514608\n",
      "(Iteration 4601 / 4900) loss: 1.598547\n",
      "(Iteration 4701 / 4900) loss: 1.790442\n",
      "(Iteration 4801 / 4900) loss: 1.758903\n",
      "(Epoch 10 / 10) train acc: 0.590000; val_acc: 0.492000\n",
      "validataion accuracy  0.507\n",
      "\n",
      "35 th try:\n",
      "learning rate is  0.0884993170812\n",
      "regularization strengths is  0.451819165314\n",
      "weight scale is  0.0383512708431\n",
      "hidden size is  202\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 288.226955\n",
      "(Epoch 0 / 10) train acc: 0.062000; val_acc: 0.089000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.115000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.079000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.087000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.106000; val_acc: 0.087000\n",
      "validataion accuracy  0.089\n",
      "\n",
      "36 th try:\n",
      "learning rate is  0.00272459954322\n",
      "regularization strengths is  0.0253084182313\n",
      "weight scale is  0.00636459035119\n",
      "hidden size is  136\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 3.191585\n",
      "(Epoch 0 / 10) train acc: 0.163000; val_acc: 0.198000\n",
      "(Iteration 101 / 4900) loss: 2.271743\n",
      "(Iteration 201 / 4900) loss: 2.199658\n",
      "(Iteration 301 / 4900) loss: 2.169484\n",
      "(Iteration 401 / 4900) loss: 2.133801\n",
      "(Epoch 1 / 10) train acc: 0.340000; val_acc: 0.343000\n",
      "(Iteration 501 / 4900) loss: 2.243514\n",
      "(Iteration 601 / 4900) loss: 1.966080\n",
      "(Iteration 701 / 4900) loss: 1.906859\n",
      "(Iteration 801 / 4900) loss: 2.324737\n",
      "(Iteration 901 / 4900) loss: 2.076649\n",
      "(Epoch 2 / 10) train acc: 0.335000; val_acc: 0.322000\n",
      "(Iteration 1001 / 4900) loss: 1.802876\n",
      "(Iteration 1101 / 4900) loss: 1.937636\n",
      "(Iteration 1201 / 4900) loss: 2.423615\n",
      "(Iteration 1301 / 4900) loss: 2.429291\n",
      "(Iteration 1401 / 4900) loss: 1.925210\n",
      "(Epoch 3 / 10) train acc: 0.452000; val_acc: 0.407000\n",
      "(Iteration 1501 / 4900) loss: 2.195563\n",
      "(Iteration 1601 / 4900) loss: 1.629567\n",
      "(Iteration 1701 / 4900) loss: 1.933035\n",
      "(Iteration 1801 / 4900) loss: 2.600128\n",
      "(Iteration 1901 / 4900) loss: 2.057630\n",
      "(Epoch 4 / 10) train acc: 0.435000; val_acc: 0.404000\n",
      "(Iteration 2001 / 4900) loss: 1.998439\n",
      "(Iteration 2101 / 4900) loss: 1.837026\n",
      "(Iteration 2201 / 4900) loss: 1.911453\n",
      "(Iteration 2301 / 4900) loss: 2.063301\n",
      "(Iteration 2401 / 4900) loss: 2.024178\n",
      "(Epoch 5 / 10) train acc: 0.440000; val_acc: 0.404000\n",
      "(Iteration 2501 / 4900) loss: 1.986314\n",
      "(Iteration 2601 / 4900) loss: 1.941936\n",
      "(Iteration 2701 / 4900) loss: 1.837111\n",
      "(Iteration 2801 / 4900) loss: 1.967188\n",
      "(Iteration 2901 / 4900) loss: 1.750904\n",
      "(Epoch 6 / 10) train acc: 0.439000; val_acc: 0.408000\n",
      "(Iteration 3001 / 4900) loss: 1.731657\n",
      "(Iteration 3101 / 4900) loss: 1.498529\n",
      "(Iteration 3201 / 4900) loss: 2.028212\n",
      "(Iteration 3301 / 4900) loss: 1.671914\n",
      "(Iteration 3401 / 4900) loss: 1.750749\n",
      "(Epoch 7 / 10) train acc: 0.437000; val_acc: 0.423000\n",
      "(Iteration 3501 / 4900) loss: 2.007231\n",
      "(Iteration 3601 / 4900) loss: 1.555477\n",
      "(Iteration 3701 / 4900) loss: 1.952015\n",
      "(Iteration 3801 / 4900) loss: 1.571406\n",
      "(Iteration 3901 / 4900) loss: 1.712253\n",
      "(Epoch 8 / 10) train acc: 0.523000; val_acc: 0.448000\n",
      "(Iteration 4001 / 4900) loss: 1.763641\n",
      "(Iteration 4101 / 4900) loss: 1.625897\n",
      "(Iteration 4201 / 4900) loss: 1.650644\n",
      "(Iteration 4301 / 4900) loss: 1.524544\n",
      "(Iteration 4401 / 4900) loss: 1.591006\n",
      "(Epoch 9 / 10) train acc: 0.498000; val_acc: 0.420000\n",
      "(Iteration 4501 / 4900) loss: 1.588174\n",
      "(Iteration 4601 / 4900) loss: 1.612033\n",
      "(Iteration 4701 / 4900) loss: 1.417016\n",
      "(Iteration 4801 / 4900) loss: 1.396883\n",
      "(Epoch 10 / 10) train acc: 0.481000; val_acc: 0.441000\n",
      "validataion accuracy  0.448\n",
      "\n",
      "37 th try:\n",
      "learning rate is  1.82916915729e-06\n",
      "regularization strengths is  0.332492373473\n",
      "weight scale is  0.0045435323316\n",
      "hidden size is  116\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 3.646881\n",
      "(Epoch 0 / 10) train acc: 0.086000; val_acc: 0.094000\n",
      "(Iteration 101 / 4900) loss: 3.640971\n",
      "(Iteration 201 / 4900) loss: 3.545193\n",
      "(Iteration 301 / 4900) loss: 3.591259\n",
      "(Iteration 401 / 4900) loss: 3.515247\n",
      "(Epoch 1 / 10) train acc: 0.141000; val_acc: 0.153000\n",
      "(Iteration 501 / 4900) loss: 3.518099\n",
      "(Iteration 601 / 4900) loss: 3.457016\n",
      "(Iteration 701 / 4900) loss: 3.592315\n",
      "(Iteration 801 / 4900) loss: 3.483094\n",
      "(Iteration 901 / 4900) loss: 3.530843\n",
      "(Epoch 2 / 10) train acc: 0.175000; val_acc: 0.182000\n",
      "(Iteration 1001 / 4900) loss: 3.490234\n",
      "(Iteration 1101 / 4900) loss: 3.358737\n",
      "(Iteration 1201 / 4900) loss: 3.404233\n",
      "(Iteration 1301 / 4900) loss: 3.512792\n",
      "(Iteration 1401 / 4900) loss: 3.351996\n",
      "(Epoch 3 / 10) train acc: 0.185000; val_acc: 0.208000\n",
      "(Iteration 1501 / 4900) loss: 3.482488\n",
      "(Iteration 1601 / 4900) loss: 3.380529\n",
      "(Iteration 1701 / 4900) loss: 3.421080\n",
      "(Iteration 1801 / 4900) loss: 3.338657\n",
      "(Iteration 1901 / 4900) loss: 3.362904\n",
      "(Epoch 4 / 10) train acc: 0.206000; val_acc: 0.216000\n",
      "(Iteration 2001 / 4900) loss: 3.447400\n",
      "(Iteration 2101 / 4900) loss: 3.417125\n",
      "(Iteration 2201 / 4900) loss: 3.439408\n",
      "(Iteration 2301 / 4900) loss: 3.265469\n",
      "(Iteration 2401 / 4900) loss: 3.350531\n",
      "(Epoch 5 / 10) train acc: 0.224000; val_acc: 0.236000\n",
      "(Iteration 2501 / 4900) loss: 3.325436\n",
      "(Iteration 2601 / 4900) loss: 3.408333\n",
      "(Iteration 2701 / 4900) loss: 3.386848\n",
      "(Iteration 2801 / 4900) loss: 3.318325\n",
      "(Iteration 2901 / 4900) loss: 3.343316\n",
      "(Epoch 6 / 10) train acc: 0.243000; val_acc: 0.245000\n",
      "(Iteration 3001 / 4900) loss: 3.373305\n",
      "(Iteration 3101 / 4900) loss: 3.326344\n",
      "(Iteration 3201 / 4900) loss: 3.258131\n",
      "(Iteration 3301 / 4900) loss: 3.320266\n",
      "(Iteration 3401 / 4900) loss: 3.268109\n",
      "(Epoch 7 / 10) train acc: 0.266000; val_acc: 0.266000\n",
      "(Iteration 3501 / 4900) loss: 3.367084\n",
      "(Iteration 3601 / 4900) loss: 3.280151\n",
      "(Iteration 3701 / 4900) loss: 3.320534\n",
      "(Iteration 3801 / 4900) loss: 3.174709\n",
      "(Iteration 3901 / 4900) loss: 3.248475\n",
      "(Epoch 8 / 10) train acc: 0.256000; val_acc: 0.281000\n",
      "(Iteration 4001 / 4900) loss: 3.325977\n",
      "(Iteration 4101 / 4900) loss: 3.225356\n",
      "(Iteration 4201 / 4900) loss: 3.221984\n",
      "(Iteration 4301 / 4900) loss: 3.189688\n",
      "(Iteration 4401 / 4900) loss: 3.325602\n",
      "(Epoch 9 / 10) train acc: 0.248000; val_acc: 0.290000\n",
      "(Iteration 4501 / 4900) loss: 3.237442\n",
      "(Iteration 4601 / 4900) loss: 3.220279\n",
      "(Iteration 4701 / 4900) loss: 3.349589\n",
      "(Iteration 4801 / 4900) loss: 3.241776\n",
      "(Epoch 10 / 10) train acc: 0.254000; val_acc: 0.294000\n",
      "validataion accuracy  0.294\n",
      "\n",
      "38 th try:\n",
      "learning rate is  7.29815394397e-05\n",
      "regularization strengths is  0.315403733388\n",
      "weight scale is  0.00399911665639\n",
      "hidden size is  163\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 3.727382\n",
      "(Epoch 0 / 10) train acc: 0.077000; val_acc: 0.078000\n",
      "(Iteration 101 / 4900) loss: 3.341318\n",
      "(Iteration 201 / 4900) loss: 3.125630\n",
      "(Iteration 301 / 4900) loss: 3.086767\n",
      "(Iteration 401 / 4900) loss: 3.233265\n",
      "(Epoch 1 / 10) train acc: 0.364000; val_acc: 0.385000\n",
      "(Iteration 501 / 4900) loss: 3.040930\n",
      "(Iteration 601 / 4900) loss: 2.865292\n",
      "(Iteration 701 / 4900) loss: 2.837665\n",
      "(Iteration 801 / 4900) loss: 2.943918\n",
      "(Iteration 901 / 4900) loss: 2.908882\n",
      "(Epoch 2 / 10) train acc: 0.421000; val_acc: 0.411000\n",
      "(Iteration 1001 / 4900) loss: 3.003156\n",
      "(Iteration 1101 / 4900) loss: 2.802074\n",
      "(Iteration 1201 / 4900) loss: 2.935998\n",
      "(Iteration 1301 / 4900) loss: 2.916652\n",
      "(Iteration 1401 / 4900) loss: 2.929120\n",
      "(Epoch 3 / 10) train acc: 0.423000; val_acc: 0.433000\n",
      "(Iteration 1501 / 4900) loss: 2.820815\n",
      "(Iteration 1601 / 4900) loss: 2.848854\n",
      "(Iteration 1701 / 4900) loss: 2.705309\n",
      "(Iteration 1801 / 4900) loss: 2.775274\n",
      "(Iteration 1901 / 4900) loss: 2.800001\n",
      "(Epoch 4 / 10) train acc: 0.437000; val_acc: 0.439000\n",
      "(Iteration 2001 / 4900) loss: 2.898598\n",
      "(Iteration 2101 / 4900) loss: 2.560076\n",
      "(Iteration 2201 / 4900) loss: 2.715298\n",
      "(Iteration 2301 / 4900) loss: 3.036194\n",
      "(Iteration 2401 / 4900) loss: 2.826232\n",
      "(Epoch 5 / 10) train acc: 0.435000; val_acc: 0.450000\n",
      "(Iteration 2501 / 4900) loss: 2.719478\n",
      "(Iteration 2601 / 4900) loss: 2.786127\n",
      "(Iteration 2701 / 4900) loss: 2.693911\n",
      "(Iteration 2801 / 4900) loss: 2.642491\n",
      "(Iteration 2901 / 4900) loss: 2.604992\n",
      "(Epoch 6 / 10) train acc: 0.449000; val_acc: 0.456000\n",
      "(Iteration 3001 / 4900) loss: 2.693206\n",
      "(Iteration 3101 / 4900) loss: 2.624135\n",
      "(Iteration 3201 / 4900) loss: 2.696387\n",
      "(Iteration 3301 / 4900) loss: 2.657177\n",
      "(Iteration 3401 / 4900) loss: 2.719493\n",
      "(Epoch 7 / 10) train acc: 0.457000; val_acc: 0.466000\n",
      "(Iteration 3501 / 4900) loss: 2.725091\n",
      "(Iteration 3601 / 4900) loss: 2.638584\n",
      "(Iteration 3701 / 4900) loss: 2.797126\n",
      "(Iteration 3801 / 4900) loss: 2.665754\n",
      "(Iteration 3901 / 4900) loss: 2.846501\n",
      "(Epoch 8 / 10) train acc: 0.442000; val_acc: 0.469000\n",
      "(Iteration 4001 / 4900) loss: 2.633700\n",
      "(Iteration 4101 / 4900) loss: 2.586625\n",
      "(Iteration 4201 / 4900) loss: 2.686465\n",
      "(Iteration 4301 / 4900) loss: 2.569377\n",
      "(Iteration 4401 / 4900) loss: 2.570643\n",
      "(Epoch 9 / 10) train acc: 0.476000; val_acc: 0.468000\n",
      "(Iteration 4501 / 4900) loss: 2.606611\n",
      "(Iteration 4601 / 4900) loss: 2.585451\n",
      "(Iteration 4701 / 4900) loss: 2.522825\n",
      "(Iteration 4801 / 4900) loss: 2.486338\n",
      "(Epoch 10 / 10) train acc: 0.494000; val_acc: 0.470000\n",
      "validataion accuracy  0.47\n",
      "\n",
      "39 th try:\n",
      "learning rate is  3.25028080819e-05\n",
      "regularization strengths is  0.0938716672896\n",
      "weight scale is  0.00141644886213\n",
      "hidden size is  50\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.319803\n",
      "(Epoch 0 / 10) train acc: 0.099000; val_acc: 0.092000\n",
      "(Iteration 101 / 4900) loss: 2.290731\n",
      "(Iteration 201 / 4900) loss: 2.280628\n",
      "(Iteration 301 / 4900) loss: 2.256122\n",
      "(Iteration 401 / 4900) loss: 2.244151\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.234000\n",
      "(Iteration 501 / 4900) loss: 2.169490\n",
      "(Iteration 601 / 4900) loss: 2.119428\n",
      "(Iteration 701 / 4900) loss: 2.099214\n",
      "(Iteration 801 / 4900) loss: 2.107470\n",
      "(Iteration 901 / 4900) loss: 2.053220\n",
      "(Epoch 2 / 10) train acc: 0.290000; val_acc: 0.290000\n",
      "(Iteration 1001 / 4900) loss: 1.947555\n",
      "(Iteration 1101 / 4900) loss: 2.068487\n",
      "(Iteration 1201 / 4900) loss: 1.936361\n",
      "(Iteration 1301 / 4900) loss: 1.844123\n",
      "(Iteration 1401 / 4900) loss: 1.957994\n",
      "(Epoch 3 / 10) train acc: 0.298000; val_acc: 0.314000\n",
      "(Iteration 1501 / 4900) loss: 1.884798\n",
      "(Iteration 1601 / 4900) loss: 1.984492\n",
      "(Iteration 1701 / 4900) loss: 1.937626\n",
      "(Iteration 1801 / 4900) loss: 1.870259\n",
      "(Iteration 1901 / 4900) loss: 1.880742\n",
      "(Epoch 4 / 10) train acc: 0.352000; val_acc: 0.338000\n",
      "(Iteration 2001 / 4900) loss: 2.020382\n",
      "(Iteration 2101 / 4900) loss: 1.861076\n",
      "(Iteration 2201 / 4900) loss: 1.974710\n",
      "(Iteration 2301 / 4900) loss: 1.685268\n",
      "(Iteration 2401 / 4900) loss: 2.000170\n",
      "(Epoch 5 / 10) train acc: 0.361000; val_acc: 0.350000\n",
      "(Iteration 2501 / 4900) loss: 1.816663\n",
      "(Iteration 2601 / 4900) loss: 1.815690\n",
      "(Iteration 2701 / 4900) loss: 1.745571\n",
      "(Iteration 2801 / 4900) loss: 1.800019\n",
      "(Iteration 2901 / 4900) loss: 1.890394\n",
      "(Epoch 6 / 10) train acc: 0.325000; val_acc: 0.364000\n",
      "(Iteration 3001 / 4900) loss: 1.823450\n",
      "(Iteration 3101 / 4900) loss: 1.832259\n",
      "(Iteration 3201 / 4900) loss: 1.783446\n",
      "(Iteration 3301 / 4900) loss: 1.789632\n",
      "(Iteration 3401 / 4900) loss: 1.811609\n",
      "(Epoch 7 / 10) train acc: 0.388000; val_acc: 0.374000\n",
      "(Iteration 3501 / 4900) loss: 1.854929\n",
      "(Iteration 3601 / 4900) loss: 1.909436\n",
      "(Iteration 3701 / 4900) loss: 1.731906\n",
      "(Iteration 3801 / 4900) loss: 1.890044\n",
      "(Iteration 3901 / 4900) loss: 1.792141\n",
      "(Epoch 8 / 10) train acc: 0.385000; val_acc: 0.382000\n",
      "(Iteration 4001 / 4900) loss: 1.667039\n",
      "(Iteration 4101 / 4900) loss: 1.731534\n",
      "(Iteration 4201 / 4900) loss: 1.669015\n",
      "(Iteration 4301 / 4900) loss: 1.829689\n",
      "(Iteration 4401 / 4900) loss: 1.558572\n",
      "(Epoch 9 / 10) train acc: 0.359000; val_acc: 0.389000\n",
      "(Iteration 4501 / 4900) loss: 1.788861\n",
      "(Iteration 4601 / 4900) loss: 1.796688\n",
      "(Iteration 4701 / 4900) loss: 1.810779\n",
      "(Iteration 4801 / 4900) loss: 1.709646\n",
      "(Epoch 10 / 10) train acc: 0.371000; val_acc: 0.395000\n",
      "validataion accuracy  0.395\n",
      "\n",
      "40 th try:\n",
      "learning rate is  0.000594526358502\n",
      "regularization strengths is  0.273410191008\n",
      "weight scale is  0.032320426172\n",
      "hidden size is  51\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 58.574312\n",
      "(Epoch 0 / 10) train acc: 0.120000; val_acc: 0.124000\n",
      "(Iteration 101 / 4900) loss: 25.669826\n",
      "(Iteration 201 / 4900) loss: 23.937554\n",
      "(Iteration 301 / 4900) loss: 22.388183\n",
      "(Iteration 401 / 4900) loss: 22.541112\n",
      "(Epoch 1 / 10) train acc: 0.287000; val_acc: 0.279000\n",
      "(Iteration 501 / 4900) loss: 21.352438\n",
      "(Iteration 601 / 4900) loss: 20.574047\n",
      "(Iteration 701 / 4900) loss: 19.892951\n",
      "(Iteration 801 / 4900) loss: 19.347137\n",
      "(Iteration 901 / 4900) loss: 18.814432\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.270000\n",
      "(Iteration 1001 / 4900) loss: 18.259433\n",
      "(Iteration 1101 / 4900) loss: 17.464381\n",
      "(Iteration 1201 / 4900) loss: 17.228038\n",
      "(Iteration 1301 / 4900) loss: 16.571666\n",
      "(Iteration 1401 / 4900) loss: 16.086538\n",
      "(Epoch 3 / 10) train acc: 0.341000; val_acc: 0.331000\n",
      "(Iteration 1501 / 4900) loss: 15.667437\n",
      "(Iteration 1601 / 4900) loss: 15.486271\n",
      "(Iteration 1701 / 4900) loss: 15.096287\n",
      "(Iteration 1801 / 4900) loss: 14.656840\n",
      "(Iteration 1901 / 4900) loss: 14.254647\n",
      "(Epoch 4 / 10) train acc: 0.321000; val_acc: 0.355000\n",
      "(Iteration 2001 / 4900) loss: 13.807029\n",
      "(Iteration 2101 / 4900) loss: 13.643266\n",
      "(Iteration 2201 / 4900) loss: 13.212505\n",
      "(Iteration 2301 / 4900) loss: 13.041079\n",
      "(Iteration 2401 / 4900) loss: 12.574064\n",
      "(Epoch 5 / 10) train acc: 0.395000; val_acc: 0.381000\n",
      "(Iteration 2501 / 4900) loss: 12.539863\n",
      "(Iteration 2601 / 4900) loss: 11.896498\n",
      "(Iteration 2701 / 4900) loss: 11.939841\n",
      "(Iteration 2801 / 4900) loss: 11.640770\n",
      "(Iteration 2901 / 4900) loss: 11.020559\n",
      "(Epoch 6 / 10) train acc: 0.387000; val_acc: 0.411000\n",
      "(Iteration 3001 / 4900) loss: 10.967431\n",
      "(Iteration 3101 / 4900) loss: 10.863304\n",
      "(Iteration 3201 / 4900) loss: 10.809162\n",
      "(Iteration 3301 / 4900) loss: 10.582777\n",
      "(Iteration 3401 / 4900) loss: 10.109888\n",
      "(Epoch 7 / 10) train acc: 0.391000; val_acc: 0.400000\n",
      "(Iteration 3501 / 4900) loss: 10.194024\n",
      "(Iteration 3601 / 4900) loss: 9.984705\n",
      "(Iteration 3701 / 4900) loss: 9.534155\n",
      "(Iteration 3801 / 4900) loss: 9.360605\n",
      "(Iteration 3901 / 4900) loss: 9.227432\n",
      "(Epoch 8 / 10) train acc: 0.394000; val_acc: 0.430000\n",
      "(Iteration 4001 / 4900) loss: 9.071920\n",
      "(Iteration 4101 / 4900) loss: 8.752487\n",
      "(Iteration 4201 / 4900) loss: 8.824531\n",
      "(Iteration 4301 / 4900) loss: 8.485078\n",
      "(Iteration 4401 / 4900) loss: 8.510904\n",
      "(Epoch 9 / 10) train acc: 0.417000; val_acc: 0.421000\n",
      "(Iteration 4501 / 4900) loss: 8.267954\n",
      "(Iteration 4601 / 4900) loss: 8.023308\n",
      "(Iteration 4701 / 4900) loss: 8.008044\n",
      "(Iteration 4801 / 4900) loss: 7.787344\n",
      "(Epoch 10 / 10) train acc: 0.440000; val_acc: 0.424000\n",
      "validataion accuracy  0.43\n",
      "\n",
      "41 th try:\n",
      "learning rate is  0.0216797070915\n",
      "regularization strengths is  0.0118856762672\n",
      "weight scale is  0.0366717252985\n",
      "hidden size is  102\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 44.461106\n",
      "(Epoch 0 / 10) train acc: 0.123000; val_acc: 0.115000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.082000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.124000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.090000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.090000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.093000; val_acc: 0.087000\n",
      "validataion accuracy  0.115\n",
      "\n",
      "42 th try:\n",
      "learning rate is  0.00380402581935\n",
      "regularization strengths is  0.756377361881\n",
      "weight scale is  0.00807459248875\n",
      "hidden size is  66\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 7.869927\n",
      "(Epoch 0 / 10) train acc: 0.148000; val_acc: 0.144000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.116000; val_acc: 0.124000\n",
      "(Iteration 501 / 4900) loss: inf\n",
      "(Iteration 601 / 4900) loss: inf\n",
      "(Iteration 701 / 4900) loss: inf\n",
      "(Iteration 801 / 4900) loss: inf\n",
      "(Iteration 901 / 4900) loss: inf\n",
      "(Epoch 2 / 10) train acc: 0.145000; val_acc: 0.133000\n",
      "(Iteration 1001 / 4900) loss: inf\n",
      "(Iteration 1101 / 4900) loss: inf\n",
      "(Iteration 1201 / 4900) loss: inf\n",
      "(Iteration 1301 / 4900) loss: inf\n",
      "(Iteration 1401 / 4900) loss: inf\n",
      "(Epoch 3 / 10) train acc: 0.155000; val_acc: 0.172000\n",
      "(Iteration 1501 / 4900) loss: inf\n",
      "(Iteration 1601 / 4900) loss: inf\n",
      "(Iteration 1701 / 4900) loss: inf\n",
      "(Iteration 1801 / 4900) loss: inf\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.101000; val_acc: 0.087000\n",
      "validataion accuracy  0.172\n",
      "\n",
      "43 th try:\n",
      "learning rate is  0.0019053723491\n",
      "regularization strengths is  0.578398253843\n",
      "weight scale is  0.0541213909889\n",
      "hidden size is  78\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 301.551422\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.135000\n",
      "(Iteration 101 / 4900) loss: 179.978612\n",
      "(Iteration 201 / 4900) loss: 138.019272\n",
      "(Iteration 301 / 4900) loss: 107.249010\n",
      "(Iteration 401 / 4900) loss: 85.466562\n",
      "(Epoch 1 / 10) train acc: 0.248000; val_acc: 0.279000\n",
      "(Iteration 501 / 4900) loss: 66.952333\n",
      "(Iteration 601 / 4900) loss: 54.411096\n",
      "(Iteration 701 / 4900) loss: 43.810069\n",
      "(Iteration 801 / 4900) loss: 36.111956\n",
      "(Iteration 901 / 4900) loss: 29.430233\n",
      "(Epoch 2 / 10) train acc: 0.324000; val_acc: 0.312000\n",
      "(Iteration 1001 / 4900) loss: 24.113924\n",
      "(Iteration 1101 / 4900) loss: 19.926501\n",
      "(Iteration 1201 / 4900) loss: 16.617786\n",
      "(Iteration 1301 / 4900) loss: 13.924087\n",
      "(Iteration 1401 / 4900) loss: 11.778757\n",
      "(Epoch 3 / 10) train acc: 0.427000; val_acc: 0.421000\n",
      "(Iteration 1501 / 4900) loss: 9.920306\n",
      "(Iteration 1601 / 4900) loss: 8.647625\n",
      "(Iteration 1701 / 4900) loss: 7.287439\n",
      "(Iteration 1801 / 4900) loss: 6.349618\n",
      "(Iteration 1901 / 4900) loss: 5.462660\n",
      "(Epoch 4 / 10) train acc: 0.448000; val_acc: 0.411000\n",
      "(Iteration 2001 / 4900) loss: 4.811182\n",
      "(Iteration 2101 / 4900) loss: 4.362771\n",
      "(Iteration 2201 / 4900) loss: 3.787850\n",
      "(Iteration 2301 / 4900) loss: 3.559602\n",
      "(Iteration 2401 / 4900) loss: 3.366162\n",
      "(Epoch 5 / 10) train acc: 0.442000; val_acc: 0.433000\n",
      "(Iteration 2501 / 4900) loss: 2.926796\n",
      "(Iteration 2601 / 4900) loss: 2.764209\n",
      "(Iteration 2701 / 4900) loss: 2.548617\n",
      "(Iteration 2801 / 4900) loss: 2.310160\n",
      "(Iteration 2901 / 4900) loss: 2.432938\n",
      "(Epoch 6 / 10) train acc: 0.491000; val_acc: 0.469000\n",
      "(Iteration 3001 / 4900) loss: 2.031279\n",
      "(Iteration 3101 / 4900) loss: 2.003544\n",
      "(Iteration 3201 / 4900) loss: 1.982017\n",
      "(Iteration 3301 / 4900) loss: 1.931043\n",
      "(Iteration 3401 / 4900) loss: 1.869007\n",
      "(Epoch 7 / 10) train acc: 0.483000; val_acc: 0.452000\n",
      "(Iteration 3501 / 4900) loss: 1.838971\n",
      "(Iteration 3601 / 4900) loss: 1.795805\n",
      "(Iteration 3701 / 4900) loss: 1.526286\n",
      "(Iteration 3801 / 4900) loss: 1.546215\n",
      "(Iteration 3901 / 4900) loss: 1.694694\n",
      "(Epoch 8 / 10) train acc: 0.499000; val_acc: 0.482000\n",
      "(Iteration 4001 / 4900) loss: 1.669895\n",
      "(Iteration 4101 / 4900) loss: 1.636611\n",
      "(Iteration 4201 / 4900) loss: 1.687772\n",
      "(Iteration 4301 / 4900) loss: 1.551188\n",
      "(Iteration 4401 / 4900) loss: 1.574277\n",
      "(Epoch 9 / 10) train acc: 0.496000; val_acc: 0.490000\n",
      "(Iteration 4501 / 4900) loss: 1.485212\n",
      "(Iteration 4601 / 4900) loss: 1.501933\n",
      "(Iteration 4701 / 4900) loss: 1.619398\n",
      "(Iteration 4801 / 4900) loss: 1.604560\n",
      "(Epoch 10 / 10) train acc: 0.509000; val_acc: 0.450000\n",
      "validataion accuracy  0.49\n",
      "\n",
      "44 th try:\n",
      "learning rate is  0.000107474131756\n",
      "regularization strengths is  0.0631479135194\n",
      "weight scale is  0.0264405100072\n",
      "hidden size is  239\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 50.995063\n",
      "(Epoch 0 / 10) train acc: 0.124000; val_acc: 0.112000\n",
      "(Iteration 101 / 4900) loss: 28.401792\n",
      "(Iteration 201 / 4900) loss: 24.708264\n",
      "(Iteration 301 / 4900) loss: 24.787948\n",
      "(Iteration 401 / 4900) loss: 22.117978\n",
      "(Epoch 1 / 10) train acc: 0.307000; val_acc: 0.298000\n",
      "(Iteration 501 / 4900) loss: 20.826607\n",
      "(Iteration 601 / 4900) loss: 21.532971\n",
      "(Iteration 701 / 4900) loss: 20.306651\n",
      "(Iteration 801 / 4900) loss: 19.907696\n",
      "(Iteration 901 / 4900) loss: 19.892762\n",
      "(Epoch 2 / 10) train acc: 0.320000; val_acc: 0.313000\n",
      "(Iteration 1001 / 4900) loss: 18.844130\n",
      "(Iteration 1101 / 4900) loss: 19.356874\n",
      "(Iteration 1201 / 4900) loss: 18.645149\n",
      "(Iteration 1301 / 4900) loss: 18.373625\n",
      "(Iteration 1401 / 4900) loss: 18.192998\n",
      "(Epoch 3 / 10) train acc: 0.327000; val_acc: 0.319000\n",
      "(Iteration 1501 / 4900) loss: 18.020302\n",
      "(Iteration 1601 / 4900) loss: 17.664159\n",
      "(Iteration 1701 / 4900) loss: 17.966366\n",
      "(Iteration 1801 / 4900) loss: 17.830598\n",
      "(Iteration 1901 / 4900) loss: 17.911237\n",
      "(Epoch 4 / 10) train acc: 0.316000; val_acc: 0.299000\n",
      "(Iteration 2001 / 4900) loss: 17.641413\n",
      "(Iteration 2101 / 4900) loss: 17.702549\n",
      "(Iteration 2201 / 4900) loss: 17.629868\n",
      "(Iteration 2301 / 4900) loss: 17.648460\n",
      "(Iteration 2401 / 4900) loss: 17.538515\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.322000\n",
      "(Iteration 2501 / 4900) loss: 17.577915\n",
      "(Iteration 2601 / 4900) loss: 17.329791\n",
      "(Iteration 2701 / 4900) loss: 17.453012\n",
      "(Iteration 2801 / 4900) loss: 17.457870\n",
      "(Iteration 2901 / 4900) loss: 17.368920\n",
      "(Epoch 6 / 10) train acc: 0.398000; val_acc: 0.364000\n",
      "(Iteration 3001 / 4900) loss: 17.402386\n",
      "(Iteration 3101 / 4900) loss: 17.252943\n",
      "(Iteration 3201 / 4900) loss: 17.453401\n",
      "(Iteration 3301 / 4900) loss: 16.868482\n",
      "(Iteration 3401 / 4900) loss: 17.232523\n",
      "(Epoch 7 / 10) train acc: 0.365000; val_acc: 0.351000\n",
      "(Iteration 3501 / 4900) loss: 17.050321\n",
      "(Iteration 3601 / 4900) loss: 17.350302\n",
      "(Iteration 3701 / 4900) loss: 17.102130\n",
      "(Iteration 3801 / 4900) loss: 17.174778\n",
      "(Iteration 3901 / 4900) loss: 17.059864\n",
      "(Epoch 8 / 10) train acc: 0.366000; val_acc: 0.387000\n",
      "(Iteration 4001 / 4900) loss: 17.242557\n",
      "(Iteration 4101 / 4900) loss: 16.807288\n",
      "(Iteration 4201 / 4900) loss: 16.992809\n",
      "(Iteration 4301 / 4900) loss: 17.176719\n",
      "(Iteration 4401 / 4900) loss: 16.885524\n",
      "(Epoch 9 / 10) train acc: 0.367000; val_acc: 0.363000\n",
      "(Iteration 4501 / 4900) loss: 17.166896\n",
      "(Iteration 4601 / 4900) loss: 16.870354\n",
      "(Iteration 4701 / 4900) loss: 16.955315\n",
      "(Iteration 4801 / 4900) loss: 17.125272\n",
      "(Epoch 10 / 10) train acc: 0.423000; val_acc: 0.385000\n",
      "validataion accuracy  0.387\n",
      "\n",
      "45 th try:\n",
      "learning rate is  0.0115166902135\n",
      "regularization strengths is  0.0044509959927\n",
      "weight scale is  0.00768799590667\n",
      "hidden size is  176\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 3.261840\n",
      "(Epoch 0 / 10) train acc: 0.139000; val_acc: 0.132000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "validataion accuracy  0.132\n",
      "\n",
      "46 th try:\n",
      "learning rate is  5.82838771988e-05\n",
      "regularization strengths is  0.00112996996632\n",
      "weight scale is  0.0521175722036\n",
      "hidden size is  177\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 127.196277\n",
      "(Epoch 0 / 10) train acc: 0.074000; val_acc: 0.098000\n",
      "(Iteration 101 / 4900) loss: 38.942915\n",
      "(Iteration 201 / 4900) loss: 51.042593\n",
      "(Iteration 301 / 4900) loss: 33.611507\n",
      "(Iteration 401 / 4900) loss: 31.000791\n",
      "(Epoch 1 / 10) train acc: 0.279000; val_acc: 0.273000\n",
      "(Iteration 501 / 4900) loss: 21.682489\n",
      "(Iteration 601 / 4900) loss: 24.947770\n",
      "(Iteration 701 / 4900) loss: 19.529894\n",
      "(Iteration 801 / 4900) loss: 19.623922\n",
      "(Iteration 901 / 4900) loss: 17.846678\n",
      "(Epoch 2 / 10) train acc: 0.287000; val_acc: 0.323000\n",
      "(Iteration 1001 / 4900) loss: 16.520623\n",
      "(Iteration 1101 / 4900) loss: 16.184835\n",
      "(Iteration 1201 / 4900) loss: 14.108626\n",
      "(Iteration 1301 / 4900) loss: 11.564302\n",
      "(Iteration 1401 / 4900) loss: 12.206444\n",
      "(Epoch 3 / 10) train acc: 0.330000; val_acc: 0.323000\n",
      "(Iteration 1501 / 4900) loss: 12.240902\n",
      "(Iteration 1601 / 4900) loss: 9.401900\n",
      "(Iteration 1701 / 4900) loss: 10.134538\n",
      "(Iteration 1801 / 4900) loss: 8.044924\n",
      "(Iteration 1901 / 4900) loss: 8.316438\n",
      "(Epoch 4 / 10) train acc: 0.315000; val_acc: 0.322000\n",
      "(Iteration 2001 / 4900) loss: 7.122285\n",
      "(Iteration 2101 / 4900) loss: 6.280750\n",
      "(Iteration 2201 / 4900) loss: 5.658679\n",
      "(Iteration 2301 / 4900) loss: 4.417192\n",
      "(Iteration 2401 / 4900) loss: 5.076543\n",
      "(Epoch 5 / 10) train acc: 0.324000; val_acc: 0.332000\n",
      "(Iteration 2501 / 4900) loss: 4.232449\n",
      "(Iteration 2601 / 4900) loss: 4.164115\n",
      "(Iteration 2701 / 4900) loss: 3.673321\n",
      "(Iteration 2801 / 4900) loss: 4.185667\n",
      "(Iteration 2901 / 4900) loss: 3.528115\n",
      "(Epoch 6 / 10) train acc: 0.338000; val_acc: 0.326000\n",
      "(Iteration 3001 / 4900) loss: 4.201906\n",
      "(Iteration 3101 / 4900) loss: 3.930072\n",
      "(Iteration 3201 / 4900) loss: 3.782043\n",
      "(Iteration 3301 / 4900) loss: 3.614951\n",
      "(Iteration 3401 / 4900) loss: 4.408582\n",
      "(Epoch 7 / 10) train acc: 0.301000; val_acc: 0.285000\n",
      "(Iteration 3501 / 4900) loss: 4.249805\n",
      "(Iteration 3601 / 4900) loss: 3.554182\n",
      "(Iteration 3701 / 4900) loss: 3.868873\n",
      "(Iteration 3801 / 4900) loss: 3.753756\n",
      "(Iteration 3901 / 4900) loss: 3.318691\n",
      "(Epoch 8 / 10) train acc: 0.307000; val_acc: 0.285000\n",
      "(Iteration 4001 / 4900) loss: 3.026592\n",
      "(Iteration 4101 / 4900) loss: 3.552917\n",
      "(Iteration 4201 / 4900) loss: 2.852948\n",
      "(Iteration 4301 / 4900) loss: 3.414707\n",
      "(Iteration 4401 / 4900) loss: 3.352265\n",
      "(Epoch 9 / 10) train acc: 0.318000; val_acc: 0.320000\n",
      "(Iteration 4501 / 4900) loss: 3.502553\n",
      "(Iteration 4601 / 4900) loss: 3.066217\n",
      "(Iteration 4701 / 4900) loss: 3.503117\n",
      "(Iteration 4801 / 4900) loss: 3.010791\n",
      "(Epoch 10 / 10) train acc: 0.286000; val_acc: 0.315000\n",
      "validataion accuracy  0.332\n",
      "\n",
      "47 th try:\n",
      "learning rate is  7.02501233479e-06\n",
      "regularization strengths is  0.235422381559\n",
      "weight scale is  0.00197275518611\n",
      "hidden size is  142\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.530180\n",
      "(Epoch 0 / 10) train acc: 0.081000; val_acc: 0.068000\n",
      "(Iteration 101 / 4900) loss: 2.515598\n",
      "(Iteration 201 / 4900) loss: 2.475861\n",
      "(Iteration 301 / 4900) loss: 2.481245\n",
      "(Iteration 401 / 4900) loss: 2.470588\n",
      "(Epoch 1 / 10) train acc: 0.199000; val_acc: 0.211000\n",
      "(Iteration 501 / 4900) loss: 2.434111\n",
      "(Iteration 601 / 4900) loss: 2.429154\n",
      "(Iteration 701 / 4900) loss: 2.403187\n",
      "(Iteration 801 / 4900) loss: 2.441839\n",
      "(Iteration 901 / 4900) loss: 2.403011\n",
      "(Epoch 2 / 10) train acc: 0.236000; val_acc: 0.253000\n",
      "(Iteration 1001 / 4900) loss: 2.393246\n",
      "(Iteration 1101 / 4900) loss: 2.372576\n",
      "(Iteration 1201 / 4900) loss: 2.335758\n",
      "(Iteration 1301 / 4900) loss: 2.350265\n",
      "(Iteration 1401 / 4900) loss: 2.317732\n",
      "(Epoch 3 / 10) train acc: 0.270000; val_acc: 0.270000\n",
      "(Iteration 1501 / 4900) loss: 2.357118\n",
      "(Iteration 1601 / 4900) loss: 2.299727\n",
      "(Iteration 1701 / 4900) loss: 2.325728\n",
      "(Iteration 1801 / 4900) loss: 2.314401\n",
      "(Iteration 1901 / 4900) loss: 2.294140\n",
      "(Epoch 4 / 10) train acc: 0.271000; val_acc: 0.283000\n",
      "(Iteration 2001 / 4900) loss: 2.306561\n",
      "(Iteration 2101 / 4900) loss: 2.206522\n",
      "(Iteration 2201 / 4900) loss: 2.307353\n",
      "(Iteration 2301 / 4900) loss: 2.194313\n",
      "(Iteration 2401 / 4900) loss: 2.297650\n",
      "(Epoch 5 / 10) train acc: 0.277000; val_acc: 0.295000\n",
      "(Iteration 2501 / 4900) loss: 2.302282\n",
      "(Iteration 2601 / 4900) loss: 2.218416\n",
      "(Iteration 2701 / 4900) loss: 2.279926\n",
      "(Iteration 2801 / 4900) loss: 2.271131\n",
      "(Iteration 2901 / 4900) loss: 2.183751\n",
      "(Epoch 6 / 10) train acc: 0.265000; val_acc: 0.305000\n",
      "(Iteration 3001 / 4900) loss: 2.308053\n",
      "(Iteration 3101 / 4900) loss: 2.218852\n",
      "(Iteration 3201 / 4900) loss: 2.256127\n",
      "(Iteration 3301 / 4900) loss: 2.259878\n",
      "(Iteration 3401 / 4900) loss: 2.249613\n",
      "(Epoch 7 / 10) train acc: 0.295000; val_acc: 0.302000\n",
      "(Iteration 3501 / 4900) loss: 2.243619\n",
      "(Iteration 3601 / 4900) loss: 2.292032\n",
      "(Iteration 3701 / 4900) loss: 2.103953\n",
      "(Iteration 3801 / 4900) loss: 2.191388\n",
      "(Iteration 3901 / 4900) loss: 2.222624\n",
      "(Epoch 8 / 10) train acc: 0.316000; val_acc: 0.304000\n",
      "(Iteration 4001 / 4900) loss: 2.228811\n",
      "(Iteration 4101 / 4900) loss: 2.129512\n",
      "(Iteration 4201 / 4900) loss: 2.187554\n",
      "(Iteration 4301 / 4900) loss: 2.238440\n",
      "(Iteration 4401 / 4900) loss: 2.180339\n",
      "(Epoch 9 / 10) train acc: 0.312000; val_acc: 0.307000\n",
      "(Iteration 4501 / 4900) loss: 2.148267\n",
      "(Iteration 4601 / 4900) loss: 2.135954\n",
      "(Iteration 4701 / 4900) loss: 2.165958\n",
      "(Iteration 4801 / 4900) loss: 2.041163\n",
      "(Epoch 10 / 10) train acc: 0.310000; val_acc: 0.310000\n",
      "validataion accuracy  0.31\n",
      "\n",
      "48 th try:\n",
      "learning rate is  0.000330510585277\n",
      "regularization strengths is  0.188885826962\n",
      "weight scale is  0.0392260661068\n",
      "hidden size is  187\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 159.688863\n",
      "(Epoch 0 / 10) train acc: 0.140000; val_acc: 0.157000\n",
      "(Iteration 101 / 4900) loss: 103.076540\n",
      "(Iteration 201 / 4900) loss: 93.147407\n",
      "(Iteration 301 / 4900) loss: 89.996140\n",
      "(Iteration 401 / 4900) loss: 85.928133\n",
      "(Epoch 1 / 10) train acc: 0.338000; val_acc: 0.337000\n",
      "(Iteration 501 / 4900) loss: 88.799753\n",
      "(Iteration 601 / 4900) loss: 85.105286\n",
      "(Iteration 701 / 4900) loss: 82.700158\n",
      "(Iteration 801 / 4900) loss: 80.311365\n",
      "(Iteration 901 / 4900) loss: 79.470223\n",
      "(Epoch 2 / 10) train acc: 0.333000; val_acc: 0.351000\n",
      "(Iteration 1001 / 4900) loss: 78.555861\n",
      "(Iteration 1101 / 4900) loss: 78.923117\n",
      "(Iteration 1201 / 4900) loss: 81.848139\n",
      "(Iteration 1301 / 4900) loss: 75.715239\n",
      "(Iteration 1401 / 4900) loss: 76.959683\n",
      "(Epoch 3 / 10) train acc: 0.262000; val_acc: 0.281000\n",
      "(Iteration 1501 / 4900) loss: 73.387650\n",
      "(Iteration 1601 / 4900) loss: 72.954299\n",
      "(Iteration 1701 / 4900) loss: 71.685381\n",
      "(Iteration 1801 / 4900) loss: 71.003261\n",
      "(Iteration 1901 / 4900) loss: 69.420745\n",
      "(Epoch 4 / 10) train acc: 0.227000; val_acc: 0.247000\n",
      "(Iteration 2001 / 4900) loss: 69.241473\n",
      "(Iteration 2101 / 4900) loss: 69.059993\n",
      "(Iteration 2201 / 4900) loss: 67.350867\n",
      "(Iteration 2301 / 4900) loss: 66.977275\n",
      "(Iteration 2401 / 4900) loss: 66.998565\n",
      "(Epoch 5 / 10) train acc: 0.323000; val_acc: 0.303000\n",
      "(Iteration 2501 / 4900) loss: 64.772833\n",
      "(Iteration 2601 / 4900) loss: 64.129630\n",
      "(Iteration 2701 / 4900) loss: 64.079962\n",
      "(Iteration 2801 / 4900) loss: 62.591576\n",
      "(Iteration 2901 / 4900) loss: 62.600341\n",
      "(Epoch 6 / 10) train acc: 0.337000; val_acc: 0.355000\n",
      "(Iteration 3001 / 4900) loss: 61.830513\n",
      "(Iteration 3101 / 4900) loss: 60.942967\n",
      "(Iteration 3201 / 4900) loss: 60.466026\n",
      "(Iteration 3301 / 4900) loss: 59.737159\n",
      "(Iteration 3401 / 4900) loss: 58.972697\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.354000\n",
      "(Iteration 3501 / 4900) loss: 58.401635\n",
      "(Iteration 3601 / 4900) loss: 58.388835\n",
      "(Iteration 3701 / 4900) loss: 57.788137\n",
      "(Iteration 3801 / 4900) loss: 57.345065\n",
      "(Iteration 3901 / 4900) loss: 56.603025\n",
      "(Epoch 8 / 10) train acc: 0.345000; val_acc: 0.363000\n",
      "(Iteration 4001 / 4900) loss: 56.551772\n",
      "(Iteration 4101 / 4900) loss: 55.532501\n",
      "(Iteration 4201 / 4900) loss: 55.037913\n",
      "(Iteration 4301 / 4900) loss: 55.046279\n",
      "(Iteration 4401 / 4900) loss: 54.538117\n",
      "(Epoch 9 / 10) train acc: 0.331000; val_acc: 0.347000\n",
      "(Iteration 4501 / 4900) loss: 54.098841\n",
      "(Iteration 4601 / 4900) loss: 53.654734\n",
      "(Iteration 4701 / 4900) loss: 53.091349\n",
      "(Iteration 4801 / 4900) loss: 52.690693\n",
      "(Epoch 10 / 10) train acc: 0.363000; val_acc: 0.373000\n",
      "validataion accuracy  0.373\n",
      "\n",
      "49 th try:\n",
      "learning rate is  0.0285018302418\n",
      "regularization strengths is  0.616908877268\n",
      "weight scale is  0.00116359099555\n",
      "hidden size is  95\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.429247\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.136000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.097000; val_acc: 0.087000\n",
      "validataion accuracy  0.136\n",
      "\n",
      "50 th try:\n",
      "learning rate is  0.0010063813613\n",
      "regularization strengths is  0.164919332842\n",
      "weight scale is  0.074272234501\n",
      "hidden size is  72\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 244.801122\n",
      "(Epoch 0 / 10) train acc: 0.142000; val_acc: 0.142000\n",
      "(Iteration 101 / 4900) loss: 140.054171\n",
      "(Iteration 201 / 4900) loss: 123.817269\n",
      "(Iteration 301 / 4900) loss: 105.084714\n",
      "(Iteration 401 / 4900) loss: 100.952354\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.201000\n",
      "(Iteration 501 / 4900) loss: 105.089819\n",
      "(Iteration 601 / 4900) loss: 91.930367\n",
      "(Iteration 701 / 4900) loss: 88.981379\n",
      "(Iteration 801 / 4900) loss: 84.329894\n",
      "(Iteration 901 / 4900) loss: 85.175408\n",
      "(Epoch 2 / 10) train acc: 0.230000; val_acc: 0.215000\n",
      "(Iteration 1001 / 4900) loss: 83.386347\n",
      "(Iteration 1101 / 4900) loss: 77.043514\n",
      "(Iteration 1201 / 4900) loss: 72.696728\n",
      "(Iteration 1301 / 4900) loss: 69.410159\n",
      "(Iteration 1401 / 4900) loss: 69.273532\n",
      "(Epoch 3 / 10) train acc: 0.182000; val_acc: 0.192000\n",
      "(Iteration 1501 / 4900) loss: 64.041662\n",
      "(Iteration 1601 / 4900) loss: 61.788935\n",
      "(Iteration 1701 / 4900) loss: 60.448899\n",
      "(Iteration 1801 / 4900) loss: 60.100673\n",
      "(Iteration 1901 / 4900) loss: 58.440052\n",
      "(Epoch 4 / 10) train acc: 0.293000; val_acc: 0.258000\n",
      "(Iteration 2001 / 4900) loss: 55.216445\n",
      "(Iteration 2101 / 4900) loss: 55.247416\n",
      "(Iteration 2201 / 4900) loss: 52.696503\n",
      "(Iteration 2301 / 4900) loss: 51.969485\n",
      "(Iteration 2401 / 4900) loss: 50.121935\n",
      "(Epoch 5 / 10) train acc: 0.284000; val_acc: 0.243000\n",
      "(Iteration 2501 / 4900) loss: 48.478115\n",
      "(Iteration 2601 / 4900) loss: 46.792992\n",
      "(Iteration 2701 / 4900) loss: 46.071308\n",
      "(Iteration 2801 / 4900) loss: 44.578163\n",
      "(Iteration 2901 / 4900) loss: 43.233511\n",
      "(Epoch 6 / 10) train acc: 0.299000; val_acc: 0.282000\n",
      "(Iteration 3001 / 4900) loss: 42.993047\n",
      "(Iteration 3101 / 4900) loss: 41.290029\n",
      "(Iteration 3201 / 4900) loss: 40.256614\n",
      "(Iteration 3301 / 4900) loss: 38.608524\n",
      "(Iteration 3401 / 4900) loss: 38.165475\n",
      "(Epoch 7 / 10) train acc: 0.340000; val_acc: 0.289000\n",
      "(Iteration 3501 / 4900) loss: 36.994758\n",
      "(Iteration 3601 / 4900) loss: 36.494058\n",
      "(Iteration 3701 / 4900) loss: 35.354730\n",
      "(Iteration 3801 / 4900) loss: 34.218783\n",
      "(Iteration 3901 / 4900) loss: 33.599752\n",
      "(Epoch 8 / 10) train acc: 0.355000; val_acc: 0.330000\n",
      "(Iteration 4001 / 4900) loss: 32.858385\n",
      "(Iteration 4101 / 4900) loss: 32.356900\n",
      "(Iteration 4201 / 4900) loss: 31.807643\n",
      "(Iteration 4301 / 4900) loss: 31.052690\n",
      "(Iteration 4401 / 4900) loss: 30.363346\n",
      "(Epoch 9 / 10) train acc: 0.333000; val_acc: 0.311000\n",
      "(Iteration 4501 / 4900) loss: 29.975302\n",
      "(Iteration 4601 / 4900) loss: 29.127451\n",
      "(Iteration 4701 / 4900) loss: 28.730393\n",
      "(Iteration 4801 / 4900) loss: 27.792500\n",
      "(Epoch 10 / 10) train acc: 0.345000; val_acc: 0.351000\n",
      "validataion accuracy  0.351\n",
      "\n",
      "51 th try:\n",
      "learning rate is  0.00289210585461\n",
      "regularization strengths is  0.0508655862035\n",
      "weight scale is  0.00805878765429\n",
      "hidden size is  181\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 5.519987\n",
      "(Epoch 0 / 10) train acc: 0.225000; val_acc: 0.201000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.163000; val_acc: 0.185000\n",
      "(Iteration 501 / 4900) loss: inf\n",
      "(Iteration 601 / 4900) loss: inf\n",
      "(Iteration 701 / 4900) loss: inf\n",
      "(Iteration 801 / 4900) loss: inf\n",
      "(Iteration 901 / 4900) loss: inf\n",
      "(Epoch 2 / 10) train acc: 0.122000; val_acc: 0.130000\n",
      "(Iteration 1001 / 4900) loss: inf\n",
      "(Iteration 1101 / 4900) loss: inf\n",
      "(Iteration 1201 / 4900) loss: inf\n",
      "(Iteration 1301 / 4900) loss: inf\n",
      "(Iteration 1401 / 4900) loss: inf\n",
      "(Epoch 3 / 10) train acc: 0.165000; val_acc: 0.186000\n",
      "(Iteration 1501 / 4900) loss: inf\n",
      "(Iteration 1601 / 4900) loss: inf\n",
      "(Iteration 1701 / 4900) loss: inf\n",
      "(Iteration 1801 / 4900) loss: inf\n",
      "(Iteration 1901 / 4900) loss: inf\n",
      "(Epoch 4 / 10) train acc: 0.177000; val_acc: 0.159000\n",
      "(Iteration 2001 / 4900) loss: inf\n",
      "(Iteration 2101 / 4900) loss: inf\n",
      "(Iteration 2201 / 4900) loss: inf\n",
      "(Iteration 2301 / 4900) loss: inf\n",
      "(Iteration 2401 / 4900) loss: inf\n",
      "(Epoch 5 / 10) train acc: 0.161000; val_acc: 0.148000\n",
      "(Iteration 2501 / 4900) loss: inf\n",
      "(Iteration 2601 / 4900) loss: inf\n",
      "(Iteration 2701 / 4900) loss: inf\n",
      "(Iteration 2801 / 4900) loss: inf\n",
      "(Iteration 2901 / 4900) loss: inf\n",
      "(Epoch 6 / 10) train acc: 0.232000; val_acc: 0.236000\n",
      "(Iteration 3001 / 4900) loss: inf\n",
      "(Iteration 3101 / 4900) loss: inf\n",
      "(Iteration 3201 / 4900) loss: inf\n",
      "(Iteration 3301 / 4900) loss: inf\n",
      "(Iteration 3401 / 4900) loss: inf\n",
      "(Epoch 7 / 10) train acc: 0.213000; val_acc: 0.232000\n",
      "(Iteration 3501 / 4900) loss: inf\n",
      "(Iteration 3601 / 4900) loss: inf\n",
      "(Iteration 3701 / 4900) loss: inf\n",
      "(Iteration 3801 / 4900) loss: inf\n",
      "(Iteration 3901 / 4900) loss: inf\n",
      "(Epoch 8 / 10) train acc: 0.205000; val_acc: 0.213000\n",
      "(Iteration 4001 / 4900) loss: inf\n",
      "(Iteration 4101 / 4900) loss: inf\n",
      "(Iteration 4201 / 4900) loss: inf\n",
      "(Iteration 4301 / 4900) loss: inf\n",
      "(Iteration 4401 / 4900) loss: inf\n",
      "(Epoch 9 / 10) train acc: 0.284000; val_acc: 0.264000\n",
      "(Iteration 4501 / 4900) loss: inf\n",
      "(Iteration 4601 / 4900) loss: inf\n",
      "(Iteration 4701 / 4900) loss: inf\n",
      "(Iteration 4801 / 4900) loss: inf\n",
      "(Epoch 10 / 10) train acc: 0.287000; val_acc: 0.297000\n",
      "validataion accuracy  0.297\n",
      "\n",
      "52 th try:\n",
      "learning rate is  0.000173369205923\n",
      "regularization strengths is  0.149768106492\n",
      "weight scale is  0.096557782411\n",
      "hidden size is  92\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: inf\n",
      "(Epoch 0 / 10) train acc: 0.116000; val_acc: 0.106000\n",
      "(Iteration 101 / 4900) loss: 273.423924\n",
      "(Iteration 201 / 4900) loss: 246.608879\n",
      "(Iteration 301 / 4900) loss: 228.153594\n",
      "(Iteration 401 / 4900) loss: 215.926572\n",
      "(Epoch 1 / 10) train acc: 0.273000; val_acc: 0.267000\n",
      "(Iteration 501 / 4900) loss: 210.863871\n",
      "(Iteration 601 / 4900) loss: 207.035713\n",
      "(Iteration 701 / 4900) loss: 205.199394\n",
      "(Iteration 801 / 4900) loss: 203.763057\n",
      "(Iteration 901 / 4900) loss: 205.755547\n",
      "(Epoch 2 / 10) train acc: 0.242000; val_acc: 0.267000\n",
      "(Iteration 1001 / 4900) loss: 202.673090\n",
      "(Iteration 1101 / 4900) loss: 206.446550\n",
      "(Iteration 1201 / 4900) loss: 202.483451\n",
      "(Iteration 1301 / 4900) loss: 202.306910\n",
      "(Iteration 1401 / 4900) loss: 200.022324\n",
      "(Epoch 3 / 10) train acc: 0.226000; val_acc: 0.202000\n",
      "(Iteration 1501 / 4900) loss: 190.092460\n",
      "(Iteration 1601 / 4900) loss: 194.576532\n",
      "(Iteration 1701 / 4900) loss: 195.273403\n",
      "(Iteration 1801 / 4900) loss: 189.721505\n",
      "(Iteration 1901 / 4900) loss: 195.150075\n",
      "(Epoch 4 / 10) train acc: 0.209000; val_acc: 0.209000\n",
      "(Iteration 2001 / 4900) loss: 185.700071\n",
      "(Iteration 2101 / 4900) loss: 191.568595\n",
      "(Iteration 2201 / 4900) loss: 189.911059\n",
      "(Iteration 2301 / 4900) loss: 187.830042\n",
      "(Iteration 2401 / 4900) loss: 187.636470\n",
      "(Epoch 5 / 10) train acc: 0.229000; val_acc: 0.258000\n",
      "(Iteration 2501 / 4900) loss: 185.035077\n",
      "(Iteration 2601 / 4900) loss: 180.342370\n",
      "(Iteration 2701 / 4900) loss: 182.787383\n",
      "(Iteration 2801 / 4900) loss: 179.310342\n",
      "(Iteration 2901 / 4900) loss: 177.959917\n",
      "(Epoch 6 / 10) train acc: 0.213000; val_acc: 0.253000\n",
      "(Iteration 3001 / 4900) loss: 175.767606\n",
      "(Iteration 3101 / 4900) loss: 179.511048\n",
      "(Iteration 3201 / 4900) loss: 178.129935\n",
      "(Iteration 3301 / 4900) loss: 178.729510\n",
      "(Iteration 3401 / 4900) loss: 175.912814\n",
      "(Epoch 7 / 10) train acc: 0.220000; val_acc: 0.259000\n",
      "(Iteration 3501 / 4900) loss: 177.718699\n",
      "(Iteration 3601 / 4900) loss: 176.987484\n",
      "(Iteration 3701 / 4900) loss: 173.655728\n",
      "(Iteration 3801 / 4900) loss: 173.569567\n",
      "(Iteration 3901 / 4900) loss: 170.998524\n",
      "(Epoch 8 / 10) train acc: 0.233000; val_acc: 0.262000\n",
      "(Iteration 4001 / 4900) loss: 174.236267\n",
      "(Iteration 4101 / 4900) loss: 168.942245\n",
      "(Iteration 4201 / 4900) loss: 168.500159\n",
      "(Iteration 4301 / 4900) loss: 168.527906\n",
      "(Iteration 4401 / 4900) loss: 170.171916\n",
      "(Epoch 9 / 10) train acc: 0.221000; val_acc: 0.218000\n",
      "(Iteration 4501 / 4900) loss: 168.682362\n",
      "(Iteration 4601 / 4900) loss: 165.308938\n",
      "(Iteration 4701 / 4900) loss: 167.472525\n",
      "(Iteration 4801 / 4900) loss: 165.747576\n",
      "(Epoch 10 / 10) train acc: 0.176000; val_acc: 0.181000\n",
      "validataion accuracy  0.267\n",
      "\n",
      "53 th try:\n",
      "learning rate is  0.0245665880155\n",
      "regularization strengths is  0.97964245543\n",
      "weight scale is  0.0116758821054\n",
      "hidden size is  89\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 24.513827\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.094000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.090000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.099000; val_acc: 0.087000\n",
      "validataion accuracy  0.094\n",
      "\n",
      "54 th try:\n",
      "learning rate is  0.000635812651627\n",
      "regularization strengths is  0.249615525412\n",
      "weight scale is  0.00401116909948\n",
      "hidden size is  75\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.790848\n",
      "(Epoch 0 / 10) train acc: 0.093000; val_acc: 0.114000\n",
      "(Iteration 101 / 4900) loss: 2.147211\n",
      "(Iteration 201 / 4900) loss: 2.199689\n",
      "(Iteration 301 / 4900) loss: 2.157409\n",
      "(Iteration 401 / 4900) loss: 2.048473\n",
      "(Epoch 1 / 10) train acc: 0.410000; val_acc: 0.422000\n",
      "(Iteration 501 / 4900) loss: 1.976027\n",
      "(Iteration 601 / 4900) loss: 1.967015\n",
      "(Iteration 701 / 4900) loss: 2.004742\n",
      "(Iteration 801 / 4900) loss: 1.759876\n",
      "(Iteration 901 / 4900) loss: 1.762303\n",
      "(Epoch 2 / 10) train acc: 0.502000; val_acc: 0.452000\n",
      "(Iteration 1001 / 4900) loss: 1.812814\n",
      "(Iteration 1101 / 4900) loss: 1.925171\n",
      "(Iteration 1201 / 4900) loss: 1.709103\n",
      "(Iteration 1301 / 4900) loss: 1.798179\n",
      "(Iteration 1401 / 4900) loss: 1.812987\n",
      "(Epoch 3 / 10) train acc: 0.505000; val_acc: 0.475000\n",
      "(Iteration 1501 / 4900) loss: 1.865278\n",
      "(Iteration 1601 / 4900) loss: 1.721136\n",
      "(Iteration 1701 / 4900) loss: 1.726276\n",
      "(Iteration 1801 / 4900) loss: 1.684620\n",
      "(Iteration 1901 / 4900) loss: 1.593688\n",
      "(Epoch 4 / 10) train acc: 0.498000; val_acc: 0.496000\n",
      "(Iteration 2001 / 4900) loss: 1.579338\n",
      "(Iteration 2101 / 4900) loss: 1.717506\n",
      "(Iteration 2201 / 4900) loss: 1.575579\n",
      "(Iteration 2301 / 4900) loss: 1.602648\n",
      "(Iteration 2401 / 4900) loss: 1.757516\n",
      "(Epoch 5 / 10) train acc: 0.521000; val_acc: 0.491000\n",
      "(Iteration 2501 / 4900) loss: 1.825755\n",
      "(Iteration 2601 / 4900) loss: 1.496941\n",
      "(Iteration 2701 / 4900) loss: 1.502870\n",
      "(Iteration 2801 / 4900) loss: 1.596301\n",
      "(Iteration 2901 / 4900) loss: 1.542514\n",
      "(Epoch 6 / 10) train acc: 0.522000; val_acc: 0.493000\n",
      "(Iteration 3001 / 4900) loss: 1.653737\n",
      "(Iteration 3101 / 4900) loss: 1.622525\n",
      "(Iteration 3201 / 4900) loss: 1.515812\n",
      "(Iteration 3301 / 4900) loss: 1.588116\n",
      "(Iteration 3401 / 4900) loss: 1.538246\n",
      "(Epoch 7 / 10) train acc: 0.561000; val_acc: 0.482000\n",
      "(Iteration 3501 / 4900) loss: 1.475253\n",
      "(Iteration 3601 / 4900) loss: 1.579886\n",
      "(Iteration 3701 / 4900) loss: 1.600258\n",
      "(Iteration 3801 / 4900) loss: 1.482184\n",
      "(Iteration 3901 / 4900) loss: 1.565634\n",
      "(Epoch 8 / 10) train acc: 0.558000; val_acc: 0.499000\n",
      "(Iteration 4001 / 4900) loss: 1.463478\n",
      "(Iteration 4101 / 4900) loss: 1.428715\n",
      "(Iteration 4201 / 4900) loss: 1.582081\n",
      "(Iteration 4301 / 4900) loss: 1.445325\n",
      "(Iteration 4401 / 4900) loss: 1.460950\n",
      "(Epoch 9 / 10) train acc: 0.567000; val_acc: 0.497000\n",
      "(Iteration 4501 / 4900) loss: 1.309709\n",
      "(Iteration 4601 / 4900) loss: 1.512467\n",
      "(Iteration 4701 / 4900) loss: 1.388842\n",
      "(Iteration 4801 / 4900) loss: 1.592437\n",
      "(Epoch 10 / 10) train acc: 0.571000; val_acc: 0.490000\n",
      "validataion accuracy  0.499\n",
      "\n",
      "55 th try:\n",
      "learning rate is  0.00421885822245\n",
      "regularization strengths is  0.045205817209\n",
      "weight scale is  0.0588397944705\n",
      "hidden size is  110\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 155.467252\n",
      "(Epoch 0 / 10) train acc: 0.159000; val_acc: 0.156000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.110000; val_acc: 0.111000\n",
      "(Iteration 501 / 4900) loss: inf\n",
      "(Iteration 601 / 4900) loss: inf\n",
      "(Iteration 701 / 4900) loss: inf\n",
      "(Iteration 801 / 4900) loss: inf\n",
      "(Iteration 901 / 4900) loss: inf\n",
      "(Epoch 2 / 10) train acc: 0.089000; val_acc: 0.102000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.079000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.090000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "validataion accuracy  0.156\n",
      "\n",
      "56 th try:\n",
      "learning rate is  0.00684293341163\n",
      "regularization strengths is  0.0109761078865\n",
      "weight scale is  0.00100270459898\n",
      "hidden size is  107\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.307080\n",
      "(Epoch 0 / 10) train acc: 0.204000; val_acc: 0.201000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.078000; val_acc: 0.087000\n",
      "validataion accuracy  0.201\n",
      "\n",
      "57 th try:\n",
      "learning rate is  1.92979323604e-06\n",
      "regularization strengths is  0.170843791255\n",
      "weight scale is  0.00305097975114\n",
      "hidden size is  113\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.617369\n",
      "(Epoch 0 / 10) train acc: 0.078000; val_acc: 0.081000\n",
      "(Iteration 101 / 4900) loss: 2.623366\n",
      "(Iteration 201 / 4900) loss: 2.589519\n",
      "(Iteration 301 / 4900) loss: 2.592593\n",
      "(Iteration 401 / 4900) loss: 2.611997\n",
      "(Epoch 1 / 10) train acc: 0.127000; val_acc: 0.098000\n",
      "(Iteration 501 / 4900) loss: 2.581189\n",
      "(Iteration 601 / 4900) loss: 2.592838\n",
      "(Iteration 701 / 4900) loss: 2.546969\n",
      "(Iteration 801 / 4900) loss: 2.552608\n",
      "(Iteration 901 / 4900) loss: 2.520194\n",
      "(Epoch 2 / 10) train acc: 0.150000; val_acc: 0.132000\n",
      "(Iteration 1001 / 4900) loss: 2.511660\n",
      "(Iteration 1101 / 4900) loss: 2.525963\n",
      "(Iteration 1201 / 4900) loss: 2.517818\n",
      "(Iteration 1301 / 4900) loss: 2.541269\n",
      "(Iteration 1401 / 4900) loss: 2.515873\n",
      "(Epoch 3 / 10) train acc: 0.165000; val_acc: 0.160000\n",
      "(Iteration 1501 / 4900) loss: 2.527104\n",
      "(Iteration 1601 / 4900) loss: 2.536081\n",
      "(Iteration 1701 / 4900) loss: 2.471549\n",
      "(Iteration 1801 / 4900) loss: 2.526725\n",
      "(Iteration 1901 / 4900) loss: 2.493803\n",
      "(Epoch 4 / 10) train acc: 0.208000; val_acc: 0.186000\n",
      "(Iteration 2001 / 4900) loss: 2.521948\n",
      "(Iteration 2101 / 4900) loss: 2.511560\n",
      "(Iteration 2201 / 4900) loss: 2.506201\n",
      "(Iteration 2301 / 4900) loss: 2.474782\n",
      "(Iteration 2401 / 4900) loss: 2.470789\n",
      "(Epoch 5 / 10) train acc: 0.234000; val_acc: 0.201000\n",
      "(Iteration 2501 / 4900) loss: 2.467538\n",
      "(Iteration 2601 / 4900) loss: 2.445870\n",
      "(Iteration 2701 / 4900) loss: 2.409416\n",
      "(Iteration 2801 / 4900) loss: 2.489661\n",
      "(Iteration 2901 / 4900) loss: 2.463753\n",
      "(Epoch 6 / 10) train acc: 0.228000; val_acc: 0.231000\n",
      "(Iteration 3001 / 4900) loss: 2.456842\n",
      "(Iteration 3101 / 4900) loss: 2.413705\n",
      "(Iteration 3201 / 4900) loss: 2.486304\n",
      "(Iteration 3301 / 4900) loss: 2.492248\n",
      "(Iteration 3401 / 4900) loss: 2.422500\n",
      "(Epoch 7 / 10) train acc: 0.244000; val_acc: 0.239000\n",
      "(Iteration 3501 / 4900) loss: 2.423840\n",
      "(Iteration 3601 / 4900) loss: 2.409572\n",
      "(Iteration 3701 / 4900) loss: 2.451577\n",
      "(Iteration 3801 / 4900) loss: 2.442877\n",
      "(Iteration 3901 / 4900) loss: 2.447880\n",
      "(Epoch 8 / 10) train acc: 0.267000; val_acc: 0.250000\n",
      "(Iteration 4001 / 4900) loss: 2.473297\n",
      "(Iteration 4101 / 4900) loss: 2.387950\n",
      "(Iteration 4201 / 4900) loss: 2.360621\n",
      "(Iteration 4301 / 4900) loss: 2.416511\n",
      "(Iteration 4401 / 4900) loss: 2.465328\n",
      "(Epoch 9 / 10) train acc: 0.285000; val_acc: 0.252000\n",
      "(Iteration 4501 / 4900) loss: 2.407562\n",
      "(Iteration 4601 / 4900) loss: 2.402524\n",
      "(Iteration 4701 / 4900) loss: 2.470714\n",
      "(Iteration 4801 / 4900) loss: 2.386920\n",
      "(Epoch 10 / 10) train acc: 0.267000; val_acc: 0.258000\n",
      "validataion accuracy  0.258\n",
      "\n",
      "58 th try:\n",
      "learning rate is  0.00154117239653\n",
      "regularization strengths is  0.149160320204\n",
      "weight scale is  0.0593512553989\n",
      "hidden size is  227\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 388.081941\n",
      "(Epoch 0 / 10) train acc: 0.123000; val_acc: 0.139000\n",
      "(Iteration 101 / 4900) loss: 249.961466\n",
      "(Iteration 201 / 4900) loss: 242.854919\n",
      "(Iteration 301 / 4900) loss: 213.879359\n",
      "(Iteration 401 / 4900) loss: 182.959055\n",
      "(Epoch 1 / 10) train acc: 0.266000; val_acc: 0.256000\n",
      "(Iteration 501 / 4900) loss: 169.473552\n",
      "(Iteration 601 / 4900) loss: 161.095901\n",
      "(Iteration 701 / 4900) loss: 145.525883\n",
      "(Iteration 801 / 4900) loss: 144.477888\n",
      "(Iteration 901 / 4900) loss: 134.952435\n",
      "(Epoch 2 / 10) train acc: 0.231000; val_acc: 0.220000\n",
      "(Iteration 1001 / 4900) loss: 123.490868\n",
      "(Iteration 1101 / 4900) loss: 126.565108\n",
      "(Iteration 1201 / 4900) loss: 114.478786\n",
      "(Iteration 1301 / 4900) loss: 109.161811\n",
      "(Iteration 1401 / 4900) loss: 105.680785\n",
      "(Epoch 3 / 10) train acc: 0.257000; val_acc: 0.222000\n",
      "(Iteration 1501 / 4900) loss: 98.241874\n",
      "(Iteration 1601 / 4900) loss: 97.319051\n",
      "(Iteration 1701 / 4900) loss: 91.375844\n",
      "(Iteration 1801 / 4900) loss: 88.701486\n",
      "(Iteration 1901 / 4900) loss: 85.543381\n",
      "(Epoch 4 / 10) train acc: 0.297000; val_acc: 0.272000\n",
      "(Iteration 2001 / 4900) loss: 78.993543\n",
      "(Iteration 2101 / 4900) loss: 78.352126\n",
      "(Iteration 2201 / 4900) loss: 73.293371\n",
      "(Iteration 2301 / 4900) loss: 68.552647\n",
      "(Iteration 2401 / 4900) loss: 70.125879\n",
      "(Epoch 5 / 10) train acc: 0.338000; val_acc: 0.347000\n",
      "(Iteration 2501 / 4900) loss: 65.844200\n",
      "(Iteration 2601 / 4900) loss: 61.982479\n",
      "(Iteration 2701 / 4900) loss: 59.697592\n",
      "(Iteration 2801 / 4900) loss: 57.776563\n",
      "(Iteration 2901 / 4900) loss: 59.820435\n",
      "(Epoch 6 / 10) train acc: 0.343000; val_acc: 0.295000\n",
      "(Iteration 3001 / 4900) loss: 52.690075\n",
      "(Iteration 3101 / 4900) loss: 51.106351\n",
      "(Iteration 3201 / 4900) loss: 48.807065\n",
      "(Iteration 3301 / 4900) loss: 49.166377\n",
      "(Iteration 3401 / 4900) loss: 46.149909\n",
      "(Epoch 7 / 10) train acc: 0.397000; val_acc: 0.333000\n",
      "(Iteration 3501 / 4900) loss: 44.845956\n",
      "(Iteration 3601 / 4900) loss: 43.254937\n",
      "(Iteration 3701 / 4900) loss: 41.806820\n",
      "(Iteration 3801 / 4900) loss: 40.359381\n",
      "(Iteration 3901 / 4900) loss: 39.345628\n",
      "(Epoch 8 / 10) train acc: 0.360000; val_acc: 0.331000\n",
      "(Iteration 4001 / 4900) loss: 38.201817\n",
      "(Iteration 4101 / 4900) loss: 36.317402\n",
      "(Iteration 4201 / 4900) loss: 35.588936\n",
      "(Iteration 4301 / 4900) loss: 35.032374\n",
      "(Iteration 4401 / 4900) loss: 33.662902\n",
      "(Epoch 9 / 10) train acc: 0.428000; val_acc: 0.374000\n",
      "(Iteration 4501 / 4900) loss: 32.499020\n",
      "(Iteration 4601 / 4900) loss: 31.480278\n",
      "(Iteration 4701 / 4900) loss: 30.546272\n",
      "(Iteration 4801 / 4900) loss: 29.881010\n",
      "(Epoch 10 / 10) train acc: 0.485000; val_acc: 0.376000\n",
      "validataion accuracy  0.376\n",
      "\n",
      "59 th try:\n",
      "learning rate is  0.0307460545638\n",
      "regularization strengths is  0.0013105397741\n",
      "weight scale is  0.00158364694125\n",
      "hidden size is  216\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.307078\n",
      "(Epoch 0 / 10) train acc: 0.121000; val_acc: 0.116000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.114000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.082000; val_acc: 0.087000\n",
      "validataion accuracy  0.116\n",
      "\n",
      "60 th try:\n",
      "learning rate is  0.000360061909893\n",
      "regularization strengths is  0.00291104606964\n",
      "weight scale is  0.00420024338773\n",
      "hidden size is  172\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.538545\n",
      "(Epoch 0 / 10) train acc: 0.106000; val_acc: 0.102000\n",
      "(Iteration 101 / 4900) loss: 1.924557\n",
      "(Iteration 201 / 4900) loss: 1.807969\n",
      "(Iteration 301 / 4900) loss: 1.677122\n",
      "(Iteration 401 / 4900) loss: 1.657254\n",
      "(Epoch 1 / 10) train acc: 0.433000; val_acc: 0.419000\n",
      "(Iteration 501 / 4900) loss: 1.791371\n",
      "(Iteration 601 / 4900) loss: 1.523994\n",
      "(Iteration 701 / 4900) loss: 1.731031\n",
      "(Iteration 801 / 4900) loss: 1.747190\n",
      "(Iteration 901 / 4900) loss: 1.366888\n",
      "(Epoch 2 / 10) train acc: 0.484000; val_acc: 0.460000\n",
      "(Iteration 1001 / 4900) loss: 1.495826\n",
      "(Iteration 1101 / 4900) loss: 1.408507\n",
      "(Iteration 1201 / 4900) loss: 1.511075\n",
      "(Iteration 1301 / 4900) loss: 1.521635\n",
      "(Iteration 1401 / 4900) loss: 1.445046\n",
      "(Epoch 3 / 10) train acc: 0.506000; val_acc: 0.463000\n",
      "(Iteration 1501 / 4900) loss: 1.413256\n",
      "(Iteration 1601 / 4900) loss: 1.602825\n",
      "(Iteration 1701 / 4900) loss: 1.425977\n",
      "(Iteration 1801 / 4900) loss: 1.643706\n",
      "(Iteration 1901 / 4900) loss: 1.566342\n",
      "(Epoch 4 / 10) train acc: 0.523000; val_acc: 0.479000\n",
      "(Iteration 2001 / 4900) loss: 1.366215\n",
      "(Iteration 2101 / 4900) loss: 1.430552\n",
      "(Iteration 2201 / 4900) loss: 1.384004\n",
      "(Iteration 2301 / 4900) loss: 1.454431\n",
      "(Iteration 2401 / 4900) loss: 1.444693\n",
      "(Epoch 5 / 10) train acc: 0.524000; val_acc: 0.471000\n",
      "(Iteration 2501 / 4900) loss: 1.412747\n",
      "(Iteration 2601 / 4900) loss: 1.199660\n",
      "(Iteration 2701 / 4900) loss: 1.447049\n",
      "(Iteration 2801 / 4900) loss: 1.357920\n",
      "(Iteration 2901 / 4900) loss: 1.375391\n",
      "(Epoch 6 / 10) train acc: 0.564000; val_acc: 0.472000\n",
      "(Iteration 3001 / 4900) loss: 1.178155\n",
      "(Iteration 3101 / 4900) loss: 1.340580\n",
      "(Iteration 3201 / 4900) loss: 1.284931\n",
      "(Iteration 3301 / 4900) loss: 1.198255\n",
      "(Iteration 3401 / 4900) loss: 1.193465\n",
      "(Epoch 7 / 10) train acc: 0.571000; val_acc: 0.496000\n",
      "(Iteration 3501 / 4900) loss: 1.361383\n",
      "(Iteration 3601 / 4900) loss: 1.260361\n",
      "(Iteration 3701 / 4900) loss: 1.041391\n",
      "(Iteration 3801 / 4900) loss: 1.306942\n",
      "(Iteration 3901 / 4900) loss: 1.070659\n",
      "(Epoch 8 / 10) train acc: 0.576000; val_acc: 0.508000\n",
      "(Iteration 4001 / 4900) loss: 1.292206\n",
      "(Iteration 4101 / 4900) loss: 1.335011\n",
      "(Iteration 4201 / 4900) loss: 1.311421\n",
      "(Iteration 4301 / 4900) loss: 1.286802\n",
      "(Iteration 4401 / 4900) loss: 1.209677\n",
      "(Epoch 9 / 10) train acc: 0.600000; val_acc: 0.515000\n",
      "(Iteration 4501 / 4900) loss: 1.152311\n",
      "(Iteration 4601 / 4900) loss: 1.381583\n",
      "(Iteration 4701 / 4900) loss: 1.118209\n",
      "(Iteration 4801 / 4900) loss: 1.268637\n",
      "(Epoch 10 / 10) train acc: 0.582000; val_acc: 0.511000\n",
      "validataion accuracy  0.515\n",
      "\n",
      "61 th try:\n",
      "learning rate is  1.06481168136e-06\n",
      "regularization strengths is  0.503195373716\n",
      "weight scale is  0.0969344634955\n",
      "hidden size is  84\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: inf\n",
      "(Epoch 0 / 10) train acc: 0.099000; val_acc: 0.064000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: 842.719794\n",
      "(Iteration 401 / 4900) loss: 835.526616\n",
      "(Epoch 1 / 10) train acc: 0.097000; val_acc: 0.077000\n",
      "(Iteration 501 / 4900) loss: 824.216214\n",
      "(Iteration 601 / 4900) loss: 802.649819\n",
      "(Iteration 701 / 4900) loss: inf\n",
      "(Iteration 801 / 4900) loss: 787.124621\n",
      "(Iteration 901 / 4900) loss: 777.538261\n",
      "(Epoch 2 / 10) train acc: 0.126000; val_acc: 0.095000\n",
      "(Iteration 1001 / 4900) loss: 788.748203\n",
      "(Iteration 1101 / 4900) loss: 789.225598\n",
      "(Iteration 1201 / 4900) loss: 781.072218\n",
      "(Iteration 1301 / 4900) loss: 781.868242\n",
      "(Iteration 1401 / 4900) loss: 799.258854\n",
      "(Epoch 3 / 10) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 1501 / 4900) loss: 792.880479\n",
      "(Iteration 1601 / 4900) loss: 750.321827\n",
      "(Iteration 1701 / 4900) loss: 789.334976\n",
      "(Iteration 1801 / 4900) loss: 775.937066\n",
      "(Iteration 1901 / 4900) loss: 779.637340\n",
      "(Epoch 4 / 10) train acc: 0.132000; val_acc: 0.112000\n",
      "(Iteration 2001 / 4900) loss: 779.547719\n",
      "(Iteration 2101 / 4900) loss: 774.358538\n",
      "(Iteration 2201 / 4900) loss: 771.363864\n",
      "(Iteration 2301 / 4900) loss: 750.821285\n",
      "(Iteration 2401 / 4900) loss: 759.937446\n",
      "(Epoch 5 / 10) train acc: 0.150000; val_acc: 0.121000\n",
      "(Iteration 2501 / 4900) loss: inf\n",
      "(Iteration 2601 / 4900) loss: 759.919765\n",
      "(Iteration 2701 / 4900) loss: 747.284549\n",
      "(Iteration 2801 / 4900) loss: 761.042947\n",
      "(Iteration 2901 / 4900) loss: 776.187793\n",
      "(Epoch 6 / 10) train acc: 0.142000; val_acc: 0.129000\n",
      "(Iteration 3001 / 4900) loss: 729.003138\n",
      "(Iteration 3101 / 4900) loss: inf\n",
      "(Iteration 3201 / 4900) loss: 738.117576\n",
      "(Iteration 3301 / 4900) loss: 756.450605\n",
      "(Iteration 3401 / 4900) loss: 763.311812\n",
      "(Epoch 7 / 10) train acc: 0.148000; val_acc: 0.135000\n",
      "(Iteration 3501 / 4900) loss: 723.339749\n",
      "(Iteration 3601 / 4900) loss: 764.090755\n",
      "(Iteration 3701 / 4900) loss: 762.023122\n",
      "(Iteration 3801 / 4900) loss: 740.648532\n",
      "(Iteration 3901 / 4900) loss: 749.728706\n",
      "(Epoch 8 / 10) train acc: 0.160000; val_acc: 0.137000\n",
      "(Iteration 4001 / 4900) loss: 746.336643\n",
      "(Iteration 4101 / 4900) loss: 728.367807\n",
      "(Iteration 4201 / 4900) loss: 735.589916\n",
      "(Iteration 4301 / 4900) loss: 751.895789\n",
      "(Iteration 4401 / 4900) loss: 753.246745\n",
      "(Epoch 9 / 10) train acc: 0.146000; val_acc: 0.140000\n",
      "(Iteration 4501 / 4900) loss: 723.773999\n",
      "(Iteration 4601 / 4900) loss: 754.921626\n",
      "(Iteration 4701 / 4900) loss: 729.788109\n",
      "(Iteration 4801 / 4900) loss: 721.006323\n",
      "(Epoch 10 / 10) train acc: 0.136000; val_acc: 0.147000\n",
      "validataion accuracy  0.147\n",
      "\n",
      "62 th try:\n",
      "learning rate is  0.0567426677351\n",
      "regularization strengths is  0.00465379950611\n",
      "weight scale is  0.0568932725582\n",
      "hidden size is  133\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 112.317900\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.080000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.087000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.106000; val_acc: 0.087000\n",
      "validataion accuracy  0.087\n",
      "\n",
      "63 th try:\n",
      "learning rate is  4.21764820993e-06\n",
      "regularization strengths is  0.0168949363879\n",
      "weight scale is  0.0147678078259\n",
      "hidden size is  98\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 7.642995\n",
      "(Epoch 0 / 10) train acc: 0.059000; val_acc: 0.085000\n",
      "(Iteration 101 / 4900) loss: 8.021807\n",
      "(Iteration 201 / 4900) loss: 6.593049\n",
      "(Iteration 301 / 4900) loss: 5.044355\n",
      "(Iteration 401 / 4900) loss: 5.480120\n",
      "(Epoch 1 / 10) train acc: 0.153000; val_acc: 0.142000\n",
      "(Iteration 501 / 4900) loss: 5.070867\n",
      "(Iteration 601 / 4900) loss: 5.491272\n",
      "(Iteration 701 / 4900) loss: 4.331390\n",
      "(Iteration 801 / 4900) loss: 5.201281\n",
      "(Iteration 901 / 4900) loss: 3.971073\n",
      "(Epoch 2 / 10) train acc: 0.168000; val_acc: 0.172000\n",
      "(Iteration 1001 / 4900) loss: 4.525512\n",
      "(Iteration 1101 / 4900) loss: 4.395074\n",
      "(Iteration 1201 / 4900) loss: 4.547388\n",
      "(Iteration 1301 / 4900) loss: 3.986812\n",
      "(Iteration 1401 / 4900) loss: 4.122700\n",
      "(Epoch 3 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Iteration 1501 / 4900) loss: 4.238286\n",
      "(Iteration 1601 / 4900) loss: 4.006302\n",
      "(Iteration 1701 / 4900) loss: 4.090674\n",
      "(Iteration 1801 / 4900) loss: 3.934359\n",
      "(Iteration 1901 / 4900) loss: 3.971506\n",
      "(Epoch 4 / 10) train acc: 0.200000; val_acc: 0.206000\n",
      "(Iteration 2001 / 4900) loss: 4.388500\n",
      "(Iteration 2101 / 4900) loss: 3.917425\n",
      "(Iteration 2201 / 4900) loss: 3.616895\n",
      "(Iteration 2301 / 4900) loss: 3.829334\n",
      "(Iteration 2401 / 4900) loss: 3.586968\n",
      "(Epoch 5 / 10) train acc: 0.210000; val_acc: 0.223000\n",
      "(Iteration 2501 / 4900) loss: 3.540283\n",
      "(Iteration 2601 / 4900) loss: 4.224565\n",
      "(Iteration 2701 / 4900) loss: 3.427531\n",
      "(Iteration 2801 / 4900) loss: 3.668468\n",
      "(Iteration 2901 / 4900) loss: 3.655888\n",
      "(Epoch 6 / 10) train acc: 0.204000; val_acc: 0.239000\n",
      "(Iteration 3001 / 4900) loss: 3.675558\n",
      "(Iteration 3101 / 4900) loss: 3.666589\n",
      "(Iteration 3201 / 4900) loss: 3.663951\n",
      "(Iteration 3301 / 4900) loss: 3.685454\n",
      "(Iteration 3401 / 4900) loss: 3.544021\n",
      "(Epoch 7 / 10) train acc: 0.245000; val_acc: 0.250000\n",
      "(Iteration 3501 / 4900) loss: 3.671379\n",
      "(Iteration 3601 / 4900) loss: 3.992680\n",
      "(Iteration 3701 / 4900) loss: 3.439449\n",
      "(Iteration 3801 / 4900) loss: 3.157487\n",
      "(Iteration 3901 / 4900) loss: 3.183217\n",
      "(Epoch 8 / 10) train acc: 0.264000; val_acc: 0.247000\n",
      "(Iteration 4001 / 4900) loss: 3.276992\n",
      "(Iteration 4101 / 4900) loss: 3.411446\n",
      "(Iteration 4201 / 4900) loss: 3.522475\n",
      "(Iteration 4301 / 4900) loss: 3.755425\n",
      "(Iteration 4401 / 4900) loss: 3.307024\n",
      "(Epoch 9 / 10) train acc: 0.231000; val_acc: 0.260000\n",
      "(Iteration 4501 / 4900) loss: 3.651832\n",
      "(Iteration 4601 / 4900) loss: 3.188517\n",
      "(Iteration 4701 / 4900) loss: 2.740466\n",
      "(Iteration 4801 / 4900) loss: 3.345302\n",
      "(Epoch 10 / 10) train acc: 0.252000; val_acc: 0.272000\n",
      "validataion accuracy  0.272\n",
      "\n",
      "64 th try:\n",
      "learning rate is  0.000146841722535\n",
      "regularization strengths is  0.641506038875\n",
      "weight scale is  0.00974658352548\n",
      "hidden size is  115\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 14.602825\n",
      "(Epoch 0 / 10) train acc: 0.124000; val_acc: 0.103000\n",
      "(Iteration 101 / 4900) loss: 12.513053\n",
      "(Iteration 201 / 4900) loss: 12.460671\n",
      "(Iteration 301 / 4900) loss: 12.190253\n",
      "(Iteration 401 / 4900) loss: 11.779738\n",
      "(Epoch 1 / 10) train acc: 0.371000; val_acc: 0.393000\n",
      "(Iteration 501 / 4900) loss: 11.463516\n",
      "(Iteration 601 / 4900) loss: 11.400109\n",
      "(Iteration 701 / 4900) loss: 10.915586\n",
      "(Iteration 801 / 4900) loss: 11.008711\n",
      "(Iteration 901 / 4900) loss: 10.798290\n",
      "(Epoch 2 / 10) train acc: 0.419000; val_acc: 0.415000\n",
      "(Iteration 1001 / 4900) loss: 10.567824\n",
      "(Iteration 1101 / 4900) loss: 10.361269\n",
      "(Iteration 1201 / 4900) loss: 10.083361\n",
      "(Iteration 1301 / 4900) loss: 10.123641\n",
      "(Iteration 1401 / 4900) loss: 9.921569\n",
      "(Epoch 3 / 10) train acc: 0.419000; val_acc: 0.428000\n",
      "(Iteration 1501 / 4900) loss: 9.818263\n",
      "(Iteration 1601 / 4900) loss: 9.520849\n",
      "(Iteration 1701 / 4900) loss: 9.568298\n",
      "(Iteration 1801 / 4900) loss: 9.331242\n",
      "(Iteration 1901 / 4900) loss: 9.305118\n",
      "(Epoch 4 / 10) train acc: 0.435000; val_acc: 0.446000\n",
      "(Iteration 2001 / 4900) loss: 9.152019\n",
      "(Iteration 2101 / 4900) loss: 9.150140\n",
      "(Iteration 2201 / 4900) loss: 8.954231\n",
      "(Iteration 2301 / 4900) loss: 8.953790\n",
      "(Iteration 2401 / 4900) loss: 8.900214\n",
      "(Epoch 5 / 10) train acc: 0.443000; val_acc: 0.448000\n",
      "(Iteration 2501 / 4900) loss: 8.655073\n",
      "(Iteration 2601 / 4900) loss: 8.277227\n",
      "(Iteration 2701 / 4900) loss: 8.373959\n",
      "(Iteration 2801 / 4900) loss: 8.140160\n",
      "(Iteration 2901 / 4900) loss: 8.145991\n",
      "(Epoch 6 / 10) train acc: 0.451000; val_acc: 0.451000\n",
      "(Iteration 3001 / 4900) loss: 8.317565\n",
      "(Iteration 3101 / 4900) loss: 8.015418\n",
      "(Iteration 3201 / 4900) loss: 7.742347\n",
      "(Iteration 3301 / 4900) loss: 7.731491\n",
      "(Iteration 3401 / 4900) loss: 7.723339\n",
      "(Epoch 7 / 10) train acc: 0.468000; val_acc: 0.454000\n",
      "(Iteration 3501 / 4900) loss: 7.507838\n",
      "(Iteration 3601 / 4900) loss: 7.566075\n",
      "(Iteration 3701 / 4900) loss: 7.650232\n",
      "(Iteration 3801 / 4900) loss: 7.386615\n",
      "(Iteration 3901 / 4900) loss: 7.290295\n",
      "(Epoch 8 / 10) train acc: 0.442000; val_acc: 0.463000\n",
      "(Iteration 4001 / 4900) loss: 7.060115\n",
      "(Iteration 4101 / 4900) loss: 7.002182\n",
      "(Iteration 4201 / 4900) loss: 7.294836\n",
      "(Iteration 4301 / 4900) loss: 6.950615\n",
      "(Iteration 4401 / 4900) loss: 7.153321\n",
      "(Epoch 9 / 10) train acc: 0.510000; val_acc: 0.463000\n",
      "(Iteration 4501 / 4900) loss: 6.951786\n",
      "(Iteration 4601 / 4900) loss: 6.892888\n",
      "(Iteration 4701 / 4900) loss: 6.983411\n",
      "(Iteration 4801 / 4900) loss: 6.657913\n",
      "(Epoch 10 / 10) train acc: 0.451000; val_acc: 0.460000\n",
      "validataion accuracy  0.463\n",
      "\n",
      "65 th try:\n",
      "learning rate is  0.00465638525081\n",
      "regularization strengths is  0.175790973518\n",
      "weight scale is  0.0012807409784\n",
      "hidden size is  74\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.336650\n",
      "(Epoch 0 / 10) train acc: 0.159000; val_acc: 0.173000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.110000; val_acc: 0.120000\n",
      "(Iteration 501 / 4900) loss: inf\n",
      "(Iteration 601 / 4900) loss: inf\n",
      "(Iteration 701 / 4900) loss: inf\n",
      "(Iteration 801 / 4900) loss: inf\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.105000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.084000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.080000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.107000; val_acc: 0.087000\n",
      "validataion accuracy  0.173\n",
      "\n",
      "66 th try:\n",
      "learning rate is  0.00263660408333\n",
      "regularization strengths is  0.0784020554053\n",
      "weight scale is  0.0672896841541\n",
      "hidden size is  80\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 198.219808\n",
      "(Epoch 0 / 10) train acc: 0.165000; val_acc: 0.142000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.127000; val_acc: 0.104000\n",
      "(Iteration 501 / 4900) loss: inf\n",
      "(Iteration 601 / 4900) loss: inf\n",
      "(Iteration 701 / 4900) loss: inf\n",
      "(Iteration 801 / 4900) loss: inf\n",
      "(Iteration 901 / 4900) loss: inf\n",
      "(Epoch 2 / 10) train acc: 0.172000; val_acc: 0.182000\n",
      "(Iteration 1001 / 4900) loss: inf\n",
      "(Iteration 1101 / 4900) loss: inf\n",
      "(Iteration 1201 / 4900) loss: inf\n",
      "(Iteration 1301 / 4900) loss: inf\n",
      "(Iteration 1401 / 4900) loss: inf\n",
      "(Epoch 3 / 10) train acc: 0.195000; val_acc: 0.179000\n",
      "(Iteration 1501 / 4900) loss: inf\n",
      "(Iteration 1601 / 4900) loss: inf\n",
      "(Iteration 1701 / 4900) loss: inf\n",
      "(Iteration 1801 / 4900) loss: inf\n",
      "(Iteration 1901 / 4900) loss: inf\n",
      "(Epoch 4 / 10) train acc: 0.161000; val_acc: 0.177000\n",
      "(Iteration 2001 / 4900) loss: inf\n",
      "(Iteration 2101 / 4900) loss: inf\n",
      "(Iteration 2201 / 4900) loss: inf\n",
      "(Iteration 2301 / 4900) loss: inf\n",
      "(Iteration 2401 / 4900) loss: inf\n",
      "(Epoch 5 / 10) train acc: 0.225000; val_acc: 0.247000\n",
      "(Iteration 2501 / 4900) loss: inf\n",
      "(Iteration 2601 / 4900) loss: inf\n",
      "(Iteration 2701 / 4900) loss: inf\n",
      "(Iteration 2801 / 4900) loss: inf\n",
      "(Iteration 2901 / 4900) loss: inf\n",
      "(Epoch 6 / 10) train acc: 0.267000; val_acc: 0.283000\n",
      "(Iteration 3001 / 4900) loss: inf\n",
      "(Iteration 3101 / 4900) loss: inf\n",
      "(Iteration 3201 / 4900) loss: inf\n",
      "(Iteration 3301 / 4900) loss: inf\n",
      "(Iteration 3401 / 4900) loss: inf\n",
      "(Epoch 7 / 10) train acc: 0.238000; val_acc: 0.223000\n",
      "(Iteration 3501 / 4900) loss: inf\n",
      "(Iteration 3601 / 4900) loss: inf\n",
      "(Iteration 3701 / 4900) loss: inf\n",
      "(Iteration 3801 / 4900) loss: inf\n",
      "(Iteration 3901 / 4900) loss: inf\n",
      "(Epoch 8 / 10) train acc: 0.319000; val_acc: 0.328000\n",
      "(Iteration 4001 / 4900) loss: inf\n",
      "(Iteration 4101 / 4900) loss: inf\n",
      "(Iteration 4201 / 4900) loss: inf\n",
      "(Iteration 4301 / 4900) loss: inf\n",
      "(Iteration 4401 / 4900) loss: inf\n",
      "(Epoch 9 / 10) train acc: 0.267000; val_acc: 0.278000\n",
      "(Iteration 4501 / 4900) loss: inf\n",
      "(Iteration 4601 / 4900) loss: inf\n",
      "(Iteration 4701 / 4900) loss: inf\n",
      "(Iteration 4801 / 4900) loss: inf\n",
      "(Epoch 10 / 10) train acc: 0.236000; val_acc: 0.242000\n",
      "validataion accuracy  0.328\n",
      "\n",
      "67 th try:\n",
      "learning rate is  0.00137827236064\n",
      "regularization strengths is  0.0493277573656\n",
      "weight scale is  0.0445820618714\n",
      "hidden size is  122\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 90.621298\n",
      "(Epoch 0 / 10) train acc: 0.167000; val_acc: 0.124000\n",
      "(Iteration 101 / 4900) loss: 35.732345\n",
      "(Iteration 201 / 4900) loss: 36.629201\n",
      "(Iteration 301 / 4900) loss: 34.341107\n",
      "(Iteration 401 / 4900) loss: 28.879137\n",
      "(Epoch 1 / 10) train acc: 0.211000; val_acc: 0.213000\n",
      "(Iteration 501 / 4900) loss: 31.670021\n",
      "(Iteration 601 / 4900) loss: 29.979471\n",
      "(Iteration 701 / 4900) loss: 25.775467\n",
      "(Iteration 801 / 4900) loss: 21.647984\n",
      "(Iteration 901 / 4900) loss: 25.093729\n",
      "(Epoch 2 / 10) train acc: 0.238000; val_acc: 0.243000\n",
      "(Iteration 1001 / 4900) loss: 21.175646\n",
      "(Iteration 1101 / 4900) loss: 24.235097\n",
      "(Iteration 1201 / 4900) loss: 20.874546\n",
      "(Iteration 1301 / 4900) loss: 25.583539\n",
      "(Iteration 1401 / 4900) loss: 19.523128\n",
      "(Epoch 3 / 10) train acc: 0.228000; val_acc: 0.221000\n",
      "(Iteration 1501 / 4900) loss: 17.655192\n",
      "(Iteration 1601 / 4900) loss: 18.462882\n",
      "(Iteration 1701 / 4900) loss: 17.781357\n",
      "(Iteration 1801 / 4900) loss: 16.720149\n",
      "(Iteration 1901 / 4900) loss: 17.633985\n",
      "(Epoch 4 / 10) train acc: 0.267000; val_acc: 0.240000\n",
      "(Iteration 2001 / 4900) loss: 16.812040\n",
      "(Iteration 2101 / 4900) loss: 17.615121\n",
      "(Iteration 2201 / 4900) loss: 17.690425\n",
      "(Iteration 2301 / 4900) loss: 16.183093\n",
      "(Iteration 2401 / 4900) loss: 17.012925\n",
      "(Epoch 5 / 10) train acc: 0.253000; val_acc: 0.256000\n",
      "(Iteration 2501 / 4900) loss: 14.954607\n",
      "(Iteration 2601 / 4900) loss: 15.229022\n",
      "(Iteration 2701 / 4900) loss: 15.766482\n",
      "(Iteration 2801 / 4900) loss: 14.758586\n",
      "(Iteration 2901 / 4900) loss: 15.179153\n",
      "(Epoch 6 / 10) train acc: 0.371000; val_acc: 0.326000\n",
      "(Iteration 3001 / 4900) loss: 14.747901\n",
      "(Iteration 3101 / 4900) loss: 15.008988\n",
      "(Iteration 3201 / 4900) loss: 14.049409\n",
      "(Iteration 3301 / 4900) loss: 14.112854\n",
      "(Iteration 3401 / 4900) loss: 14.026888\n",
      "(Epoch 7 / 10) train acc: 0.345000; val_acc: 0.324000\n",
      "(Iteration 3501 / 4900) loss: 13.350184\n",
      "(Iteration 3601 / 4900) loss: 13.864978\n",
      "(Iteration 3701 / 4900) loss: 13.456176\n",
      "(Iteration 3801 / 4900) loss: 12.980088\n",
      "(Iteration 3901 / 4900) loss: 13.175637\n",
      "(Epoch 8 / 10) train acc: 0.392000; val_acc: 0.331000\n",
      "(Iteration 4001 / 4900) loss: 12.600791\n",
      "(Iteration 4101 / 4900) loss: 12.811268\n",
      "(Iteration 4201 / 4900) loss: 12.408085\n",
      "(Iteration 4301 / 4900) loss: 12.052222\n",
      "(Iteration 4401 / 4900) loss: 12.391287\n",
      "(Epoch 9 / 10) train acc: 0.389000; val_acc: 0.358000\n",
      "(Iteration 4501 / 4900) loss: 11.801589\n",
      "(Iteration 4601 / 4900) loss: 12.193094\n",
      "(Iteration 4701 / 4900) loss: 12.384183\n",
      "(Iteration 4801 / 4900) loss: 12.055053\n",
      "(Epoch 10 / 10) train acc: 0.440000; val_acc: 0.390000\n",
      "validataion accuracy  0.39\n",
      "\n",
      "68 th try:\n",
      "learning rate is  0.000267479139719\n",
      "regularization strengths is  0.0115209663421\n",
      "weight scale is  0.00303502731261\n",
      "hidden size is  128\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.340107\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.124000\n",
      "(Iteration 101 / 4900) loss: 1.920574\n",
      "(Iteration 201 / 4900) loss: 1.878945\n",
      "(Iteration 301 / 4900) loss: 1.854985\n",
      "(Iteration 401 / 4900) loss: 1.598352\n",
      "(Epoch 1 / 10) train acc: 0.405000; val_acc: 0.437000\n",
      "(Iteration 501 / 4900) loss: 1.788244\n",
      "(Iteration 601 / 4900) loss: 1.551203\n",
      "(Iteration 701 / 4900) loss: 1.650624\n",
      "(Iteration 801 / 4900) loss: 1.661126\n",
      "(Iteration 901 / 4900) loss: 1.615375\n",
      "(Epoch 2 / 10) train acc: 0.459000; val_acc: 0.459000\n",
      "(Iteration 1001 / 4900) loss: 1.792321\n",
      "(Iteration 1101 / 4900) loss: 1.531720\n",
      "(Iteration 1201 / 4900) loss: 1.813360\n",
      "(Iteration 1301 / 4900) loss: 1.512867\n",
      "(Iteration 1401 / 4900) loss: 1.453338\n",
      "(Epoch 3 / 10) train acc: 0.478000; val_acc: 0.475000\n",
      "(Iteration 1501 / 4900) loss: 1.586627\n",
      "(Iteration 1601 / 4900) loss: 1.593452\n",
      "(Iteration 1701 / 4900) loss: 1.372649\n",
      "(Iteration 1801 / 4900) loss: 1.464147\n",
      "(Iteration 1901 / 4900) loss: 1.352927\n",
      "(Epoch 4 / 10) train acc: 0.510000; val_acc: 0.482000\n",
      "(Iteration 2001 / 4900) loss: 1.448999\n",
      "(Iteration 2101 / 4900) loss: 1.308036\n",
      "(Iteration 2201 / 4900) loss: 1.560701\n",
      "(Iteration 2301 / 4900) loss: 1.542079\n",
      "(Iteration 2401 / 4900) loss: 1.445763\n",
      "(Epoch 5 / 10) train acc: 0.490000; val_acc: 0.495000\n",
      "(Iteration 2501 / 4900) loss: 1.441951\n",
      "(Iteration 2601 / 4900) loss: 1.355954\n",
      "(Iteration 2701 / 4900) loss: 1.323148\n",
      "(Iteration 2801 / 4900) loss: 1.438608\n",
      "(Iteration 2901 / 4900) loss: 1.319396\n",
      "(Epoch 6 / 10) train acc: 0.535000; val_acc: 0.509000\n",
      "(Iteration 3001 / 4900) loss: 1.464561\n",
      "(Iteration 3101 / 4900) loss: 1.507159\n",
      "(Iteration 3201 / 4900) loss: 1.379556\n",
      "(Iteration 3301 / 4900) loss: 1.143420\n",
      "(Iteration 3401 / 4900) loss: 1.508557\n",
      "(Epoch 7 / 10) train acc: 0.540000; val_acc: 0.508000\n",
      "(Iteration 3501 / 4900) loss: 1.307855\n",
      "(Iteration 3601 / 4900) loss: 1.395908\n",
      "(Iteration 3701 / 4900) loss: 1.395829\n",
      "(Iteration 3801 / 4900) loss: 1.174559\n",
      "(Iteration 3901 / 4900) loss: 1.380671\n",
      "(Epoch 8 / 10) train acc: 0.553000; val_acc: 0.531000\n",
      "(Iteration 4001 / 4900) loss: 1.256426\n",
      "(Iteration 4101 / 4900) loss: 1.255847\n",
      "(Iteration 4201 / 4900) loss: 1.383885\n",
      "(Iteration 4301 / 4900) loss: 1.417002\n",
      "(Iteration 4401 / 4900) loss: 1.245401\n",
      "(Epoch 9 / 10) train acc: 0.561000; val_acc: 0.504000\n",
      "(Iteration 4501 / 4900) loss: 1.169747\n",
      "(Iteration 4601 / 4900) loss: 1.208006\n",
      "(Iteration 4701 / 4900) loss: 1.268937\n",
      "(Iteration 4801 / 4900) loss: 1.258324\n",
      "(Epoch 10 / 10) train acc: 0.576000; val_acc: 0.521000\n",
      "validataion accuracy  0.531\n",
      "\n",
      "69 th try:\n",
      "learning rate is  0.000911591765171\n",
      "regularization strengths is  0.158507157489\n",
      "weight scale is  0.00930413821934\n",
      "hidden size is  296\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 11.784218\n",
      "(Epoch 0 / 10) train acc: 0.120000; val_acc: 0.130000\n",
      "(Iteration 101 / 4900) loss: 8.369709\n",
      "(Iteration 201 / 4900) loss: 8.102448\n",
      "(Iteration 301 / 4900) loss: 7.912701\n",
      "(Iteration 401 / 4900) loss: 7.202910\n",
      "(Epoch 1 / 10) train acc: 0.380000; val_acc: 0.361000\n",
      "(Iteration 501 / 4900) loss: 6.929773\n",
      "(Iteration 601 / 4900) loss: 6.973597\n",
      "(Iteration 701 / 4900) loss: 6.958699\n",
      "(Iteration 801 / 4900) loss: 6.350473\n",
      "(Iteration 901 / 4900) loss: 6.427506\n",
      "(Epoch 2 / 10) train acc: 0.465000; val_acc: 0.413000\n",
      "(Iteration 1001 / 4900) loss: 6.187765\n",
      "(Iteration 1101 / 4900) loss: 6.170986\n",
      "(Iteration 1201 / 4900) loss: 5.774072\n",
      "(Iteration 1301 / 4900) loss: 5.681388\n",
      "(Iteration 1401 / 4900) loss: 5.527713\n",
      "(Epoch 3 / 10) train acc: 0.514000; val_acc: 0.463000\n",
      "(Iteration 1501 / 4900) loss: 5.542030\n",
      "(Iteration 1601 / 4900) loss: 5.555361\n",
      "(Iteration 1701 / 4900) loss: 5.180512\n",
      "(Iteration 1801 / 4900) loss: 5.136322\n",
      "(Iteration 1901 / 4900) loss: 5.029065\n",
      "(Epoch 4 / 10) train acc: 0.528000; val_acc: 0.461000\n",
      "(Iteration 2001 / 4900) loss: 5.013395\n",
      "(Iteration 2101 / 4900) loss: 4.873347\n",
      "(Iteration 2201 / 4900) loss: 4.596063\n",
      "(Iteration 2301 / 4900) loss: 4.576176\n",
      "(Iteration 2401 / 4900) loss: 4.603299\n",
      "(Epoch 5 / 10) train acc: 0.570000; val_acc: 0.489000\n",
      "(Iteration 2501 / 4900) loss: 4.603501\n",
      "(Iteration 2601 / 4900) loss: 4.509931\n",
      "(Iteration 2701 / 4900) loss: 4.191341\n",
      "(Iteration 2801 / 4900) loss: 4.340999\n",
      "(Iteration 2901 / 4900) loss: 4.221524\n",
      "(Epoch 6 / 10) train acc: 0.578000; val_acc: 0.466000\n",
      "(Iteration 3001 / 4900) loss: 4.283964\n",
      "(Iteration 3101 / 4900) loss: 4.332409\n",
      "(Iteration 3201 / 4900) loss: 3.893977\n",
      "(Iteration 3301 / 4900) loss: 4.012899\n",
      "(Iteration 3401 / 4900) loss: 3.838287\n",
      "(Epoch 7 / 10) train acc: 0.576000; val_acc: 0.484000\n",
      "(Iteration 3501 / 4900) loss: 3.761475\n",
      "(Iteration 3601 / 4900) loss: 3.558647\n",
      "(Iteration 3701 / 4900) loss: 3.733203\n",
      "(Iteration 3801 / 4900) loss: 3.395742\n",
      "(Iteration 3901 / 4900) loss: 3.657721\n",
      "(Epoch 8 / 10) train acc: 0.608000; val_acc: 0.491000\n",
      "(Iteration 4001 / 4900) loss: 3.533052\n",
      "(Iteration 4101 / 4900) loss: 3.391438\n",
      "(Iteration 4201 / 4900) loss: 3.418169\n",
      "(Iteration 4301 / 4900) loss: 3.463652\n",
      "(Iteration 4401 / 4900) loss: 3.437243\n",
      "(Epoch 9 / 10) train acc: 0.611000; val_acc: 0.469000\n",
      "(Iteration 4501 / 4900) loss: 3.225439\n",
      "(Iteration 4601 / 4900) loss: 3.081181\n",
      "(Iteration 4701 / 4900) loss: 3.237774\n",
      "(Iteration 4801 / 4900) loss: 3.304942\n",
      "(Epoch 10 / 10) train acc: 0.662000; val_acc: 0.514000\n",
      "validataion accuracy  0.514\n",
      "\n",
      "70 th try:\n",
      "learning rate is  1.95825501177e-05\n",
      "regularization strengths is  0.0908830075415\n",
      "weight scale is  0.00515746892449\n",
      "hidden size is  145\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 3.166434\n",
      "(Epoch 0 / 10) train acc: 0.099000; val_acc: 0.077000\n",
      "(Iteration 101 / 4900) loss: 2.841965\n",
      "(Iteration 201 / 4900) loss: 2.671757\n",
      "(Iteration 301 / 4900) loss: 2.601029\n",
      "(Iteration 401 / 4900) loss: 2.512913\n",
      "(Epoch 1 / 10) train acc: 0.279000; val_acc: 0.314000\n",
      "(Iteration 501 / 4900) loss: 2.583237\n",
      "(Iteration 601 / 4900) loss: 2.377689\n",
      "(Iteration 701 / 4900) loss: 2.495300\n",
      "(Iteration 801 / 4900) loss: 2.442135\n",
      "(Iteration 901 / 4900) loss: 2.426093\n",
      "(Epoch 2 / 10) train acc: 0.320000; val_acc: 0.347000\n",
      "(Iteration 1001 / 4900) loss: 2.324102\n",
      "(Iteration 1101 / 4900) loss: 2.407469\n",
      "(Iteration 1201 / 4900) loss: 2.262585\n",
      "(Iteration 1301 / 4900) loss: 2.369417\n",
      "(Iteration 1401 / 4900) loss: 2.349110\n",
      "(Epoch 3 / 10) train acc: 0.327000; val_acc: 0.363000\n",
      "(Iteration 1501 / 4900) loss: 2.349600\n",
      "(Iteration 1601 / 4900) loss: 2.400094\n",
      "(Iteration 1701 / 4900) loss: 2.273384\n",
      "(Iteration 1801 / 4900) loss: 2.397951\n",
      "(Iteration 1901 / 4900) loss: 2.242569\n",
      "(Epoch 4 / 10) train acc: 0.354000; val_acc: 0.371000\n",
      "(Iteration 2001 / 4900) loss: 2.296826\n",
      "(Iteration 2101 / 4900) loss: 2.418338\n",
      "(Iteration 2201 / 4900) loss: 2.322858\n",
      "(Iteration 2301 / 4900) loss: 2.281650\n",
      "(Iteration 2401 / 4900) loss: 2.334255\n",
      "(Epoch 5 / 10) train acc: 0.402000; val_acc: 0.378000\n",
      "(Iteration 2501 / 4900) loss: 2.445631\n",
      "(Iteration 2601 / 4900) loss: 2.299946\n",
      "(Iteration 2701 / 4900) loss: 2.377128\n",
      "(Iteration 2801 / 4900) loss: 2.271377\n",
      "(Iteration 2901 / 4900) loss: 2.190908\n",
      "(Epoch 6 / 10) train acc: 0.396000; val_acc: 0.382000\n",
      "(Iteration 3001 / 4900) loss: 2.334628\n",
      "(Iteration 3101 / 4900) loss: 2.236761\n",
      "(Iteration 3201 / 4900) loss: 2.149835\n",
      "(Iteration 3301 / 4900) loss: 2.284638\n",
      "(Iteration 3401 / 4900) loss: 2.229987\n",
      "(Epoch 7 / 10) train acc: 0.394000; val_acc: 0.381000\n",
      "(Iteration 3501 / 4900) loss: 2.285009\n",
      "(Iteration 3601 / 4900) loss: 2.268458\n",
      "(Iteration 3701 / 4900) loss: 2.335468\n",
      "(Iteration 3801 / 4900) loss: 2.357522\n",
      "(Iteration 3901 / 4900) loss: 2.284962\n",
      "(Epoch 8 / 10) train acc: 0.370000; val_acc: 0.384000\n",
      "(Iteration 4001 / 4900) loss: 2.097645\n",
      "(Iteration 4101 / 4900) loss: 2.224970\n",
      "(Iteration 4201 / 4900) loss: 2.074951\n",
      "(Iteration 4301 / 4900) loss: 2.233161\n",
      "(Iteration 4401 / 4900) loss: 2.247507\n",
      "(Epoch 9 / 10) train acc: 0.391000; val_acc: 0.394000\n",
      "(Iteration 4501 / 4900) loss: 2.368723\n",
      "(Iteration 4601 / 4900) loss: 2.184933\n",
      "(Iteration 4701 / 4900) loss: 2.238705\n",
      "(Iteration 4801 / 4900) loss: 2.231637\n",
      "(Epoch 10 / 10) train acc: 0.395000; val_acc: 0.400000\n",
      "validataion accuracy  0.4\n",
      "\n",
      "71 th try:\n",
      "learning rate is  3.97204111746e-05\n",
      "regularization strengths is  0.0146230439411\n",
      "weight scale is  0.00268558989227\n",
      "hidden size is  69\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.319849\n",
      "(Epoch 0 / 10) train acc: 0.071000; val_acc: 0.108000\n",
      "(Iteration 101 / 4900) loss: 2.236875\n",
      "(Iteration 201 / 4900) loss: 2.211061\n",
      "(Iteration 301 / 4900) loss: 2.117534\n",
      "(Iteration 401 / 4900) loss: 2.040382\n",
      "(Epoch 1 / 10) train acc: 0.269000; val_acc: 0.299000\n",
      "(Iteration 501 / 4900) loss: 2.036412\n",
      "(Iteration 601 / 4900) loss: 1.979188\n",
      "(Iteration 701 / 4900) loss: 1.982361\n",
      "(Iteration 801 / 4900) loss: 1.957709\n",
      "(Iteration 901 / 4900) loss: 1.861860\n",
      "(Epoch 2 / 10) train acc: 0.310000; val_acc: 0.349000\n",
      "(Iteration 1001 / 4900) loss: 1.845795\n",
      "(Iteration 1101 / 4900) loss: 1.911262\n",
      "(Iteration 1201 / 4900) loss: 1.910613\n",
      "(Iteration 1301 / 4900) loss: 1.779468\n",
      "(Iteration 1401 / 4900) loss: 1.763669\n",
      "(Epoch 3 / 10) train acc: 0.391000; val_acc: 0.367000\n",
      "(Iteration 1501 / 4900) loss: 1.804548\n",
      "(Iteration 1601 / 4900) loss: 1.744366\n",
      "(Iteration 1701 / 4900) loss: 1.780717\n",
      "(Iteration 1801 / 4900) loss: 1.929245\n",
      "(Iteration 1901 / 4900) loss: 1.941690\n",
      "(Epoch 4 / 10) train acc: 0.360000; val_acc: 0.372000\n",
      "(Iteration 2001 / 4900) loss: 1.892550\n",
      "(Iteration 2101 / 4900) loss: 1.746373\n",
      "(Iteration 2201 / 4900) loss: 1.769432\n",
      "(Iteration 2301 / 4900) loss: 1.785618\n",
      "(Iteration 2401 / 4900) loss: 1.909044\n",
      "(Epoch 5 / 10) train acc: 0.409000; val_acc: 0.383000\n",
      "(Iteration 2501 / 4900) loss: 1.768516\n",
      "(Iteration 2601 / 4900) loss: 1.736621\n",
      "(Iteration 2701 / 4900) loss: 1.880279\n",
      "(Iteration 2801 / 4900) loss: 1.680951\n",
      "(Iteration 2901 / 4900) loss: 1.724668\n",
      "(Epoch 6 / 10) train acc: 0.385000; val_acc: 0.405000\n",
      "(Iteration 3001 / 4900) loss: 1.686459\n",
      "(Iteration 3101 / 4900) loss: 1.760631\n",
      "(Iteration 3201 / 4900) loss: 1.681860\n",
      "(Iteration 3301 / 4900) loss: 1.806923\n",
      "(Iteration 3401 / 4900) loss: 1.730715\n",
      "(Epoch 7 / 10) train acc: 0.427000; val_acc: 0.406000\n",
      "(Iteration 3501 / 4900) loss: 1.631648\n",
      "(Iteration 3601 / 4900) loss: 1.731212\n",
      "(Iteration 3701 / 4900) loss: 1.746030\n",
      "(Iteration 3801 / 4900) loss: 1.674394\n",
      "(Iteration 3901 / 4900) loss: 1.613571\n",
      "(Epoch 8 / 10) train acc: 0.403000; val_acc: 0.425000\n",
      "(Iteration 4001 / 4900) loss: 1.656469\n",
      "(Iteration 4101 / 4900) loss: 1.642911\n",
      "(Iteration 4201 / 4900) loss: 1.727185\n",
      "(Iteration 4301 / 4900) loss: 1.735011\n",
      "(Iteration 4401 / 4900) loss: 1.947303\n",
      "(Epoch 9 / 10) train acc: 0.428000; val_acc: 0.428000\n",
      "(Iteration 4501 / 4900) loss: 1.660476\n",
      "(Iteration 4601 / 4900) loss: 1.744564\n",
      "(Iteration 4701 / 4900) loss: 1.690900\n",
      "(Iteration 4801 / 4900) loss: 1.610675\n",
      "(Epoch 10 / 10) train acc: 0.423000; val_acc: 0.433000\n",
      "validataion accuracy  0.433\n",
      "\n",
      "72 th try:\n",
      "learning rate is  0.000215367666325\n",
      "regularization strengths is  0.0840211994549\n",
      "weight scale is  0.00230763862138\n",
      "hidden size is  192\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.451396\n",
      "(Epoch 0 / 10) train acc: 0.113000; val_acc: 0.113000\n",
      "(Iteration 101 / 4900) loss: 2.038111\n",
      "(Iteration 201 / 4900) loss: 1.962549\n",
      "(Iteration 301 / 4900) loss: 1.962547\n",
      "(Iteration 401 / 4900) loss: 1.743995\n",
      "(Epoch 1 / 10) train acc: 0.415000; val_acc: 0.414000\n",
      "(Iteration 501 / 4900) loss: 1.761400\n",
      "(Iteration 601 / 4900) loss: 1.771743\n",
      "(Iteration 701 / 4900) loss: 1.681482\n",
      "(Iteration 801 / 4900) loss: 1.680138\n",
      "(Iteration 901 / 4900) loss: 1.807556\n",
      "(Epoch 2 / 10) train acc: 0.446000; val_acc: 0.458000\n",
      "(Iteration 1001 / 4900) loss: 1.787435\n",
      "(Iteration 1101 / 4900) loss: 1.595232\n",
      "(Iteration 1201 / 4900) loss: 1.719778\n",
      "(Iteration 1301 / 4900) loss: 1.406387\n",
      "(Iteration 1401 / 4900) loss: 1.569147\n",
      "(Epoch 3 / 10) train acc: 0.496000; val_acc: 0.460000\n",
      "(Iteration 1501 / 4900) loss: 1.668243\n",
      "(Iteration 1601 / 4900) loss: 1.694252\n",
      "(Iteration 1701 / 4900) loss: 1.590209\n",
      "(Iteration 1801 / 4900) loss: 1.660526\n",
      "(Iteration 1901 / 4900) loss: 1.592921\n",
      "(Epoch 4 / 10) train acc: 0.509000; val_acc: 0.477000\n",
      "(Iteration 2001 / 4900) loss: 1.463360\n",
      "(Iteration 2101 / 4900) loss: 1.483068\n",
      "(Iteration 2201 / 4900) loss: 1.718642\n",
      "(Iteration 2301 / 4900) loss: 1.686444\n",
      "(Iteration 2401 / 4900) loss: 1.428968\n",
      "(Epoch 5 / 10) train acc: 0.505000; val_acc: 0.499000\n",
      "(Iteration 2501 / 4900) loss: 1.586361\n",
      "(Iteration 2601 / 4900) loss: 1.420036\n",
      "(Iteration 2701 / 4900) loss: 1.466172\n",
      "(Iteration 2801 / 4900) loss: 1.423072\n",
      "(Iteration 2901 / 4900) loss: 1.454781\n",
      "(Epoch 6 / 10) train acc: 0.513000; val_acc: 0.484000\n",
      "(Iteration 3001 / 4900) loss: 1.458153\n",
      "(Iteration 3101 / 4900) loss: 1.564819\n",
      "(Iteration 3201 / 4900) loss: 1.384915\n",
      "(Iteration 3301 / 4900) loss: 1.514633\n",
      "(Iteration 3401 / 4900) loss: 1.696560\n",
      "(Epoch 7 / 10) train acc: 0.540000; val_acc: 0.502000\n",
      "(Iteration 3501 / 4900) loss: 1.454822\n",
      "(Iteration 3601 / 4900) loss: 1.623894\n",
      "(Iteration 3701 / 4900) loss: 1.493930\n",
      "(Iteration 3801 / 4900) loss: 1.409753\n",
      "(Iteration 3901 / 4900) loss: 1.557997\n",
      "(Epoch 8 / 10) train acc: 0.546000; val_acc: 0.500000\n",
      "(Iteration 4001 / 4900) loss: 1.361698\n",
      "(Iteration 4101 / 4900) loss: 1.476356\n",
      "(Iteration 4201 / 4900) loss: 1.556200\n",
      "(Iteration 4301 / 4900) loss: 1.265397\n",
      "(Iteration 4401 / 4900) loss: 1.482425\n",
      "(Epoch 9 / 10) train acc: 0.570000; val_acc: 0.508000\n",
      "(Iteration 4501 / 4900) loss: 1.446095\n",
      "(Iteration 4601 / 4900) loss: 1.482347\n",
      "(Iteration 4701 / 4900) loss: 1.295310\n",
      "(Iteration 4801 / 4900) loss: 1.253154\n",
      "(Epoch 10 / 10) train acc: 0.552000; val_acc: 0.514000\n",
      "validataion accuracy  0.514\n",
      "\n",
      "73 th try:\n",
      "learning rate is  0.00871868872872\n",
      "regularization strengths is  0.0198616933559\n",
      "weight scale is  0.01611290664\n",
      "hidden size is  61\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 7.526209\n",
      "(Epoch 0 / 10) train acc: 0.090000; val_acc: 0.086000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.096000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.089000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.093000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.113000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.084000; val_acc: 0.087000\n",
      "validataion accuracy  0.087\n",
      "\n",
      "74 th try:\n",
      "learning rate is  0.000143567185902\n",
      "regularization strengths is  0.757092715491\n",
      "weight scale is  0.00144152554754\n",
      "hidden size is  85\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.515690\n",
      "(Epoch 0 / 10) train acc: 0.091000; val_acc: 0.101000\n",
      "(Iteration 101 / 4900) loss: 2.327550\n",
      "(Iteration 201 / 4900) loss: 2.243138\n",
      "(Iteration 301 / 4900) loss: 2.068425\n",
      "(Iteration 401 / 4900) loss: 2.088132\n",
      "(Epoch 1 / 10) train acc: 0.344000; val_acc: 0.371000\n",
      "(Iteration 501 / 4900) loss: 1.932598\n",
      "(Iteration 601 / 4900) loss: 2.016880\n",
      "(Iteration 701 / 4900) loss: 1.830398\n",
      "(Iteration 801 / 4900) loss: 2.143230\n",
      "(Iteration 901 / 4900) loss: 1.936054\n",
      "(Epoch 2 / 10) train acc: 0.394000; val_acc: 0.391000\n",
      "(Iteration 1001 / 4900) loss: 1.781678\n",
      "(Iteration 1101 / 4900) loss: 1.958596\n",
      "(Iteration 1201 / 4900) loss: 1.806200\n",
      "(Iteration 1301 / 4900) loss: 1.697178\n",
      "(Iteration 1401 / 4900) loss: 1.804074\n",
      "(Epoch 3 / 10) train acc: 0.449000; val_acc: 0.431000\n",
      "(Iteration 1501 / 4900) loss: 1.880850\n",
      "(Iteration 1601 / 4900) loss: 1.730908\n",
      "(Iteration 1701 / 4900) loss: 1.949014\n",
      "(Iteration 1801 / 4900) loss: 1.767751\n",
      "(Iteration 1901 / 4900) loss: 1.848792\n",
      "(Epoch 4 / 10) train acc: 0.458000; val_acc: 0.453000\n",
      "(Iteration 2001 / 4900) loss: 1.842418\n",
      "(Iteration 2101 / 4900) loss: 1.673793\n",
      "(Iteration 2201 / 4900) loss: 1.792550\n",
      "(Iteration 2301 / 4900) loss: 1.662455\n",
      "(Iteration 2401 / 4900) loss: 1.785066\n",
      "(Epoch 5 / 10) train acc: 0.467000; val_acc: 0.452000\n",
      "(Iteration 2501 / 4900) loss: 1.671776\n",
      "(Iteration 2601 / 4900) loss: 1.655284\n",
      "(Iteration 2701 / 4900) loss: 1.646713\n",
      "(Iteration 2801 / 4900) loss: 1.847545\n",
      "(Iteration 2901 / 4900) loss: 1.581657\n",
      "(Epoch 6 / 10) train acc: 0.457000; val_acc: 0.464000\n",
      "(Iteration 3001 / 4900) loss: 1.736010\n",
      "(Iteration 3101 / 4900) loss: 1.663437\n",
      "(Iteration 3201 / 4900) loss: 1.671748\n",
      "(Iteration 3301 / 4900) loss: 1.785085\n",
      "(Iteration 3401 / 4900) loss: 1.728963\n",
      "(Epoch 7 / 10) train acc: 0.484000; val_acc: 0.458000\n",
      "(Iteration 3501 / 4900) loss: 1.645393\n",
      "(Iteration 3601 / 4900) loss: 1.699174\n",
      "(Iteration 3701 / 4900) loss: 1.776411\n",
      "(Iteration 3801 / 4900) loss: 1.454132\n",
      "(Iteration 3901 / 4900) loss: 1.520059\n",
      "(Epoch 8 / 10) train acc: 0.491000; val_acc: 0.462000\n",
      "(Iteration 4001 / 4900) loss: 1.460743\n",
      "(Iteration 4101 / 4900) loss: 1.618659\n",
      "(Iteration 4201 / 4900) loss: 1.477445\n",
      "(Iteration 4301 / 4900) loss: 1.542884\n",
      "(Iteration 4401 / 4900) loss: 1.598279\n",
      "(Epoch 9 / 10) train acc: 0.517000; val_acc: 0.477000\n",
      "(Iteration 4501 / 4900) loss: 1.485500\n",
      "(Iteration 4601 / 4900) loss: 1.523094\n",
      "(Iteration 4701 / 4900) loss: 1.546313\n",
      "(Iteration 4801 / 4900) loss: 1.637243\n",
      "(Epoch 10 / 10) train acc: 0.509000; val_acc: 0.478000\n",
      "validataion accuracy  0.478\n",
      "\n",
      "75 th try:\n",
      "learning rate is  0.00389634757556\n",
      "regularization strengths is  0.0581427796888\n",
      "weight scale is  0.0343726123506\n",
      "hidden size is  214\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 98.893441\n",
      "(Epoch 0 / 10) train acc: 0.163000; val_acc: 0.159000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.116000; val_acc: 0.127000\n",
      "(Iteration 501 / 4900) loss: inf\n",
      "(Iteration 601 / 4900) loss: inf\n",
      "(Iteration 701 / 4900) loss: inf\n",
      "(Iteration 801 / 4900) loss: inf\n",
      "(Iteration 901 / 4900) loss: inf\n",
      "(Epoch 2 / 10) train acc: 0.145000; val_acc: 0.143000\n",
      "(Iteration 1001 / 4900) loss: inf\n",
      "(Iteration 1101 / 4900) loss: inf\n",
      "(Iteration 1201 / 4900) loss: inf\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.089000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.110000; val_acc: 0.087000\n",
      "validataion accuracy  0.159\n",
      "\n",
      "76 th try:\n",
      "learning rate is  0.0973328429078\n",
      "regularization strengths is  0.00897154509489\n",
      "weight scale is  0.0542274302783\n",
      "hidden size is  284\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 196.788423\n",
      "(Epoch 0 / 10) train acc: 0.108000; val_acc: 0.078000\n",
      "(Iteration 101 / 4900) loss: nan\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.082000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.097000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.092000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.104000; val_acc: 0.087000\n",
      "validataion accuracy  0.087\n",
      "\n",
      "77 th try:\n",
      "learning rate is  1.85571998039e-05\n",
      "regularization strengths is  0.00356881357103\n",
      "weight scale is  0.0176313897158\n",
      "hidden size is  136\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 14.739975\n",
      "(Epoch 0 / 10) train acc: 0.095000; val_acc: 0.103000\n",
      "(Iteration 101 / 4900) loss: 6.738259\n",
      "(Iteration 201 / 4900) loss: 6.404345\n",
      "(Iteration 301 / 4900) loss: 5.539387\n",
      "(Iteration 401 / 4900) loss: 5.292278\n",
      "(Epoch 1 / 10) train acc: 0.221000; val_acc: 0.254000\n",
      "(Iteration 501 / 4900) loss: 4.805822\n",
      "(Iteration 601 / 4900) loss: 5.167128\n",
      "(Iteration 701 / 4900) loss: 4.432544\n",
      "(Iteration 801 / 4900) loss: 4.135066\n",
      "(Iteration 901 / 4900) loss: 4.219412\n",
      "(Epoch 2 / 10) train acc: 0.262000; val_acc: 0.269000\n",
      "(Iteration 1001 / 4900) loss: 4.293108\n",
      "(Iteration 1101 / 4900) loss: 4.667437\n",
      "(Iteration 1201 / 4900) loss: 3.275304\n",
      "(Iteration 1301 / 4900) loss: 4.009253\n",
      "(Iteration 1401 / 4900) loss: 3.762190\n",
      "(Epoch 3 / 10) train acc: 0.287000; val_acc: 0.296000\n",
      "(Iteration 1501 / 4900) loss: 3.394307\n",
      "(Iteration 1601 / 4900) loss: 3.215175\n",
      "(Iteration 1701 / 4900) loss: 2.944334\n",
      "(Iteration 1801 / 4900) loss: 3.290078\n",
      "(Iteration 1901 / 4900) loss: 2.718682\n",
      "(Epoch 4 / 10) train acc: 0.308000; val_acc: 0.320000\n",
      "(Iteration 2001 / 4900) loss: 3.307817\n",
      "(Iteration 2101 / 4900) loss: 3.392498\n",
      "(Iteration 2201 / 4900) loss: 2.736675\n",
      "(Iteration 2301 / 4900) loss: 2.828212\n",
      "(Iteration 2401 / 4900) loss: 2.856380\n",
      "(Epoch 5 / 10) train acc: 0.314000; val_acc: 0.322000\n",
      "(Iteration 2501 / 4900) loss: 2.613970\n",
      "(Iteration 2601 / 4900) loss: 3.106965\n",
      "(Iteration 2701 / 4900) loss: 2.651725\n",
      "(Iteration 2801 / 4900) loss: 2.689147\n",
      "(Iteration 2901 / 4900) loss: 2.285526\n",
      "(Epoch 6 / 10) train acc: 0.288000; val_acc: 0.321000\n",
      "(Iteration 3001 / 4900) loss: 2.720596\n",
      "(Iteration 3101 / 4900) loss: 2.744245\n",
      "(Iteration 3201 / 4900) loss: 2.691667\n",
      "(Iteration 3301 / 4900) loss: 2.713537\n",
      "(Iteration 3401 / 4900) loss: 2.099621\n",
      "(Epoch 7 / 10) train acc: 0.326000; val_acc: 0.325000\n",
      "(Iteration 3501 / 4900) loss: 2.393237\n",
      "(Iteration 3601 / 4900) loss: 2.500223\n",
      "(Iteration 3701 / 4900) loss: 2.557950\n",
      "(Iteration 3801 / 4900) loss: 2.396466\n",
      "(Iteration 3901 / 4900) loss: 2.045670\n",
      "(Epoch 8 / 10) train acc: 0.349000; val_acc: 0.334000\n",
      "(Iteration 4001 / 4900) loss: 2.690241\n",
      "(Iteration 4101 / 4900) loss: 2.647587\n",
      "(Iteration 4201 / 4900) loss: 2.270748\n",
      "(Iteration 4301 / 4900) loss: 2.450781\n",
      "(Iteration 4401 / 4900) loss: 2.435150\n",
      "(Epoch 9 / 10) train acc: 0.353000; val_acc: 0.348000\n",
      "(Iteration 4501 / 4900) loss: 2.197962\n",
      "(Iteration 4601 / 4900) loss: 2.293393\n",
      "(Iteration 4701 / 4900) loss: 2.119704\n",
      "(Iteration 4801 / 4900) loss: 2.195207\n",
      "(Epoch 10 / 10) train acc: 0.324000; val_acc: 0.356000\n",
      "validataion accuracy  0.356\n",
      "\n",
      "78 th try:\n",
      "learning rate is  2.09529010489e-05\n",
      "regularization strengths is  0.0871650019826\n",
      "weight scale is  0.0612697036788\n",
      "hidden size is  54\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 133.474456\n",
      "(Epoch 0 / 10) train acc: 0.088000; val_acc: 0.099000\n",
      "(Iteration 101 / 4900) loss: 70.821718\n",
      "(Iteration 201 / 4900) loss: 63.683936\n",
      "(Iteration 301 / 4900) loss: 57.792268\n",
      "(Iteration 401 / 4900) loss: 58.024221\n",
      "(Epoch 1 / 10) train acc: 0.198000; val_acc: 0.198000\n",
      "(Iteration 501 / 4900) loss: 51.624222\n",
      "(Iteration 601 / 4900) loss: 49.205144\n",
      "(Iteration 701 / 4900) loss: 47.770764\n",
      "(Iteration 801 / 4900) loss: 49.325713\n",
      "(Iteration 901 / 4900) loss: 45.257069\n",
      "(Epoch 2 / 10) train acc: 0.223000; val_acc: 0.225000\n",
      "(Iteration 1001 / 4900) loss: 44.655909\n",
      "(Iteration 1101 / 4900) loss: 42.081458\n",
      "(Iteration 1201 / 4900) loss: 40.381173\n",
      "(Iteration 1301 / 4900) loss: 37.506014\n",
      "(Iteration 1401 / 4900) loss: 41.098166\n",
      "(Epoch 3 / 10) train acc: 0.230000; val_acc: 0.231000\n",
      "(Iteration 1501 / 4900) loss: 38.478890\n",
      "(Iteration 1601 / 4900) loss: 38.581011\n",
      "(Iteration 1701 / 4900) loss: 36.036133\n",
      "(Iteration 1801 / 4900) loss: 34.884820\n",
      "(Iteration 1901 / 4900) loss: 33.912694\n",
      "(Epoch 4 / 10) train acc: 0.243000; val_acc: 0.234000\n",
      "(Iteration 2001 / 4900) loss: 33.514176\n",
      "(Iteration 2101 / 4900) loss: 32.880016\n",
      "(Iteration 2201 / 4900) loss: 32.120645\n",
      "(Iteration 2301 / 4900) loss: 30.468571\n",
      "(Iteration 2401 / 4900) loss: 31.316593\n",
      "(Epoch 5 / 10) train acc: 0.270000; val_acc: 0.244000\n",
      "(Iteration 2501 / 4900) loss: 30.437306\n",
      "(Iteration 2601 / 4900) loss: 30.034658\n",
      "(Iteration 2701 / 4900) loss: 29.497627\n",
      "(Iteration 2801 / 4900) loss: 29.125048\n",
      "(Iteration 2901 / 4900) loss: 29.284541\n",
      "(Epoch 6 / 10) train acc: 0.273000; val_acc: 0.281000\n",
      "(Iteration 3001 / 4900) loss: 29.264829\n",
      "(Iteration 3101 / 4900) loss: 29.152597\n",
      "(Iteration 3201 / 4900) loss: 28.562304\n",
      "(Iteration 3301 / 4900) loss: 28.877844\n",
      "(Iteration 3401 / 4900) loss: 28.608308\n",
      "(Epoch 7 / 10) train acc: 0.293000; val_acc: 0.318000\n",
      "(Iteration 3501 / 4900) loss: 28.708982\n",
      "(Iteration 3601 / 4900) loss: 28.781253\n",
      "(Iteration 3701 / 4900) loss: 28.631588\n",
      "(Iteration 3801 / 4900) loss: 28.764607\n",
      "(Iteration 3901 / 4900) loss: 28.595202\n",
      "(Epoch 8 / 10) train acc: 0.292000; val_acc: 0.328000\n",
      "(Iteration 4001 / 4900) loss: 28.442791\n",
      "(Iteration 4101 / 4900) loss: 28.678709\n",
      "(Iteration 4201 / 4900) loss: 28.626911\n",
      "(Iteration 4301 / 4900) loss: 28.522349\n",
      "(Iteration 4401 / 4900) loss: 28.544708\n",
      "(Epoch 9 / 10) train acc: 0.316000; val_acc: 0.329000\n",
      "(Iteration 4501 / 4900) loss: 28.507155\n",
      "(Iteration 4601 / 4900) loss: 28.456967\n",
      "(Iteration 4701 / 4900) loss: 28.510031\n",
      "(Iteration 4801 / 4900) loss: 28.534357\n",
      "(Epoch 10 / 10) train acc: 0.302000; val_acc: 0.326000\n",
      "validataion accuracy  0.329\n",
      "\n",
      "79 th try:\n",
      "learning rate is  0.0123895275237\n",
      "regularization strengths is  0.0014456059506\n",
      "weight scale is  0.00346085025467\n",
      "hidden size is  264\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.435380\n",
      "(Epoch 0 / 10) train acc: 0.192000; val_acc: 0.167000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.072000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.089000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.094000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.084000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.111000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.098000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.092000; val_acc: 0.087000\n",
      "validataion accuracy  0.167\n",
      "\n",
      "80 th try:\n",
      "learning rate is  1.24039099205e-06\n",
      "regularization strengths is  0.536490093634\n",
      "weight scale is  0.00551706691218\n",
      "hidden size is  307\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 11.285997\n",
      "(Epoch 0 / 10) train acc: 0.092000; val_acc: 0.078000\n",
      "(Iteration 101 / 4900) loss: 10.877223\n",
      "(Iteration 201 / 4900) loss: 10.512107\n",
      "(Iteration 301 / 4900) loss: 10.348840\n",
      "(Iteration 401 / 4900) loss: 10.636608\n",
      "(Epoch 1 / 10) train acc: 0.119000; val_acc: 0.095000\n",
      "(Iteration 501 / 4900) loss: 10.155117\n",
      "(Iteration 601 / 4900) loss: 10.193926\n",
      "(Iteration 701 / 4900) loss: 10.091147\n",
      "(Iteration 801 / 4900) loss: 10.167009\n",
      "(Iteration 901 / 4900) loss: 10.037501\n",
      "(Epoch 2 / 10) train acc: 0.174000; val_acc: 0.149000\n",
      "(Iteration 1001 / 4900) loss: 10.147061\n",
      "(Iteration 1101 / 4900) loss: 10.209117\n",
      "(Iteration 1201 / 4900) loss: 10.044387\n",
      "(Iteration 1301 / 4900) loss: 10.036766\n",
      "(Iteration 1401 / 4900) loss: 10.137382\n",
      "(Epoch 3 / 10) train acc: 0.169000; val_acc: 0.178000\n",
      "(Iteration 1501 / 4900) loss: 10.102548\n",
      "(Iteration 1601 / 4900) loss: 9.987616\n",
      "(Iteration 1701 / 4900) loss: 10.213536\n",
      "(Iteration 1801 / 4900) loss: 10.033744\n",
      "(Iteration 1901 / 4900) loss: 9.994646\n",
      "(Epoch 4 / 10) train acc: 0.224000; val_acc: 0.186000\n",
      "(Iteration 2001 / 4900) loss: 9.890774\n",
      "(Iteration 2101 / 4900) loss: 9.906377\n",
      "(Iteration 2201 / 4900) loss: 9.844425\n",
      "(Iteration 2301 / 4900) loss: 9.964251\n",
      "(Iteration 2401 / 4900) loss: 9.944502\n",
      "(Epoch 5 / 10) train acc: 0.215000; val_acc: 0.206000\n",
      "(Iteration 2501 / 4900) loss: 10.006429\n",
      "(Iteration 2601 / 4900) loss: 9.832135\n",
      "(Iteration 2701 / 4900) loss: 9.776480\n",
      "(Iteration 2801 / 4900) loss: 10.103978\n",
      "(Iteration 2901 / 4900) loss: 9.825554\n",
      "(Epoch 6 / 10) train acc: 0.227000; val_acc: 0.218000\n",
      "(Iteration 3001 / 4900) loss: 9.879010\n",
      "(Iteration 3101 / 4900) loss: 10.011496\n",
      "(Iteration 3201 / 4900) loss: 9.835254\n",
      "(Iteration 3301 / 4900) loss: 9.781635\n",
      "(Iteration 3401 / 4900) loss: 9.794115\n",
      "(Epoch 7 / 10) train acc: 0.234000; val_acc: 0.227000\n",
      "(Iteration 3501 / 4900) loss: 9.761135\n",
      "(Iteration 3601 / 4900) loss: 9.589364\n",
      "(Iteration 3701 / 4900) loss: 9.769046\n",
      "(Iteration 3801 / 4900) loss: 9.911245\n",
      "(Iteration 3901 / 4900) loss: 9.743029\n",
      "(Epoch 8 / 10) train acc: 0.244000; val_acc: 0.236000\n",
      "(Iteration 4001 / 4900) loss: 9.712908\n",
      "(Iteration 4101 / 4900) loss: 9.830851\n",
      "(Iteration 4201 / 4900) loss: 9.829047\n",
      "(Iteration 4301 / 4900) loss: 9.879462\n",
      "(Iteration 4401 / 4900) loss: 9.664000\n",
      "(Epoch 9 / 10) train acc: 0.251000; val_acc: 0.244000\n",
      "(Iteration 4501 / 4900) loss: 9.884193\n",
      "(Iteration 4601 / 4900) loss: 9.780909\n",
      "(Iteration 4701 / 4900) loss: 9.640110\n",
      "(Iteration 4801 / 4900) loss: 9.849662\n",
      "(Epoch 10 / 10) train acc: 0.264000; val_acc: 0.247000\n",
      "validataion accuracy  0.247\n",
      "\n",
      "81 th try:\n",
      "learning rate is  0.000395605327156\n",
      "regularization strengths is  0.283615430378\n",
      "weight scale is  0.00976159700213\n",
      "hidden size is  68\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 6.738648\n",
      "(Epoch 0 / 10) train acc: 0.133000; val_acc: 0.138000\n",
      "(Iteration 101 / 4900) loss: 4.859532\n",
      "(Iteration 201 / 4900) loss: 4.473146\n",
      "(Iteration 301 / 4900) loss: 4.460218\n",
      "(Iteration 401 / 4900) loss: 4.242351\n",
      "(Epoch 1 / 10) train acc: 0.389000; val_acc: 0.409000\n",
      "(Iteration 501 / 4900) loss: 4.364102\n",
      "(Iteration 601 / 4900) loss: 4.179271\n",
      "(Iteration 701 / 4900) loss: 4.166697\n",
      "(Iteration 801 / 4900) loss: 4.122877\n",
      "(Iteration 901 / 4900) loss: 4.094994\n",
      "(Epoch 2 / 10) train acc: 0.444000; val_acc: 0.427000\n",
      "(Iteration 1001 / 4900) loss: 3.777768\n",
      "(Iteration 1101 / 4900) loss: 3.937233\n",
      "(Iteration 1201 / 4900) loss: 3.714938\n",
      "(Iteration 1301 / 4900) loss: 3.904030\n",
      "(Iteration 1401 / 4900) loss: 3.836586\n",
      "(Epoch 3 / 10) train acc: 0.466000; val_acc: 0.444000\n",
      "(Iteration 1501 / 4900) loss: 3.771140\n",
      "(Iteration 1601 / 4900) loss: 3.425241\n",
      "(Iteration 1701 / 4900) loss: 3.634122\n",
      "(Iteration 1801 / 4900) loss: 3.546838\n",
      "(Iteration 1901 / 4900) loss: 3.415651\n",
      "(Epoch 4 / 10) train acc: 0.450000; val_acc: 0.469000\n",
      "(Iteration 2001 / 4900) loss: 3.457379\n",
      "(Iteration 2101 / 4900) loss: 3.315987\n",
      "(Iteration 2201 / 4900) loss: 3.209880\n",
      "(Iteration 2301 / 4900) loss: 3.056001\n",
      "(Iteration 2401 / 4900) loss: 3.264666\n",
      "(Epoch 5 / 10) train acc: 0.485000; val_acc: 0.477000\n",
      "(Iteration 2501 / 4900) loss: 2.995127\n",
      "(Iteration 2601 / 4900) loss: 3.121119\n",
      "(Iteration 2701 / 4900) loss: 3.059851\n",
      "(Iteration 2801 / 4900) loss: 2.900661\n",
      "(Iteration 2901 / 4900) loss: 3.093739\n",
      "(Epoch 6 / 10) train acc: 0.471000; val_acc: 0.472000\n",
      "(Iteration 3001 / 4900) loss: 3.040421\n",
      "(Iteration 3101 / 4900) loss: 2.872009\n",
      "(Iteration 3201 / 4900) loss: 2.935984\n",
      "(Iteration 3301 / 4900) loss: 2.810929\n",
      "(Iteration 3401 / 4900) loss: 2.890997\n",
      "(Epoch 7 / 10) train acc: 0.503000; val_acc: 0.491000\n",
      "(Iteration 3501 / 4900) loss: 2.912565\n",
      "(Iteration 3601 / 4900) loss: 2.757539\n",
      "(Iteration 3701 / 4900) loss: 2.689642\n",
      "(Iteration 3801 / 4900) loss: 2.660764\n",
      "(Iteration 3901 / 4900) loss: 2.658164\n",
      "(Epoch 8 / 10) train acc: 0.535000; val_acc: 0.496000\n",
      "(Iteration 4001 / 4900) loss: 2.523124\n",
      "(Iteration 4101 / 4900) loss: 2.726617\n",
      "(Iteration 4201 / 4900) loss: 2.686582\n",
      "(Iteration 4301 / 4900) loss: 2.748784\n",
      "(Iteration 4401 / 4900) loss: 2.703497\n",
      "(Epoch 9 / 10) train acc: 0.521000; val_acc: 0.487000\n",
      "(Iteration 4501 / 4900) loss: 2.689756\n",
      "(Iteration 4601 / 4900) loss: 2.682103\n",
      "(Iteration 4701 / 4900) loss: 2.649763\n",
      "(Iteration 4801 / 4900) loss: 2.569148\n",
      "(Epoch 10 / 10) train acc: 0.489000; val_acc: 0.499000\n",
      "validataion accuracy  0.499\n",
      "\n",
      "82 th try:\n",
      "learning rate is  0.0293831961798\n",
      "regularization strengths is  0.00202811172501\n",
      "weight scale is  0.0235098492531\n",
      "hidden size is  56\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 17.092930\n",
      "(Epoch 0 / 10) train acc: 0.115000; val_acc: 0.110000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: nan\n",
      "(Iteration 301 / 4900) loss: nan\n",
      "(Iteration 401 / 4900) loss: nan\n",
      "(Epoch 1 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 501 / 4900) loss: nan\n",
      "(Iteration 601 / 4900) loss: nan\n",
      "(Iteration 701 / 4900) loss: nan\n",
      "(Iteration 801 / 4900) loss: nan\n",
      "(Iteration 901 / 4900) loss: nan\n",
      "(Epoch 2 / 10) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 1001 / 4900) loss: nan\n",
      "(Iteration 1101 / 4900) loss: nan\n",
      "(Iteration 1201 / 4900) loss: nan\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.110000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.086000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.088000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.108000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.112000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.087000; val_acc: 0.087000\n",
      "validataion accuracy  0.11\n",
      "\n",
      "83 th try:\n",
      "learning rate is  4.06882508633e-06\n",
      "regularization strengths is  0.672713687479\n",
      "weight scale is  0.0938201144015\n",
      "hidden size is  77\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: inf\n",
      "(Epoch 0 / 10) train acc: 0.138000; val_acc: 0.144000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: 850.094239\n",
      "(Iteration 301 / 4900) loss: 845.518879\n",
      "(Iteration 401 / 4900) loss: 843.946858\n",
      "(Epoch 1 / 10) train acc: 0.136000; val_acc: 0.160000\n",
      "(Iteration 501 / 4900) loss: inf\n",
      "(Iteration 601 / 4900) loss: 820.441909\n",
      "(Iteration 701 / 4900) loss: 829.979531\n",
      "(Iteration 801 / 4900) loss: 841.541011\n",
      "(Iteration 901 / 4900) loss: 802.749019\n",
      "(Epoch 2 / 10) train acc: 0.175000; val_acc: 0.174000\n",
      "(Iteration 1001 / 4900) loss: 818.442968\n",
      "(Iteration 1101 / 4900) loss: 811.862273\n",
      "(Iteration 1201 / 4900) loss: 822.049779\n",
      "(Iteration 1301 / 4900) loss: 807.528041\n",
      "(Iteration 1401 / 4900) loss: 800.377220\n",
      "(Epoch 3 / 10) train acc: 0.157000; val_acc: 0.185000\n",
      "(Iteration 1501 / 4900) loss: 802.148115\n",
      "(Iteration 1601 / 4900) loss: 785.401815\n",
      "(Iteration 1701 / 4900) loss: 818.981367\n",
      "(Iteration 1801 / 4900) loss: 795.317792\n",
      "(Iteration 1901 / 4900) loss: 780.648676\n",
      "(Epoch 4 / 10) train acc: 0.215000; val_acc: 0.200000\n",
      "(Iteration 2001 / 4900) loss: 794.489518\n",
      "(Iteration 2101 / 4900) loss: 796.140072\n",
      "(Iteration 2201 / 4900) loss: 800.675005\n",
      "(Iteration 2301 / 4900) loss: 777.574974\n",
      "(Iteration 2401 / 4900) loss: 785.102649\n",
      "(Epoch 5 / 10) train acc: 0.197000; val_acc: 0.207000\n",
      "(Iteration 2501 / 4900) loss: 794.922809\n",
      "(Iteration 2601 / 4900) loss: 777.770842\n",
      "(Iteration 2701 / 4900) loss: 779.463572\n",
      "(Iteration 2801 / 4900) loss: 783.434986\n",
      "(Iteration 2901 / 4900) loss: 766.521422\n",
      "(Epoch 6 / 10) train acc: 0.194000; val_acc: 0.206000\n",
      "(Iteration 3001 / 4900) loss: 767.200591\n",
      "(Iteration 3101 / 4900) loss: 772.528010\n",
      "(Iteration 3201 / 4900) loss: 762.172752\n",
      "(Iteration 3301 / 4900) loss: 777.361337\n",
      "(Iteration 3401 / 4900) loss: 773.029042\n",
      "(Epoch 7 / 10) train acc: 0.203000; val_acc: 0.216000\n",
      "(Iteration 3501 / 4900) loss: 773.807567\n",
      "(Iteration 3601 / 4900) loss: 752.254839\n",
      "(Iteration 3701 / 4900) loss: 765.499345\n",
      "(Iteration 3801 / 4900) loss: 765.570468\n",
      "(Iteration 3901 / 4900) loss: 767.008892\n",
      "(Epoch 8 / 10) train acc: 0.226000; val_acc: 0.221000\n",
      "(Iteration 4001 / 4900) loss: 773.684134\n",
      "(Iteration 4101 / 4900) loss: 748.154345\n",
      "(Iteration 4201 / 4900) loss: 752.937456\n",
      "(Iteration 4301 / 4900) loss: 756.373570\n",
      "(Iteration 4401 / 4900) loss: 745.605137\n",
      "(Epoch 9 / 10) train acc: 0.216000; val_acc: 0.228000\n",
      "(Iteration 4501 / 4900) loss: 764.986160\n",
      "(Iteration 4601 / 4900) loss: 757.636560\n",
      "(Iteration 4701 / 4900) loss: 753.411795\n",
      "(Iteration 4801 / 4900) loss: 767.346826\n",
      "(Epoch 10 / 10) train acc: 0.222000; val_acc: 0.235000\n",
      "validataion accuracy  0.235\n",
      "\n",
      "84 th try:\n",
      "learning rate is  9.79226566152e-06\n",
      "regularization strengths is  0.00110003981803\n",
      "weight scale is  0.00206009466918\n",
      "hidden size is  120\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.306403\n",
      "(Epoch 0 / 10) train acc: 0.102000; val_acc: 0.089000\n",
      "(Iteration 101 / 4900) loss: 2.273236\n",
      "(Iteration 201 / 4900) loss: 2.268167\n",
      "(Iteration 301 / 4900) loss: 2.247908\n",
      "(Iteration 401 / 4900) loss: 2.251820\n",
      "(Epoch 1 / 10) train acc: 0.188000; val_acc: 0.227000\n",
      "(Iteration 501 / 4900) loss: 2.210567\n",
      "(Iteration 601 / 4900) loss: 2.196968\n",
      "(Iteration 701 / 4900) loss: 2.188023\n",
      "(Iteration 801 / 4900) loss: 2.192971\n",
      "(Iteration 901 / 4900) loss: 2.159915\n",
      "(Epoch 2 / 10) train acc: 0.267000; val_acc: 0.270000\n",
      "(Iteration 1001 / 4900) loss: 2.140108\n",
      "(Iteration 1101 / 4900) loss: 2.152253\n",
      "(Iteration 1201 / 4900) loss: 2.072804\n",
      "(Iteration 1301 / 4900) loss: 2.055031\n",
      "(Iteration 1401 / 4900) loss: 2.157454\n",
      "(Epoch 3 / 10) train acc: 0.266000; val_acc: 0.288000\n",
      "(Iteration 1501 / 4900) loss: 2.078614\n",
      "(Iteration 1601 / 4900) loss: 2.080305\n",
      "(Iteration 1701 / 4900) loss: 2.017348\n",
      "(Iteration 1801 / 4900) loss: 2.097977\n",
      "(Iteration 1901 / 4900) loss: 2.085061\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.297000\n",
      "(Iteration 2001 / 4900) loss: 2.092869\n",
      "(Iteration 2101 / 4900) loss: 2.082542\n",
      "(Iteration 2201 / 4900) loss: 2.078209\n",
      "(Iteration 2301 / 4900) loss: 1.887765\n",
      "(Iteration 2401 / 4900) loss: 2.049819\n",
      "(Epoch 5 / 10) train acc: 0.308000; val_acc: 0.314000\n",
      "(Iteration 2501 / 4900) loss: 2.015381\n",
      "(Iteration 2601 / 4900) loss: 2.006287\n",
      "(Iteration 2701 / 4900) loss: 2.027850\n",
      "(Iteration 2801 / 4900) loss: 1.995837\n",
      "(Iteration 2901 / 4900) loss: 1.967134\n",
      "(Epoch 6 / 10) train acc: 0.309000; val_acc: 0.323000\n",
      "(Iteration 3001 / 4900) loss: 2.040043\n",
      "(Iteration 3101 / 4900) loss: 2.040848\n",
      "(Iteration 3201 / 4900) loss: 1.851191\n",
      "(Iteration 3301 / 4900) loss: 1.938072\n",
      "(Iteration 3401 / 4900) loss: 1.973311\n",
      "(Epoch 7 / 10) train acc: 0.316000; val_acc: 0.329000\n",
      "(Iteration 3501 / 4900) loss: 1.982969\n",
      "(Iteration 3601 / 4900) loss: 1.948907\n",
      "(Iteration 3701 / 4900) loss: 2.097109\n",
      "(Iteration 3801 / 4900) loss: 1.956909\n",
      "(Iteration 3901 / 4900) loss: 1.973927\n",
      "(Epoch 8 / 10) train acc: 0.314000; val_acc: 0.335000\n",
      "(Iteration 4001 / 4900) loss: 1.907204\n",
      "(Iteration 4101 / 4900) loss: 1.942331\n",
      "(Iteration 4201 / 4900) loss: 1.879045\n",
      "(Iteration 4301 / 4900) loss: 1.926140\n",
      "(Iteration 4401 / 4900) loss: 1.938967\n",
      "(Epoch 9 / 10) train acc: 0.302000; val_acc: 0.339000\n",
      "(Iteration 4501 / 4900) loss: 1.908609\n",
      "(Iteration 4601 / 4900) loss: 1.861520\n",
      "(Iteration 4701 / 4900) loss: 1.820262\n",
      "(Iteration 4801 / 4900) loss: 1.802833\n",
      "(Epoch 10 / 10) train acc: 0.345000; val_acc: 0.343000\n",
      "validataion accuracy  0.343\n",
      "\n",
      "85 th try:\n",
      "learning rate is  9.22024070314e-06\n",
      "regularization strengths is  0.00429158026608\n",
      "weight scale is  0.00386109328703\n",
      "hidden size is  194\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.377574\n",
      "(Epoch 0 / 10) train acc: 0.104000; val_acc: 0.103000\n",
      "(Iteration 101 / 4900) loss: 2.280766\n",
      "(Iteration 201 / 4900) loss: 2.213364\n",
      "(Iteration 301 / 4900) loss: 2.191218\n",
      "(Iteration 401 / 4900) loss: 2.196840\n",
      "(Epoch 1 / 10) train acc: 0.230000; val_acc: 0.247000\n",
      "(Iteration 501 / 4900) loss: 2.143426\n",
      "(Iteration 601 / 4900) loss: 2.038670\n",
      "(Iteration 701 / 4900) loss: 2.172543\n",
      "(Iteration 801 / 4900) loss: 1.995142\n",
      "(Iteration 901 / 4900) loss: 1.923261\n",
      "(Epoch 2 / 10) train acc: 0.311000; val_acc: 0.285000\n",
      "(Iteration 1001 / 4900) loss: 2.020032\n",
      "(Iteration 1101 / 4900) loss: 1.942844\n",
      "(Iteration 1201 / 4900) loss: 1.830712\n",
      "(Iteration 1301 / 4900) loss: 1.985845\n",
      "(Iteration 1401 / 4900) loss: 1.874429\n",
      "(Epoch 3 / 10) train acc: 0.316000; val_acc: 0.318000\n",
      "(Iteration 1501 / 4900) loss: 2.017953\n",
      "(Iteration 1601 / 4900) loss: 1.904304\n",
      "(Iteration 1701 / 4900) loss: 2.069060\n",
      "(Iteration 1801 / 4900) loss: 1.984141\n",
      "(Iteration 1901 / 4900) loss: 1.824250\n",
      "(Epoch 4 / 10) train acc: 0.344000; val_acc: 0.332000\n",
      "(Iteration 2001 / 4900) loss: 1.867184\n",
      "(Iteration 2101 / 4900) loss: 1.919567\n",
      "(Iteration 2201 / 4900) loss: 2.081931\n",
      "(Iteration 2301 / 4900) loss: 1.942597\n",
      "(Iteration 2401 / 4900) loss: 1.846820\n",
      "(Epoch 5 / 10) train acc: 0.357000; val_acc: 0.347000\n",
      "(Iteration 2501 / 4900) loss: 1.836239\n",
      "(Iteration 2601 / 4900) loss: 1.906446\n",
      "(Iteration 2701 / 4900) loss: 1.815975\n",
      "(Iteration 2801 / 4900) loss: 1.684516\n",
      "(Iteration 2901 / 4900) loss: 1.978085\n",
      "(Epoch 6 / 10) train acc: 0.333000; val_acc: 0.353000\n",
      "(Iteration 3001 / 4900) loss: 1.865010\n",
      "(Iteration 3101 / 4900) loss: 1.960041\n",
      "(Iteration 3201 / 4900) loss: 1.818383\n",
      "(Iteration 3301 / 4900) loss: 1.939034\n",
      "(Iteration 3401 / 4900) loss: 1.841516\n",
      "(Epoch 7 / 10) train acc: 0.358000; val_acc: 0.362000\n",
      "(Iteration 3501 / 4900) loss: 1.913323\n",
      "(Iteration 3601 / 4900) loss: 1.720381\n",
      "(Iteration 3701 / 4900) loss: 1.917637\n",
      "(Iteration 3801 / 4900) loss: 1.887719\n",
      "(Iteration 3901 / 4900) loss: 1.829962\n",
      "(Epoch 8 / 10) train acc: 0.392000; val_acc: 0.366000\n",
      "(Iteration 4001 / 4900) loss: 1.794477\n",
      "(Iteration 4101 / 4900) loss: 1.759638\n",
      "(Iteration 4201 / 4900) loss: 1.977205\n",
      "(Iteration 4301 / 4900) loss: 1.816424\n",
      "(Iteration 4401 / 4900) loss: 1.698920\n",
      "(Epoch 9 / 10) train acc: 0.369000; val_acc: 0.365000\n",
      "(Iteration 4501 / 4900) loss: 1.813087\n",
      "(Iteration 4601 / 4900) loss: 1.852816\n",
      "(Iteration 4701 / 4900) loss: 1.749939\n",
      "(Iteration 4801 / 4900) loss: 1.850727\n",
      "(Epoch 10 / 10) train acc: 0.369000; val_acc: 0.375000\n",
      "validataion accuracy  0.375\n",
      "\n",
      "86 th try:\n",
      "learning rate is  0.000122421227862\n",
      "regularization strengths is  0.0569641887013\n",
      "weight scale is  0.00662051701812\n",
      "hidden size is  199\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 4.050652\n",
      "(Epoch 0 / 10) train acc: 0.082000; val_acc: 0.117000\n",
      "(Iteration 101 / 4900) loss: 2.791004\n",
      "(Iteration 201 / 4900) loss: 2.613390\n",
      "(Iteration 301 / 4900) loss: 2.590261\n",
      "(Iteration 401 / 4900) loss: 2.437347\n",
      "(Epoch 1 / 10) train acc: 0.374000; val_acc: 0.376000\n",
      "(Iteration 501 / 4900) loss: 2.384300\n",
      "(Iteration 601 / 4900) loss: 2.696766\n",
      "(Iteration 701 / 4900) loss: 2.441832\n",
      "(Iteration 801 / 4900) loss: 2.550946\n",
      "(Iteration 901 / 4900) loss: 2.416523\n",
      "(Epoch 2 / 10) train acc: 0.408000; val_acc: 0.423000\n",
      "(Iteration 1001 / 4900) loss: 2.237079\n",
      "(Iteration 1101 / 4900) loss: 2.435510\n",
      "(Iteration 1201 / 4900) loss: 2.489152\n",
      "(Iteration 1301 / 4900) loss: 2.258231\n",
      "(Iteration 1401 / 4900) loss: 2.196875\n",
      "(Epoch 3 / 10) train acc: 0.457000; val_acc: 0.427000\n",
      "(Iteration 1501 / 4900) loss: 2.235992\n",
      "(Iteration 1601 / 4900) loss: 2.190364\n",
      "(Iteration 1701 / 4900) loss: 2.414859\n",
      "(Iteration 1801 / 4900) loss: 2.477961\n",
      "(Iteration 1901 / 4900) loss: 2.339614\n",
      "(Epoch 4 / 10) train acc: 0.469000; val_acc: 0.447000\n",
      "(Iteration 2001 / 4900) loss: 2.139086\n",
      "(Iteration 2101 / 4900) loss: 2.184842\n",
      "(Iteration 2201 / 4900) loss: 2.384048\n",
      "(Iteration 2301 / 4900) loss: 2.200784\n",
      "(Iteration 2401 / 4900) loss: 2.172301\n",
      "(Epoch 5 / 10) train acc: 0.485000; val_acc: 0.444000\n",
      "(Iteration 2501 / 4900) loss: 2.175882\n",
      "(Iteration 2601 / 4900) loss: 2.230943\n",
      "(Iteration 2701 / 4900) loss: 2.391679\n",
      "(Iteration 2801 / 4900) loss: 2.235616\n",
      "(Iteration 2901 / 4900) loss: 2.415751\n",
      "(Epoch 6 / 10) train acc: 0.476000; val_acc: 0.457000\n",
      "(Iteration 3001 / 4900) loss: 2.284431\n",
      "(Iteration 3101 / 4900) loss: 2.340303\n",
      "(Iteration 3201 / 4900) loss: 2.041569\n",
      "(Iteration 3301 / 4900) loss: 2.047551\n",
      "(Iteration 3401 / 4900) loss: 2.245655\n",
      "(Epoch 7 / 10) train acc: 0.476000; val_acc: 0.456000\n",
      "(Iteration 3501 / 4900) loss: 2.282649\n",
      "(Iteration 3601 / 4900) loss: 2.209894\n",
      "(Iteration 3701 / 4900) loss: 2.348338\n",
      "(Iteration 3801 / 4900) loss: 2.218596\n",
      "(Iteration 3901 / 4900) loss: 2.278995\n",
      "(Epoch 8 / 10) train acc: 0.510000; val_acc: 0.458000\n",
      "(Iteration 4001 / 4900) loss: 2.021566\n",
      "(Iteration 4101 / 4900) loss: 2.168745\n",
      "(Iteration 4201 / 4900) loss: 2.225139\n",
      "(Iteration 4301 / 4900) loss: 2.162176\n",
      "(Iteration 4401 / 4900) loss: 2.103828\n",
      "(Epoch 9 / 10) train acc: 0.510000; val_acc: 0.469000\n",
      "(Iteration 4501 / 4900) loss: 1.958073\n",
      "(Iteration 4601 / 4900) loss: 2.159775\n",
      "(Iteration 4701 / 4900) loss: 2.170153\n",
      "(Iteration 4801 / 4900) loss: 2.264402\n",
      "(Epoch 10 / 10) train acc: 0.542000; val_acc: 0.479000\n",
      "validataion accuracy  0.479\n",
      "\n",
      "87 th try:\n",
      "learning rate is  0.00365652676253\n",
      "regularization strengths is  0.17762922358\n",
      "weight scale is  0.00236481830509\n",
      "hidden size is  225\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.663146\n",
      "(Epoch 0 / 10) train acc: 0.151000; val_acc: 0.165000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.151000; val_acc: 0.127000\n",
      "(Iteration 501 / 4900) loss: inf\n",
      "(Iteration 601 / 4900) loss: inf\n",
      "(Iteration 701 / 4900) loss: inf\n",
      "(Iteration 801 / 4900) loss: inf\n",
      "(Iteration 901 / 4900) loss: inf\n",
      "(Epoch 2 / 10) train acc: 0.139000; val_acc: 0.145000\n",
      "(Iteration 1001 / 4900) loss: inf\n",
      "(Iteration 1101 / 4900) loss: inf\n",
      "(Iteration 1201 / 4900) loss: inf\n",
      "(Iteration 1301 / 4900) loss: inf\n",
      "(Iteration 1401 / 4900) loss: inf\n",
      "(Epoch 3 / 10) train acc: 0.135000; val_acc: 0.144000\n",
      "(Iteration 1501 / 4900) loss: inf\n",
      "(Iteration 1601 / 4900) loss: inf\n",
      "(Iteration 1701 / 4900) loss: inf\n",
      "(Iteration 1801 / 4900) loss: inf\n",
      "(Iteration 1901 / 4900) loss: inf\n",
      "(Epoch 4 / 10) train acc: 0.092000; val_acc: 0.094000\n",
      "(Iteration 2001 / 4900) loss: inf\n",
      "(Iteration 2101 / 4900) loss: inf\n",
      "(Iteration 2201 / 4900) loss: inf\n",
      "(Iteration 2301 / 4900) loss: inf\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.109000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.119000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.115000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.107000; val_acc: 0.087000\n",
      "validataion accuracy  0.165\n",
      "\n",
      "88 th try:\n",
      "learning rate is  3.41751731756e-05\n",
      "regularization strengths is  0.369337348433\n",
      "weight scale is  0.00592353136423\n",
      "hidden size is  181\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 6.585056\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.150000\n",
      "(Iteration 101 / 4900) loss: 5.838754\n",
      "(Iteration 201 / 4900) loss: 5.646182\n",
      "(Iteration 301 / 4900) loss: 5.496706\n",
      "(Iteration 401 / 4900) loss: 5.317339\n",
      "(Epoch 1 / 10) train acc: 0.315000; val_acc: 0.327000\n",
      "(Iteration 501 / 4900) loss: 5.451052\n",
      "(Iteration 601 / 4900) loss: 5.479111\n",
      "(Iteration 701 / 4900) loss: 5.394086\n",
      "(Iteration 801 / 4900) loss: 5.411151\n",
      "(Iteration 901 / 4900) loss: 5.402845\n",
      "(Epoch 2 / 10) train acc: 0.357000; val_acc: 0.357000\n",
      "(Iteration 1001 / 4900) loss: 5.372047\n",
      "(Iteration 1101 / 4900) loss: 5.258575\n",
      "(Iteration 1201 / 4900) loss: 5.252880\n",
      "(Iteration 1301 / 4900) loss: 5.236331\n",
      "(Iteration 1401 / 4900) loss: 5.456031\n",
      "(Epoch 3 / 10) train acc: 0.400000; val_acc: 0.373000\n",
      "(Iteration 1501 / 4900) loss: 5.273185\n",
      "(Iteration 1601 / 4900) loss: 5.192064\n",
      "(Iteration 1701 / 4900) loss: 5.251396\n",
      "(Iteration 1801 / 4900) loss: 5.161376\n",
      "(Iteration 1901 / 4900) loss: 5.012306\n",
      "(Epoch 4 / 10) train acc: 0.400000; val_acc: 0.385000\n",
      "(Iteration 2001 / 4900) loss: 5.108602\n",
      "(Iteration 2101 / 4900) loss: 5.169422\n",
      "(Iteration 2201 / 4900) loss: 5.092515\n",
      "(Iteration 2301 / 4900) loss: 5.098553\n",
      "(Iteration 2401 / 4900) loss: 5.139120\n",
      "(Epoch 5 / 10) train acc: 0.375000; val_acc: 0.401000\n",
      "(Iteration 2501 / 4900) loss: 5.082701\n",
      "(Iteration 2601 / 4900) loss: 5.050231\n",
      "(Iteration 2701 / 4900) loss: 5.067471\n",
      "(Iteration 2801 / 4900) loss: 5.068243\n",
      "(Iteration 2901 / 4900) loss: 4.875936\n",
      "(Epoch 6 / 10) train acc: 0.441000; val_acc: 0.395000\n",
      "(Iteration 3001 / 4900) loss: 4.989650\n",
      "(Iteration 3101 / 4900) loss: 5.163565\n",
      "(Iteration 3201 / 4900) loss: 5.129885\n",
      "(Iteration 3301 / 4900) loss: 5.083090\n",
      "(Iteration 3401 / 4900) loss: 4.910933\n",
      "(Epoch 7 / 10) train acc: 0.428000; val_acc: 0.404000\n",
      "(Iteration 3501 / 4900) loss: 4.749739\n",
      "(Iteration 3601 / 4900) loss: 4.929590\n",
      "(Iteration 3701 / 4900) loss: 4.893815\n",
      "(Iteration 3801 / 4900) loss: 4.983984\n",
      "(Iteration 3901 / 4900) loss: 4.903574\n",
      "(Epoch 8 / 10) train acc: 0.448000; val_acc: 0.413000\n",
      "(Iteration 4001 / 4900) loss: 4.930633\n",
      "(Iteration 4101 / 4900) loss: 4.824595\n",
      "(Iteration 4201 / 4900) loss: 5.033886\n",
      "(Iteration 4301 / 4900) loss: 4.944587\n",
      "(Iteration 4401 / 4900) loss: 4.858648\n",
      "(Epoch 9 / 10) train acc: 0.441000; val_acc: 0.413000\n",
      "(Iteration 4501 / 4900) loss: 4.890696\n",
      "(Iteration 4601 / 4900) loss: 4.939490\n",
      "(Iteration 4701 / 4900) loss: 5.013026\n",
      "(Iteration 4801 / 4900) loss: 4.908524\n",
      "(Epoch 10 / 10) train acc: 0.417000; val_acc: 0.422000\n",
      "validataion accuracy  0.422\n",
      "\n",
      "89 th try:\n",
      "learning rate is  4.43533428537e-05\n",
      "regularization strengths is  0.0043568365922\n",
      "weight scale is  0.00243804370438\n",
      "hidden size is  256\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.332853\n",
      "(Epoch 0 / 10) train acc: 0.083000; val_acc: 0.084000\n",
      "(Iteration 101 / 4900) loss: 2.175214\n",
      "(Iteration 201 / 4900) loss: 2.156997\n",
      "(Iteration 301 / 4900) loss: 2.005196\n",
      "(Iteration 401 / 4900) loss: 1.910850\n",
      "(Epoch 1 / 10) train acc: 0.314000; val_acc: 0.340000\n",
      "(Iteration 501 / 4900) loss: 2.110256\n",
      "(Iteration 601 / 4900) loss: 1.825601\n",
      "(Iteration 701 / 4900) loss: 1.737556\n",
      "(Iteration 801 / 4900) loss: 1.887546\n",
      "(Iteration 901 / 4900) loss: 1.894190\n",
      "(Epoch 2 / 10) train acc: 0.372000; val_acc: 0.373000\n",
      "(Iteration 1001 / 4900) loss: 1.824735\n",
      "(Iteration 1101 / 4900) loss: 1.686231\n",
      "(Iteration 1201 / 4900) loss: 1.816584\n",
      "(Iteration 1301 / 4900) loss: 1.733196\n",
      "(Iteration 1401 / 4900) loss: 1.625547\n",
      "(Epoch 3 / 10) train acc: 0.398000; val_acc: 0.404000\n",
      "(Iteration 1501 / 4900) loss: 1.756350\n",
      "(Iteration 1601 / 4900) loss: 1.556882\n",
      "(Iteration 1701 / 4900) loss: 1.792846\n",
      "(Iteration 1801 / 4900) loss: 1.612464\n",
      "(Iteration 1901 / 4900) loss: 1.753840\n",
      "(Epoch 4 / 10) train acc: 0.430000; val_acc: 0.425000\n",
      "(Iteration 2001 / 4900) loss: 1.822650\n",
      "(Iteration 2101 / 4900) loss: 1.650209\n",
      "(Iteration 2201 / 4900) loss: 1.720447\n",
      "(Iteration 2301 / 4900) loss: 1.724703\n",
      "(Iteration 2401 / 4900) loss: 1.642531\n",
      "(Epoch 5 / 10) train acc: 0.435000; val_acc: 0.434000\n",
      "(Iteration 2501 / 4900) loss: 1.605030\n",
      "(Iteration 2601 / 4900) loss: 1.643790\n",
      "(Iteration 2701 / 4900) loss: 1.642356\n",
      "(Iteration 2801 / 4900) loss: 1.567205\n",
      "(Iteration 2901 / 4900) loss: 1.541344\n",
      "(Epoch 6 / 10) train acc: 0.434000; val_acc: 0.435000\n",
      "(Iteration 3001 / 4900) loss: 1.622808\n",
      "(Iteration 3101 / 4900) loss: 1.555476\n",
      "(Iteration 3201 / 4900) loss: 1.570297\n",
      "(Iteration 3301 / 4900) loss: 1.693909\n",
      "(Iteration 3401 / 4900) loss: 1.667297\n",
      "(Epoch 7 / 10) train acc: 0.473000; val_acc: 0.443000\n",
      "(Iteration 3501 / 4900) loss: 1.619502\n",
      "(Iteration 3601 / 4900) loss: 1.735839\n",
      "(Iteration 3701 / 4900) loss: 1.648462\n",
      "(Iteration 3801 / 4900) loss: 1.585705\n",
      "(Iteration 3901 / 4900) loss: 1.457467\n",
      "(Epoch 8 / 10) train acc: 0.477000; val_acc: 0.446000\n",
      "(Iteration 4001 / 4900) loss: 1.614274\n",
      "(Iteration 4101 / 4900) loss: 1.660055\n",
      "(Iteration 4201 / 4900) loss: 1.699120\n",
      "(Iteration 4301 / 4900) loss: 1.687089\n",
      "(Iteration 4401 / 4900) loss: 1.482825\n",
      "(Epoch 9 / 10) train acc: 0.440000; val_acc: 0.455000\n",
      "(Iteration 4501 / 4900) loss: 1.634869\n",
      "(Iteration 4601 / 4900) loss: 1.500248\n",
      "(Iteration 4701 / 4900) loss: 1.555895\n",
      "(Iteration 4801 / 4900) loss: 1.465089\n",
      "(Epoch 10 / 10) train acc: 0.473000; val_acc: 0.452000\n",
      "validataion accuracy  0.455\n",
      "\n",
      "90 th try:\n",
      "learning rate is  4.25890004408e-05\n",
      "regularization strengths is  0.00125471868383\n",
      "weight scale is  0.00145250515429\n",
      "hidden size is  253\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.299390\n",
      "(Epoch 0 / 10) train acc: 0.099000; val_acc: 0.095000\n",
      "(Iteration 101 / 4900) loss: 2.192666\n",
      "(Iteration 201 / 4900) loss: 2.179118\n",
      "(Iteration 301 / 4900) loss: 2.115243\n",
      "(Iteration 401 / 4900) loss: 2.130217\n",
      "(Epoch 1 / 10) train acc: 0.299000; val_acc: 0.309000\n",
      "(Iteration 501 / 4900) loss: 2.006004\n",
      "(Iteration 601 / 4900) loss: 1.993210\n",
      "(Iteration 701 / 4900) loss: 2.060519\n",
      "(Iteration 801 / 4900) loss: 1.937636\n",
      "(Iteration 901 / 4900) loss: 1.963390\n",
      "(Epoch 2 / 10) train acc: 0.355000; val_acc: 0.348000\n",
      "(Iteration 1001 / 4900) loss: 1.910894\n",
      "(Iteration 1101 / 4900) loss: 1.937935\n",
      "(Iteration 1201 / 4900) loss: 1.853936\n",
      "(Iteration 1301 / 4900) loss: 1.812824\n",
      "(Iteration 1401 / 4900) loss: 1.749401\n",
      "(Epoch 3 / 10) train acc: 0.343000; val_acc: 0.370000\n",
      "(Iteration 1501 / 4900) loss: 1.828044\n",
      "(Iteration 1601 / 4900) loss: 1.753202\n",
      "(Iteration 1701 / 4900) loss: 1.948865\n",
      "(Iteration 1801 / 4900) loss: 1.772347\n",
      "(Iteration 1901 / 4900) loss: 1.726696\n",
      "(Epoch 4 / 10) train acc: 0.386000; val_acc: 0.388000\n",
      "(Iteration 2001 / 4900) loss: 1.736501\n",
      "(Iteration 2101 / 4900) loss: 1.664993\n",
      "(Iteration 2201 / 4900) loss: 1.590143\n",
      "(Iteration 2301 / 4900) loss: 1.615411\n",
      "(Iteration 2401 / 4900) loss: 1.855763\n",
      "(Epoch 5 / 10) train acc: 0.404000; val_acc: 0.400000\n",
      "(Iteration 2501 / 4900) loss: 1.630837\n",
      "(Iteration 2601 / 4900) loss: 1.798972\n",
      "(Iteration 2701 / 4900) loss: 1.632850\n",
      "(Iteration 2801 / 4900) loss: 1.766757\n",
      "(Iteration 2901 / 4900) loss: 1.706039\n",
      "(Epoch 6 / 10) train acc: 0.416000; val_acc: 0.410000\n",
      "(Iteration 3001 / 4900) loss: 1.698889\n",
      "(Iteration 3101 / 4900) loss: 1.756595\n",
      "(Iteration 3201 / 4900) loss: 1.672634\n",
      "(Iteration 3301 / 4900) loss: 1.723636\n",
      "(Iteration 3401 / 4900) loss: 1.773809\n",
      "(Epoch 7 / 10) train acc: 0.436000; val_acc: 0.422000\n",
      "(Iteration 3501 / 4900) loss: 1.639844\n",
      "(Iteration 3601 / 4900) loss: 1.672193\n",
      "(Iteration 3701 / 4900) loss: 1.581984\n",
      "(Iteration 3801 / 4900) loss: 1.518577\n",
      "(Iteration 3901 / 4900) loss: 1.657170\n",
      "(Epoch 8 / 10) train acc: 0.421000; val_acc: 0.432000\n",
      "(Iteration 4001 / 4900) loss: 1.730864\n",
      "(Iteration 4101 / 4900) loss: 1.585470\n",
      "(Iteration 4201 / 4900) loss: 1.625988\n",
      "(Iteration 4301 / 4900) loss: 1.588085\n",
      "(Iteration 4401 / 4900) loss: 1.707147\n",
      "(Epoch 9 / 10) train acc: 0.437000; val_acc: 0.436000\n",
      "(Iteration 4501 / 4900) loss: 1.698064\n",
      "(Iteration 4601 / 4900) loss: 1.479399\n",
      "(Iteration 4701 / 4900) loss: 1.611857\n",
      "(Iteration 4801 / 4900) loss: 1.706661\n",
      "(Epoch 10 / 10) train acc: 0.452000; val_acc: 0.444000\n",
      "validataion accuracy  0.444\n",
      "\n",
      "91 th try:\n",
      "learning rate is  4.19862148063e-05\n",
      "regularization strengths is  0.0252452594869\n",
      "weight scale is  0.0623845944715\n",
      "hidden size is  247\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: inf\n",
      "(Epoch 0 / 10) train acc: 0.114000; val_acc: 0.118000\n",
      "(Iteration 101 / 4900) loss: 114.884477\n",
      "(Iteration 201 / 4900) loss: 112.822668\n",
      "(Iteration 301 / 4900) loss: 94.387018\n",
      "(Iteration 401 / 4900) loss: 89.381186\n",
      "(Epoch 1 / 10) train acc: 0.284000; val_acc: 0.253000\n",
      "(Iteration 501 / 4900) loss: 94.914140\n",
      "(Iteration 601 / 4900) loss: 90.032652\n",
      "(Iteration 701 / 4900) loss: 91.692040\n",
      "(Iteration 801 / 4900) loss: 77.305658\n",
      "(Iteration 901 / 4900) loss: 77.858703\n",
      "(Epoch 2 / 10) train acc: 0.308000; val_acc: 0.281000\n",
      "(Iteration 1001 / 4900) loss: 76.946637\n",
      "(Iteration 1101 / 4900) loss: 74.067621\n",
      "(Iteration 1201 / 4900) loss: 76.970908\n",
      "(Iteration 1301 / 4900) loss: 62.821226\n",
      "(Iteration 1401 / 4900) loss: 68.624223\n",
      "(Epoch 3 / 10) train acc: 0.312000; val_acc: 0.301000\n",
      "(Iteration 1501 / 4900) loss: 63.894271\n",
      "(Iteration 1601 / 4900) loss: 61.652342\n",
      "(Iteration 1701 / 4900) loss: 60.267343\n",
      "(Iteration 1801 / 4900) loss: 56.522523\n",
      "(Iteration 1901 / 4900) loss: 63.436963\n",
      "(Epoch 4 / 10) train acc: 0.340000; val_acc: 0.312000\n",
      "(Iteration 2001 / 4900) loss: 58.839041\n",
      "(Iteration 2101 / 4900) loss: 55.221999\n",
      "(Iteration 2201 / 4900) loss: 58.514399\n",
      "(Iteration 2301 / 4900) loss: 56.554363\n",
      "(Iteration 2401 / 4900) loss: 53.427561\n",
      "(Epoch 5 / 10) train acc: 0.358000; val_acc: 0.313000\n",
      "(Iteration 2501 / 4900) loss: 55.302369\n",
      "(Iteration 2601 / 4900) loss: 51.321250\n",
      "(Iteration 2701 / 4900) loss: 47.880440\n",
      "(Iteration 2801 / 4900) loss: 52.720905\n",
      "(Iteration 2901 / 4900) loss: 50.511195\n",
      "(Epoch 6 / 10) train acc: 0.348000; val_acc: 0.324000\n",
      "(Iteration 3001 / 4900) loss: 51.728685\n",
      "(Iteration 3101 / 4900) loss: 52.861200\n",
      "(Iteration 3201 / 4900) loss: 49.549028\n",
      "(Iteration 3301 / 4900) loss: 47.022331\n",
      "(Iteration 3401 / 4900) loss: 46.708790\n",
      "(Epoch 7 / 10) train acc: 0.361000; val_acc: 0.316000\n",
      "(Iteration 3501 / 4900) loss: 46.323092\n",
      "(Iteration 3601 / 4900) loss: 47.483692\n",
      "(Iteration 3701 / 4900) loss: 44.898952\n",
      "(Iteration 3801 / 4900) loss: 43.882417\n",
      "(Iteration 3901 / 4900) loss: 43.265128\n",
      "(Epoch 8 / 10) train acc: 0.358000; val_acc: 0.316000\n",
      "(Iteration 4001 / 4900) loss: 44.965487\n",
      "(Iteration 4101 / 4900) loss: 43.936687\n",
      "(Iteration 4201 / 4900) loss: 43.689336\n",
      "(Iteration 4301 / 4900) loss: 43.729573\n",
      "(Iteration 4401 / 4900) loss: 44.063032\n",
      "(Epoch 9 / 10) train acc: 0.344000; val_acc: 0.322000\n",
      "(Iteration 4501 / 4900) loss: 43.700095\n",
      "(Iteration 4601 / 4900) loss: 41.294945\n",
      "(Iteration 4701 / 4900) loss: 42.617095\n",
      "(Iteration 4801 / 4900) loss: 41.507727\n",
      "(Epoch 10 / 10) train acc: 0.348000; val_acc: 0.341000\n",
      "validataion accuracy  0.341\n",
      "\n",
      "92 th try:\n",
      "learning rate is  0.00398091561361\n",
      "regularization strengths is  0.282608768674\n",
      "weight scale is  0.0150815723988\n",
      "hidden size is  61\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 13.771981\n",
      "(Epoch 0 / 10) train acc: 0.172000; val_acc: 0.172000\n",
      "(Iteration 101 / 4900) loss: inf\n",
      "(Iteration 201 / 4900) loss: inf\n",
      "(Iteration 301 / 4900) loss: inf\n",
      "(Iteration 401 / 4900) loss: inf\n",
      "(Epoch 1 / 10) train acc: 0.108000; val_acc: 0.120000\n",
      "(Iteration 501 / 4900) loss: inf\n",
      "(Iteration 601 / 4900) loss: inf\n",
      "(Iteration 701 / 4900) loss: inf\n",
      "(Iteration 801 / 4900) loss: inf\n",
      "(Iteration 901 / 4900) loss: inf\n",
      "(Epoch 2 / 10) train acc: 0.123000; val_acc: 0.142000\n",
      "(Iteration 1001 / 4900) loss: inf\n",
      "(Iteration 1101 / 4900) loss: inf\n",
      "(Iteration 1201 / 4900) loss: inf\n",
      "(Iteration 1301 / 4900) loss: nan\n",
      "(Iteration 1401 / 4900) loss: nan\n",
      "(Epoch 3 / 10) train acc: 0.087000; val_acc: 0.087000\n",
      "(Iteration 1501 / 4900) loss: nan\n",
      "(Iteration 1601 / 4900) loss: nan\n",
      "(Iteration 1701 / 4900) loss: nan\n",
      "(Iteration 1801 / 4900) loss: nan\n",
      "(Iteration 1901 / 4900) loss: nan\n",
      "(Epoch 4 / 10) train acc: 0.107000; val_acc: 0.087000\n",
      "(Iteration 2001 / 4900) loss: nan\n",
      "(Iteration 2101 / 4900) loss: nan\n",
      "(Iteration 2201 / 4900) loss: nan\n",
      "(Iteration 2301 / 4900) loss: nan\n",
      "(Iteration 2401 / 4900) loss: nan\n",
      "(Epoch 5 / 10) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 2501 / 4900) loss: nan\n",
      "(Iteration 2601 / 4900) loss: nan\n",
      "(Iteration 2701 / 4900) loss: nan\n",
      "(Iteration 2801 / 4900) loss: nan\n",
      "(Iteration 2901 / 4900) loss: nan\n",
      "(Epoch 6 / 10) train acc: 0.106000; val_acc: 0.087000\n",
      "(Iteration 3001 / 4900) loss: nan\n",
      "(Iteration 3101 / 4900) loss: nan\n",
      "(Iteration 3201 / 4900) loss: nan\n",
      "(Iteration 3301 / 4900) loss: nan\n",
      "(Iteration 3401 / 4900) loss: nan\n",
      "(Epoch 7 / 10) train acc: 0.121000; val_acc: 0.087000\n",
      "(Iteration 3501 / 4900) loss: nan\n",
      "(Iteration 3601 / 4900) loss: nan\n",
      "(Iteration 3701 / 4900) loss: nan\n",
      "(Iteration 3801 / 4900) loss: nan\n",
      "(Iteration 3901 / 4900) loss: nan\n",
      "(Epoch 8 / 10) train acc: 0.091000; val_acc: 0.087000\n",
      "(Iteration 4001 / 4900) loss: nan\n",
      "(Iteration 4101 / 4900) loss: nan\n",
      "(Iteration 4201 / 4900) loss: nan\n",
      "(Iteration 4301 / 4900) loss: nan\n",
      "(Iteration 4401 / 4900) loss: nan\n",
      "(Epoch 9 / 10) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 4501 / 4900) loss: nan\n",
      "(Iteration 4601 / 4900) loss: nan\n",
      "(Iteration 4701 / 4900) loss: nan\n",
      "(Iteration 4801 / 4900) loss: nan\n",
      "(Epoch 10 / 10) train acc: 0.105000; val_acc: 0.087000\n",
      "validataion accuracy  0.172\n",
      "\n",
      "93 th try:\n",
      "learning rate is  1.8263806725e-05\n",
      "regularization strengths is  0.00102282842267\n",
      "weight scale is  0.00318600755155\n",
      "hidden size is  89\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.387204\n",
      "(Epoch 0 / 10) train acc: 0.122000; val_acc: 0.126000\n",
      "(Iteration 101 / 4900) loss: 2.239125\n",
      "(Iteration 201 / 4900) loss: 2.232409\n",
      "(Iteration 301 / 4900) loss: 2.173410\n",
      "(Iteration 401 / 4900) loss: 2.114820\n",
      "(Epoch 1 / 10) train acc: 0.246000; val_acc: 0.278000\n",
      "(Iteration 501 / 4900) loss: 2.094076\n",
      "(Iteration 601 / 4900) loss: 2.100871\n",
      "(Iteration 701 / 4900) loss: 1.986967\n",
      "(Iteration 801 / 4900) loss: 2.008234\n",
      "(Iteration 901 / 4900) loss: 1.915103\n",
      "(Epoch 2 / 10) train acc: 0.320000; val_acc: 0.324000\n",
      "(Iteration 1001 / 4900) loss: 2.012071\n",
      "(Iteration 1101 / 4900) loss: 1.952307\n",
      "(Iteration 1201 / 4900) loss: 1.880302\n",
      "(Iteration 1301 / 4900) loss: 1.843661\n",
      "(Iteration 1401 / 4900) loss: 1.938120\n",
      "(Epoch 3 / 10) train acc: 0.329000; val_acc: 0.342000\n",
      "(Iteration 1501 / 4900) loss: 1.959919\n",
      "(Iteration 1601 / 4900) loss: 1.993463\n",
      "(Iteration 1701 / 4900) loss: 1.771948\n",
      "(Iteration 1801 / 4900) loss: 1.951267\n",
      "(Iteration 1901 / 4900) loss: 1.811632\n",
      "(Epoch 4 / 10) train acc: 0.335000; val_acc: 0.348000\n",
      "(Iteration 2001 / 4900) loss: 1.918537\n",
      "(Iteration 2101 / 4900) loss: 1.893276\n",
      "(Iteration 2201 / 4900) loss: 1.867315\n",
      "(Iteration 2301 / 4900) loss: 1.781929\n",
      "(Iteration 2401 / 4900) loss: 1.809827\n",
      "(Epoch 5 / 10) train acc: 0.364000; val_acc: 0.360000\n",
      "(Iteration 2501 / 4900) loss: 1.823402\n",
      "(Iteration 2601 / 4900) loss: 1.836896\n",
      "(Iteration 2701 / 4900) loss: 1.952024\n",
      "(Iteration 2801 / 4900) loss: 1.787658\n",
      "(Iteration 2901 / 4900) loss: 1.910129\n",
      "(Epoch 6 / 10) train acc: 0.369000; val_acc: 0.374000\n",
      "(Iteration 3001 / 4900) loss: 1.888597\n",
      "(Iteration 3101 / 4900) loss: 1.930534\n",
      "(Iteration 3201 / 4900) loss: 1.866990\n",
      "(Iteration 3301 / 4900) loss: 1.910303\n",
      "(Iteration 3401 / 4900) loss: 1.840599\n",
      "(Epoch 7 / 10) train acc: 0.372000; val_acc: 0.389000\n",
      "(Iteration 3501 / 4900) loss: 1.876134\n",
      "(Iteration 3601 / 4900) loss: 1.764317\n",
      "(Iteration 3701 / 4900) loss: 1.821272\n",
      "(Iteration 3801 / 4900) loss: 1.765257\n",
      "(Iteration 3901 / 4900) loss: 1.890455\n",
      "(Epoch 8 / 10) train acc: 0.391000; val_acc: 0.391000\n",
      "(Iteration 4001 / 4900) loss: 1.797079\n",
      "(Iteration 4101 / 4900) loss: 1.743881\n",
      "(Iteration 4201 / 4900) loss: 1.817097\n",
      "(Iteration 4301 / 4900) loss: 1.767558\n",
      "(Iteration 4401 / 4900) loss: 1.773764\n",
      "(Epoch 9 / 10) train acc: 0.379000; val_acc: 0.404000\n",
      "(Iteration 4501 / 4900) loss: 1.688448\n",
      "(Iteration 4601 / 4900) loss: 1.712697\n",
      "(Iteration 4701 / 4900) loss: 1.808616\n",
      "(Iteration 4801 / 4900) loss: 1.575038\n",
      "(Epoch 10 / 10) train acc: 0.413000; val_acc: 0.406000\n",
      "validataion accuracy  0.406\n",
      "\n",
      "94 th try:\n",
      "learning rate is  8.18494590916e-06\n",
      "regularization strengths is  0.0395522185963\n",
      "weight scale is  0.0040406658107\n",
      "hidden size is  50\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.405130\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100000\n",
      "(Iteration 101 / 4900) loss: 2.375786\n",
      "(Iteration 201 / 4900) loss: 2.353801\n",
      "(Iteration 301 / 4900) loss: 2.272864\n",
      "(Iteration 401 / 4900) loss: 2.267344\n",
      "(Epoch 1 / 10) train acc: 0.210000; val_acc: 0.194000\n",
      "(Iteration 501 / 4900) loss: 2.265019\n",
      "(Iteration 601 / 4900) loss: 2.206106\n",
      "(Iteration 701 / 4900) loss: 2.187516\n",
      "(Iteration 801 / 4900) loss: 2.272889\n",
      "(Iteration 901 / 4900) loss: 2.204648\n",
      "(Epoch 2 / 10) train acc: 0.246000; val_acc: 0.238000\n",
      "(Iteration 1001 / 4900) loss: 2.194268\n",
      "(Iteration 1101 / 4900) loss: 2.163444\n",
      "(Iteration 1201 / 4900) loss: 2.139544\n",
      "(Iteration 1301 / 4900) loss: 2.125189\n",
      "(Iteration 1401 / 4900) loss: 2.140959\n",
      "(Epoch 3 / 10) train acc: 0.241000; val_acc: 0.264000\n",
      "(Iteration 1501 / 4900) loss: 2.082113\n",
      "(Iteration 1601 / 4900) loss: 2.139998\n",
      "(Iteration 1701 / 4900) loss: 2.133556\n",
      "(Iteration 1801 / 4900) loss: 2.054487\n",
      "(Iteration 1901 / 4900) loss: 2.119387\n",
      "(Epoch 4 / 10) train acc: 0.270000; val_acc: 0.273000\n",
      "(Iteration 2001 / 4900) loss: 1.981935\n",
      "(Iteration 2101 / 4900) loss: 2.157131\n",
      "(Iteration 2201 / 4900) loss: 2.071409\n",
      "(Iteration 2301 / 4900) loss: 1.958565\n",
      "(Iteration 2401 / 4900) loss: 1.994316\n",
      "(Epoch 5 / 10) train acc: 0.265000; val_acc: 0.292000\n",
      "(Iteration 2501 / 4900) loss: 2.180855\n",
      "(Iteration 2601 / 4900) loss: 2.112117\n",
      "(Iteration 2701 / 4900) loss: 2.039317\n",
      "(Iteration 2801 / 4900) loss: 2.077305\n",
      "(Iteration 2901 / 4900) loss: 1.966187\n",
      "(Epoch 6 / 10) train acc: 0.319000; val_acc: 0.298000\n",
      "(Iteration 3001 / 4900) loss: 2.088145\n",
      "(Iteration 3101 / 4900) loss: 1.925626\n",
      "(Iteration 3201 / 4900) loss: 2.026119\n",
      "(Iteration 3301 / 4900) loss: 2.097573\n",
      "(Iteration 3401 / 4900) loss: 2.004265\n",
      "(Epoch 7 / 10) train acc: 0.294000; val_acc: 0.316000\n",
      "(Iteration 3501 / 4900) loss: 1.950951\n",
      "(Iteration 3601 / 4900) loss: 2.029600\n",
      "(Iteration 3701 / 4900) loss: 1.990921\n",
      "(Iteration 3801 / 4900) loss: 2.029695\n",
      "(Iteration 3901 / 4900) loss: 2.042399\n",
      "(Epoch 8 / 10) train acc: 0.320000; val_acc: 0.325000\n",
      "(Iteration 4001 / 4900) loss: 2.034183\n",
      "(Iteration 4101 / 4900) loss: 2.143294\n",
      "(Iteration 4201 / 4900) loss: 1.981701\n",
      "(Iteration 4301 / 4900) loss: 2.007361\n",
      "(Iteration 4401 / 4900) loss: 1.948171\n",
      "(Epoch 9 / 10) train acc: 0.308000; val_acc: 0.328000\n",
      "(Iteration 4501 / 4900) loss: 1.973053\n",
      "(Iteration 4601 / 4900) loss: 1.962364\n",
      "(Iteration 4701 / 4900) loss: 1.914335\n",
      "(Iteration 4801 / 4900) loss: 2.077347\n",
      "(Epoch 10 / 10) train acc: 0.356000; val_acc: 0.335000\n",
      "validataion accuracy  0.335\n",
      "\n",
      "95 th try:\n",
      "learning rate is  0.000158728447155\n",
      "regularization strengths is  0.0276702135643\n",
      "weight scale is  0.0214712445249\n",
      "hidden size is  68\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 14.529692\n",
      "(Epoch 0 / 10) train acc: 0.148000; val_acc: 0.118000\n",
      "(Iteration 101 / 4900) loss: 5.022500\n",
      "(Iteration 201 / 4900) loss: 3.983636\n",
      "(Iteration 301 / 4900) loss: 3.528956\n",
      "(Iteration 401 / 4900) loss: 3.180981\n",
      "(Epoch 1 / 10) train acc: 0.309000; val_acc: 0.338000\n",
      "(Iteration 501 / 4900) loss: 3.257539\n",
      "(Iteration 601 / 4900) loss: 3.246847\n",
      "(Iteration 701 / 4900) loss: 3.442696\n",
      "(Iteration 801 / 4900) loss: 3.082541\n",
      "(Iteration 901 / 4900) loss: 3.170087\n",
      "(Epoch 2 / 10) train acc: 0.353000; val_acc: 0.346000\n",
      "(Iteration 1001 / 4900) loss: 3.356757\n",
      "(Iteration 1101 / 4900) loss: 3.324589\n",
      "(Iteration 1201 / 4900) loss: 3.206101\n",
      "(Iteration 1301 / 4900) loss: 3.036270\n",
      "(Iteration 1401 / 4900) loss: 3.023893\n",
      "(Epoch 3 / 10) train acc: 0.352000; val_acc: 0.372000\n",
      "(Iteration 1501 / 4900) loss: 3.273373\n",
      "(Iteration 1601 / 4900) loss: 2.992170\n",
      "(Iteration 1701 / 4900) loss: 3.079313\n",
      "(Iteration 1801 / 4900) loss: 3.118468\n",
      "(Iteration 1901 / 4900) loss: 3.028449\n",
      "(Epoch 4 / 10) train acc: 0.357000; val_acc: 0.369000\n",
      "(Iteration 2001 / 4900) loss: 3.172866\n",
      "(Iteration 2101 / 4900) loss: 3.052248\n",
      "(Iteration 2201 / 4900) loss: 3.192899\n",
      "(Iteration 2301 / 4900) loss: 3.145036\n",
      "(Iteration 2401 / 4900) loss: 2.939979\n",
      "(Epoch 5 / 10) train acc: 0.355000; val_acc: 0.374000\n",
      "(Iteration 2501 / 4900) loss: 3.017153\n",
      "(Iteration 2601 / 4900) loss: 3.083887\n",
      "(Iteration 2701 / 4900) loss: 2.982856\n",
      "(Iteration 2801 / 4900) loss: 3.052479\n",
      "(Iteration 2901 / 4900) loss: 3.209429\n",
      "(Epoch 6 / 10) train acc: 0.384000; val_acc: 0.386000\n",
      "(Iteration 3001 / 4900) loss: 2.730003\n",
      "(Iteration 3101 / 4900) loss: 3.030842\n",
      "(Iteration 3201 / 4900) loss: 2.944849\n",
      "(Iteration 3301 / 4900) loss: 3.312719\n",
      "(Iteration 3401 / 4900) loss: 3.121023\n",
      "(Epoch 7 / 10) train acc: 0.406000; val_acc: 0.378000\n",
      "(Iteration 3501 / 4900) loss: 2.995171\n",
      "(Iteration 3601 / 4900) loss: 3.028809\n",
      "(Iteration 3701 / 4900) loss: 2.889448\n",
      "(Iteration 3801 / 4900) loss: 3.075373\n",
      "(Iteration 3901 / 4900) loss: 2.982770\n",
      "(Epoch 8 / 10) train acc: 0.397000; val_acc: 0.411000\n",
      "(Iteration 4001 / 4900) loss: 3.033334\n",
      "(Iteration 4101 / 4900) loss: 3.074291\n",
      "(Iteration 4201 / 4900) loss: 2.886974\n",
      "(Iteration 4301 / 4900) loss: 3.166782\n",
      "(Iteration 4401 / 4900) loss: 2.809549\n",
      "(Epoch 9 / 10) train acc: 0.376000; val_acc: 0.410000\n",
      "(Iteration 4501 / 4900) loss: 3.150120\n",
      "(Iteration 4601 / 4900) loss: 2.824197\n",
      "(Iteration 4701 / 4900) loss: 2.965410\n",
      "(Iteration 4801 / 4900) loss: 2.830371\n",
      "(Epoch 10 / 10) train acc: 0.380000; val_acc: 0.414000\n",
      "validataion accuracy  0.414\n",
      "\n",
      "96 th try:\n",
      "learning rate is  1.9931037771e-05\n",
      "regularization strengths is  0.0345308790796\n",
      "weight scale is  0.0107875030543\n",
      "hidden size is  147\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 6.488357\n",
      "(Epoch 0 / 10) train acc: 0.110000; val_acc: 0.085000\n",
      "(Iteration 101 / 4900) loss: 4.882731\n",
      "(Iteration 201 / 4900) loss: 3.599036\n",
      "(Iteration 301 / 4900) loss: 3.740177\n",
      "(Iteration 401 / 4900) loss: 3.377777\n",
      "(Epoch 1 / 10) train acc: 0.282000; val_acc: 0.250000\n",
      "(Iteration 501 / 4900) loss: 3.280392\n",
      "(Iteration 601 / 4900) loss: 3.192536\n",
      "(Iteration 701 / 4900) loss: 3.408614\n",
      "(Iteration 801 / 4900) loss: 3.200731\n",
      "(Iteration 901 / 4900) loss: 3.276102\n",
      "(Epoch 2 / 10) train acc: 0.310000; val_acc: 0.283000\n",
      "(Iteration 1001 / 4900) loss: 3.057756\n",
      "(Iteration 1101 / 4900) loss: 3.239944\n",
      "(Iteration 1201 / 4900) loss: 2.896329\n",
      "(Iteration 1301 / 4900) loss: 3.214498\n",
      "(Iteration 1401 / 4900) loss: 2.969779\n",
      "(Epoch 3 / 10) train acc: 0.295000; val_acc: 0.301000\n",
      "(Iteration 1501 / 4900) loss: 3.202336\n",
      "(Iteration 1601 / 4900) loss: 3.040973\n",
      "(Iteration 1701 / 4900) loss: 3.148275\n",
      "(Iteration 1801 / 4900) loss: 2.888785\n",
      "(Iteration 1901 / 4900) loss: 2.850005\n",
      "(Epoch 4 / 10) train acc: 0.340000; val_acc: 0.324000\n",
      "(Iteration 2001 / 4900) loss: 2.849041\n",
      "(Iteration 2101 / 4900) loss: 2.846869\n",
      "(Iteration 2201 / 4900) loss: 2.715390\n",
      "(Iteration 2301 / 4900) loss: 3.161976\n",
      "(Iteration 2401 / 4900) loss: 2.639064\n",
      "(Epoch 5 / 10) train acc: 0.370000; val_acc: 0.339000\n",
      "(Iteration 2501 / 4900) loss: 2.797516\n",
      "(Iteration 2601 / 4900) loss: 2.883809\n",
      "(Iteration 2701 / 4900) loss: 2.540927\n",
      "(Iteration 2801 / 4900) loss: 2.873162\n",
      "(Iteration 2901 / 4900) loss: 2.632683\n",
      "(Epoch 6 / 10) train acc: 0.367000; val_acc: 0.349000\n",
      "(Iteration 3001 / 4900) loss: 2.864703\n",
      "(Iteration 3101 / 4900) loss: 2.758545\n",
      "(Iteration 3201 / 4900) loss: 2.713020\n",
      "(Iteration 3301 / 4900) loss: 2.938015\n",
      "(Iteration 3401 / 4900) loss: 2.790769\n",
      "(Epoch 7 / 10) train acc: 0.355000; val_acc: 0.363000\n",
      "(Iteration 3501 / 4900) loss: 2.724960\n",
      "(Iteration 3601 / 4900) loss: 2.826780\n",
      "(Iteration 3701 / 4900) loss: 2.692501\n",
      "(Iteration 3801 / 4900) loss: 2.816961\n",
      "(Iteration 3901 / 4900) loss: 2.891782\n",
      "(Epoch 8 / 10) train acc: 0.352000; val_acc: 0.362000\n",
      "(Iteration 4001 / 4900) loss: 2.767272\n",
      "(Iteration 4101 / 4900) loss: 2.709219\n",
      "(Iteration 4201 / 4900) loss: 2.620180\n",
      "(Iteration 4301 / 4900) loss: 2.716721\n",
      "(Iteration 4401 / 4900) loss: 2.640481\n",
      "(Epoch 9 / 10) train acc: 0.369000; val_acc: 0.366000\n",
      "(Iteration 4501 / 4900) loss: 2.424749\n",
      "(Iteration 4601 / 4900) loss: 2.626491\n",
      "(Iteration 4701 / 4900) loss: 2.666968\n",
      "(Iteration 4801 / 4900) loss: 2.821690\n",
      "(Epoch 10 / 10) train acc: 0.369000; val_acc: 0.387000\n",
      "validataion accuracy  0.387\n",
      "\n",
      "97 th try:\n",
      "learning rate is  0.00101967941271\n",
      "regularization strengths is  0.00335740243258\n",
      "weight scale is  0.00154883801463\n",
      "hidden size is  99\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 2.306156\n",
      "(Epoch 0 / 10) train acc: 0.120000; val_acc: 0.122000\n",
      "(Iteration 101 / 4900) loss: 1.767667\n",
      "(Iteration 201 / 4900) loss: 1.920305\n",
      "(Iteration 301 / 4900) loss: 1.779012\n",
      "(Iteration 401 / 4900) loss: 1.591388\n",
      "(Epoch 1 / 10) train acc: 0.417000; val_acc: 0.415000\n",
      "(Iteration 501 / 4900) loss: 1.569490\n",
      "(Iteration 601 / 4900) loss: 1.546626\n",
      "(Iteration 701 / 4900) loss: 1.455775\n",
      "(Iteration 801 / 4900) loss: 1.535258\n",
      "(Iteration 901 / 4900) loss: 1.407123\n",
      "(Epoch 2 / 10) train acc: 0.473000; val_acc: 0.451000\n",
      "(Iteration 1001 / 4900) loss: 1.414284\n",
      "(Iteration 1101 / 4900) loss: 1.419503\n",
      "(Iteration 1201 / 4900) loss: 1.398905\n",
      "(Iteration 1301 / 4900) loss: 1.394422\n",
      "(Iteration 1401 / 4900) loss: 1.370164\n",
      "(Epoch 3 / 10) train acc: 0.497000; val_acc: 0.506000\n",
      "(Iteration 1501 / 4900) loss: 1.410261\n",
      "(Iteration 1601 / 4900) loss: 1.270605\n",
      "(Iteration 1701 / 4900) loss: 1.339898\n",
      "(Iteration 1801 / 4900) loss: 1.461359\n",
      "(Iteration 1901 / 4900) loss: 1.090601\n",
      "(Epoch 4 / 10) train acc: 0.539000; val_acc: 0.490000\n",
      "(Iteration 2001 / 4900) loss: 1.075836\n",
      "(Iteration 2101 / 4900) loss: 1.434286\n",
      "(Iteration 2201 / 4900) loss: 1.337275\n",
      "(Iteration 2301 / 4900) loss: 1.336353\n",
      "(Iteration 2401 / 4900) loss: 1.186998\n",
      "(Epoch 5 / 10) train acc: 0.526000; val_acc: 0.455000\n",
      "(Iteration 2501 / 4900) loss: 1.365109\n",
      "(Iteration 2601 / 4900) loss: 1.241197\n",
      "(Iteration 2701 / 4900) loss: 1.238798\n",
      "(Iteration 2801 / 4900) loss: 1.236973\n",
      "(Iteration 2901 / 4900) loss: 1.321425\n",
      "(Epoch 6 / 10) train acc: 0.554000; val_acc: 0.511000\n",
      "(Iteration 3001 / 4900) loss: 1.379302\n",
      "(Iteration 3101 / 4900) loss: 1.245325\n",
      "(Iteration 3201 / 4900) loss: 1.093746\n",
      "(Iteration 3301 / 4900) loss: 1.074492\n",
      "(Iteration 3401 / 4900) loss: 1.352879\n",
      "(Epoch 7 / 10) train acc: 0.593000; val_acc: 0.504000\n",
      "(Iteration 3501 / 4900) loss: 1.270370\n",
      "(Iteration 3601 / 4900) loss: 1.122580\n",
      "(Iteration 3701 / 4900) loss: 1.459314\n",
      "(Iteration 3801 / 4900) loss: 1.079345\n",
      "(Iteration 3901 / 4900) loss: 0.916395\n",
      "(Epoch 8 / 10) train acc: 0.573000; val_acc: 0.477000\n",
      "(Iteration 4001 / 4900) loss: 0.970184\n",
      "(Iteration 4101 / 4900) loss: 1.216313\n",
      "(Iteration 4201 / 4900) loss: 1.183474\n",
      "(Iteration 4301 / 4900) loss: 1.347867\n",
      "(Iteration 4401 / 4900) loss: 1.156431\n",
      "(Epoch 9 / 10) train acc: 0.623000; val_acc: 0.498000\n",
      "(Iteration 4501 / 4900) loss: 1.015611\n",
      "(Iteration 4601 / 4900) loss: 1.030274\n",
      "(Iteration 4701 / 4900) loss: 1.085335\n",
      "(Iteration 4801 / 4900) loss: 1.294737\n",
      "(Epoch 10 / 10) train acc: 0.599000; val_acc: 0.509000\n",
      "validataion accuracy  0.511\n",
      "\n",
      "98 th try:\n",
      "learning rate is  0.000332962817802\n",
      "regularization strengths is  0.00831375116664\n",
      "weight scale is  0.0125401830001\n",
      "hidden size is  100\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 6.417095\n",
      "(Epoch 0 / 10) train acc: 0.124000; val_acc: 0.099000\n",
      "(Iteration 101 / 4900) loss: 2.595885\n",
      "(Iteration 201 / 4900) loss: 2.108001\n",
      "(Iteration 301 / 4900) loss: 2.092673\n",
      "(Iteration 401 / 4900) loss: 1.918240\n",
      "(Epoch 1 / 10) train acc: 0.370000; val_acc: 0.382000\n",
      "(Iteration 501 / 4900) loss: 1.949415\n",
      "(Iteration 601 / 4900) loss: 1.785624\n",
      "(Iteration 701 / 4900) loss: 1.915095\n",
      "(Iteration 801 / 4900) loss: 1.734093\n",
      "(Iteration 901 / 4900) loss: 1.801048\n",
      "(Epoch 2 / 10) train acc: 0.429000; val_acc: 0.419000\n",
      "(Iteration 1001 / 4900) loss: 1.847579\n",
      "(Iteration 1101 / 4900) loss: 1.775378\n",
      "(Iteration 1201 / 4900) loss: 1.777093\n",
      "(Iteration 1301 / 4900) loss: 1.725120\n",
      "(Iteration 1401 / 4900) loss: 1.821991\n",
      "(Epoch 3 / 10) train acc: 0.429000; val_acc: 0.416000\n",
      "(Iteration 1501 / 4900) loss: 1.867234\n",
      "(Iteration 1601 / 4900) loss: 1.717637\n",
      "(Iteration 1701 / 4900) loss: 1.973622\n",
      "(Iteration 1801 / 4900) loss: 1.884258\n",
      "(Iteration 1901 / 4900) loss: 1.770459\n",
      "(Epoch 4 / 10) train acc: 0.430000; val_acc: 0.431000\n",
      "(Iteration 2001 / 4900) loss: 1.836200\n",
      "(Iteration 2101 / 4900) loss: 1.861276\n",
      "(Iteration 2201 / 4900) loss: 1.728797\n",
      "(Iteration 2301 / 4900) loss: 1.736976\n",
      "(Iteration 2401 / 4900) loss: 1.948242\n",
      "(Epoch 5 / 10) train acc: 0.414000; val_acc: 0.431000\n",
      "(Iteration 2501 / 4900) loss: 1.643547\n",
      "(Iteration 2601 / 4900) loss: 1.642995\n",
      "(Iteration 2701 / 4900) loss: 1.707231\n",
      "(Iteration 2801 / 4900) loss: 1.674872\n",
      "(Iteration 2901 / 4900) loss: 1.787051\n",
      "(Epoch 6 / 10) train acc: 0.468000; val_acc: 0.450000\n",
      "(Iteration 3001 / 4900) loss: 1.611968\n",
      "(Iteration 3101 / 4900) loss: 1.574262\n",
      "(Iteration 3201 / 4900) loss: 1.715430\n",
      "(Iteration 3301 / 4900) loss: 1.626788\n",
      "(Iteration 3401 / 4900) loss: 1.650951\n",
      "(Epoch 7 / 10) train acc: 0.473000; val_acc: 0.443000\n",
      "(Iteration 3501 / 4900) loss: 1.533454\n",
      "(Iteration 3601 / 4900) loss: 1.869040\n",
      "(Iteration 3701 / 4900) loss: 1.385130\n",
      "(Iteration 3801 / 4900) loss: 1.588189\n",
      "(Iteration 3901 / 4900) loss: 1.764997\n",
      "(Epoch 8 / 10) train acc: 0.452000; val_acc: 0.450000\n",
      "(Iteration 4001 / 4900) loss: 1.622951\n",
      "(Iteration 4101 / 4900) loss: 1.645262\n",
      "(Iteration 4201 / 4900) loss: 1.934791\n",
      "(Iteration 4301 / 4900) loss: 1.695136\n",
      "(Iteration 4401 / 4900) loss: 1.765980\n",
      "(Epoch 9 / 10) train acc: 0.469000; val_acc: 0.449000\n",
      "(Iteration 4501 / 4900) loss: 1.509119\n",
      "(Iteration 4601 / 4900) loss: 1.674341\n",
      "(Iteration 4701 / 4900) loss: 1.583105\n",
      "(Iteration 4801 / 4900) loss: 1.684212\n",
      "(Epoch 10 / 10) train acc: 0.507000; val_acc: 0.465000\n",
      "validataion accuracy  0.465\n",
      "\n",
      "99 th try:\n",
      "learning rate is  1.08958053245e-05\n",
      "regularization strengths is  0.977024770082\n",
      "weight scale is  0.00699841783127\n",
      "hidden size is  296\n",
      "Begining training......\n",
      "\n",
      "(Iteration 1 / 4900) loss: 25.936676\n",
      "(Epoch 0 / 10) train acc: 0.083000; val_acc: 0.106000\n",
      "(Iteration 101 / 4900) loss: 24.126810\n",
      "(Iteration 201 / 4900) loss: 23.911334\n",
      "(Iteration 301 / 4900) loss: 23.949681\n",
      "(Iteration 401 / 4900) loss: 24.155471\n",
      "(Epoch 1 / 10) train acc: 0.243000; val_acc: 0.255000\n",
      "(Iteration 501 / 4900) loss: 23.814736\n",
      "(Iteration 601 / 4900) loss: 23.579861\n",
      "(Iteration 701 / 4900) loss: 23.716113\n",
      "(Iteration 801 / 4900) loss: 23.593800\n",
      "(Iteration 901 / 4900) loss: 23.281319\n",
      "(Epoch 2 / 10) train acc: 0.321000; val_acc: 0.288000\n",
      "(Iteration 1001 / 4900) loss: 23.490305\n",
      "(Iteration 1101 / 4900) loss: 23.232694\n",
      "(Iteration 1201 / 4900) loss: 23.396480\n",
      "(Iteration 1301 / 4900) loss: 23.051910\n",
      "(Iteration 1401 / 4900) loss: 23.048928\n",
      "(Epoch 3 / 10) train acc: 0.330000; val_acc: 0.314000\n",
      "(Iteration 1501 / 4900) loss: 23.308387\n",
      "(Iteration 1601 / 4900) loss: 22.995829\n",
      "(Iteration 1701 / 4900) loss: 23.099059\n",
      "(Iteration 1801 / 4900) loss: 23.269340\n",
      "(Iteration 1901 / 4900) loss: 22.890193\n",
      "(Epoch 4 / 10) train acc: 0.335000; val_acc: 0.342000\n",
      "(Iteration 2001 / 4900) loss: 22.838294\n",
      "(Iteration 2101 / 4900) loss: 22.899448\n",
      "(Iteration 2201 / 4900) loss: 22.724318\n",
      "(Iteration 2301 / 4900) loss: 22.786329\n",
      "(Iteration 2401 / 4900) loss: 22.440582\n",
      "(Epoch 5 / 10) train acc: 0.335000; val_acc: 0.348000\n",
      "(Iteration 2501 / 4900) loss: 22.702341\n",
      "(Iteration 2601 / 4900) loss: 22.645505\n",
      "(Iteration 2701 / 4900) loss: 22.527732\n",
      "(Iteration 2801 / 4900) loss: 22.560917\n",
      "(Iteration 2901 / 4900) loss: 22.416666\n",
      "(Epoch 6 / 10) train acc: 0.366000; val_acc: 0.358000\n",
      "(Iteration 3001 / 4900) loss: 22.382619\n",
      "(Iteration 3101 / 4900) loss: 22.325743\n",
      "(Iteration 3201 / 4900) loss: 22.110146\n",
      "(Iteration 3301 / 4900) loss: 22.317117\n",
      "(Iteration 3401 / 4900) loss: 22.190113\n",
      "(Epoch 7 / 10) train acc: 0.384000; val_acc: 0.370000\n",
      "(Iteration 3501 / 4900) loss: 22.279197\n",
      "(Iteration 3601 / 4900) loss: 22.394567\n",
      "(Iteration 3701 / 4900) loss: 21.934630\n",
      "(Iteration 3801 / 4900) loss: 21.990816\n",
      "(Iteration 3901 / 4900) loss: 22.093010\n",
      "(Epoch 8 / 10) train acc: 0.362000; val_acc: 0.372000\n",
      "(Iteration 4001 / 4900) loss: 22.278796\n",
      "(Iteration 4101 / 4900) loss: 22.007455\n",
      "(Iteration 4201 / 4900) loss: 21.725113\n",
      "(Iteration 4301 / 4900) loss: 22.002043\n",
      "(Iteration 4401 / 4900) loss: 22.041139\n",
      "(Epoch 9 / 10) train acc: 0.367000; val_acc: 0.380000\n",
      "(Iteration 4501 / 4900) loss: 22.096902\n",
      "(Iteration 4601 / 4900) loss: 21.895592\n",
      "(Iteration 4701 / 4900) loss: 21.800746\n",
      "(Iteration 4801 / 4900) loss: 21.672240\n",
      "(Epoch 10 / 10) train acc: 0.393000; val_acc: 0.384000\n",
      "validataion accuracy  0.384\n",
      "learning_rate 0.000001  regularization 0.503195  weight_scale 0.096934  hidden_size 84  accuracy 0.147000\n",
      "learning_rate 0.000001  regularization 0.536490  weight_scale 0.005517  hidden_size 307  accuracy 0.247000\n",
      "learning_rate 0.000002  regularization 0.332492  weight_scale 0.004544  hidden_size 116  accuracy 0.294000\n",
      "learning_rate 0.000002  regularization 0.170844  weight_scale 0.003051  hidden_size 113  accuracy 0.258000\n",
      "learning_rate 0.000002  regularization 0.002266  weight_scale 0.021647  hidden_size 143  accuracy 0.224000\n",
      "learning_rate 0.000003  regularization 0.010376  weight_scale 0.003871  hidden_size 68  accuracy 0.281000\n",
      "learning_rate 0.000004  regularization 0.672714  weight_scale 0.093820  hidden_size 77  accuracy 0.235000\n",
      "learning_rate 0.000004  regularization 0.016895  weight_scale 0.014768  hidden_size 98  accuracy 0.272000\n",
      "learning_rate 0.000006  regularization 0.001132  weight_scale 0.003510  hidden_size 101  accuracy 0.315000\n",
      "learning_rate 0.000006  regularization 0.001267  weight_scale 0.021531  hidden_size 58  accuracy 0.266000\n",
      "learning_rate 0.000007  regularization 0.235422  weight_scale 0.001973  hidden_size 142  accuracy 0.310000\n",
      "learning_rate 0.000008  regularization 0.039552  weight_scale 0.004041  hidden_size 50  accuracy 0.335000\n",
      "learning_rate 0.000008  regularization 0.193406  weight_scale 0.051305  hidden_size 306  accuracy 0.290000\n",
      "learning_rate 0.000009  regularization 0.004292  weight_scale 0.003861  hidden_size 194  accuracy 0.375000\n",
      "learning_rate 0.000009  regularization 0.054141  weight_scale 0.004932  hidden_size 220  accuracy 0.380000\n",
      "learning_rate 0.000010  regularization 0.010454  weight_scale 0.097007  hidden_size 88  accuracy 0.264000\n",
      "learning_rate 0.000010  regularization 0.001100  weight_scale 0.002060  hidden_size 120  accuracy 0.343000\n",
      "learning_rate 0.000010  regularization 0.065090  weight_scale 0.002531  hidden_size 90  accuracy 0.351000\n",
      "learning_rate 0.000011  regularization 0.977025  weight_scale 0.006998  hidden_size 296  accuracy 0.384000\n",
      "learning_rate 0.000015  regularization 0.038403  weight_scale 0.003783  hidden_size 298  accuracy 0.409000\n",
      "learning_rate 0.000015  regularization 0.005778  weight_scale 0.001217  hidden_size 130  accuracy 0.346000\n",
      "learning_rate 0.000018  regularization 0.001023  weight_scale 0.003186  hidden_size 89  accuracy 0.406000\n",
      "learning_rate 0.000019  regularization 0.003569  weight_scale 0.017631  hidden_size 136  accuracy 0.356000\n",
      "learning_rate 0.000020  regularization 0.090883  weight_scale 0.005157  hidden_size 145  accuracy 0.400000\n",
      "learning_rate 0.000020  regularization 0.034531  weight_scale 0.010788  hidden_size 147  accuracy 0.387000\n",
      "learning_rate 0.000021  regularization 0.083024  weight_scale 0.001363  hidden_size 83  accuracy 0.350000\n",
      "learning_rate 0.000021  regularization 0.087165  weight_scale 0.061270  hidden_size 54  accuracy 0.329000\n",
      "learning_rate 0.000024  regularization 0.045369  weight_scale 0.006786  hidden_size 173  accuracy 0.416000\n",
      "learning_rate 0.000026  regularization 0.208078  weight_scale 0.011754  hidden_size 51  accuracy 0.368000\n",
      "learning_rate 0.000033  regularization 0.093872  weight_scale 0.001416  hidden_size 50  accuracy 0.395000\n",
      "learning_rate 0.000034  regularization 0.250573  weight_scale 0.013894  hidden_size 55  accuracy 0.372000\n",
      "learning_rate 0.000034  regularization 0.369337  weight_scale 0.005924  hidden_size 181  accuracy 0.422000\n",
      "learning_rate 0.000037  regularization 0.005610  weight_scale 0.002797  hidden_size 303  accuracy 0.458000\n",
      "learning_rate 0.000039  regularization 0.016639  weight_scale 0.086486  hidden_size 73  accuracy 0.268000\n",
      "learning_rate 0.000040  regularization 0.014623  weight_scale 0.002686  hidden_size 69  accuracy 0.433000\n",
      "learning_rate 0.000042  regularization 0.025245  weight_scale 0.062385  hidden_size 247  accuracy 0.341000\n",
      "learning_rate 0.000043  regularization 0.001255  weight_scale 0.001453  hidden_size 253  accuracy 0.444000\n",
      "learning_rate 0.000044  regularization 0.004357  weight_scale 0.002438  hidden_size 256  accuracy 0.455000\n",
      "learning_rate 0.000058  regularization 0.001130  weight_scale 0.052118  hidden_size 177  accuracy 0.332000\n",
      "learning_rate 0.000067  regularization 0.003369  weight_scale 0.002124  hidden_size 236  accuracy 0.476000\n",
      "learning_rate 0.000073  regularization 0.315404  weight_scale 0.003999  hidden_size 163  accuracy 0.470000\n",
      "learning_rate 0.000084  regularization 0.002676  weight_scale 0.001811  hidden_size 61  accuracy 0.470000\n",
      "learning_rate 0.000107  regularization 0.063148  weight_scale 0.026441  hidden_size 239  accuracy 0.387000\n",
      "learning_rate 0.000122  regularization 0.056964  weight_scale 0.006621  hidden_size 199  accuracy 0.479000\n",
      "learning_rate 0.000144  regularization 0.757093  weight_scale 0.001442  hidden_size 85  accuracy 0.478000\n",
      "learning_rate 0.000147  regularization 0.641506  weight_scale 0.009747  hidden_size 115  accuracy 0.463000\n",
      "learning_rate 0.000159  regularization 0.027670  weight_scale 0.021471  hidden_size 68  accuracy 0.414000\n",
      "learning_rate 0.000173  regularization 0.149768  weight_scale 0.096558  hidden_size 92  accuracy 0.267000\n",
      "learning_rate 0.000178  regularization 0.682094  weight_scale 0.003394  hidden_size 96  accuracy 0.509000\n",
      "learning_rate 0.000215  regularization 0.084021  weight_scale 0.002308  hidden_size 192  accuracy 0.514000\n",
      "learning_rate 0.000267  regularization 0.011521  weight_scale 0.003035  hidden_size 128  accuracy 0.531000\n",
      "learning_rate 0.000331  regularization 0.188886  weight_scale 0.039226  hidden_size 187  accuracy 0.373000\n",
      "learning_rate 0.000333  regularization 0.008314  weight_scale 0.012540  hidden_size 100  accuracy 0.465000\n",
      "learning_rate 0.000360  regularization 0.002911  weight_scale 0.004200  hidden_size 172  accuracy 0.515000\n",
      "learning_rate 0.000372  regularization 0.001987  weight_scale 0.002337  hidden_size 232  accuracy 0.528000\n",
      "learning_rate 0.000396  regularization 0.283615  weight_scale 0.009762  hidden_size 68  accuracy 0.499000\n",
      "learning_rate 0.000412  regularization 0.057593  weight_scale 0.006854  hidden_size 304  accuracy 0.524000\n",
      "learning_rate 0.000417  regularization 0.004929  weight_scale 0.062775  hidden_size 150  accuracy 0.325000\n",
      "learning_rate 0.000437  regularization 0.916943  weight_scale 0.078664  hidden_size 185  accuracy 0.448000\n",
      "learning_rate 0.000595  regularization 0.273410  weight_scale 0.032320  hidden_size 51  accuracy 0.430000\n",
      "learning_rate 0.000626  regularization 0.001900  weight_scale 0.091232  hidden_size 53  accuracy 0.242000\n",
      "learning_rate 0.000636  regularization 0.249616  weight_scale 0.004011  hidden_size 75  accuracy 0.499000\n",
      "learning_rate 0.000912  regularization 0.158507  weight_scale 0.009304  hidden_size 296  accuracy 0.514000\n",
      "learning_rate 0.001006  regularization 0.164919  weight_scale 0.074272  hidden_size 72  accuracy 0.351000\n",
      "learning_rate 0.001020  regularization 0.003357  weight_scale 0.001549  hidden_size 99  accuracy 0.511000\n",
      "learning_rate 0.001182  regularization 0.027654  weight_scale 0.009206  hidden_size 121  accuracy 0.507000\n",
      "learning_rate 0.001378  regularization 0.049328  weight_scale 0.044582  hidden_size 122  accuracy 0.390000\n",
      "learning_rate 0.001541  regularization 0.149160  weight_scale 0.059351  hidden_size 227  accuracy 0.376000\n",
      "learning_rate 0.001905  regularization 0.578398  weight_scale 0.054121  hidden_size 78  accuracy 0.490000\n",
      "learning_rate 0.002257  regularization 0.001084  weight_scale 0.002215  hidden_size 250  accuracy 0.492000\n",
      "learning_rate 0.002637  regularization 0.078402  weight_scale 0.067290  hidden_size 80  accuracy 0.328000\n",
      "learning_rate 0.002725  regularization 0.025308  weight_scale 0.006365  hidden_size 136  accuracy 0.448000\n",
      "learning_rate 0.002892  regularization 0.050866  weight_scale 0.008059  hidden_size 181  accuracy 0.297000\n",
      "learning_rate 0.003657  regularization 0.177629  weight_scale 0.002365  hidden_size 225  accuracy 0.165000\n",
      "learning_rate 0.003804  regularization 0.756377  weight_scale 0.008075  hidden_size 66  accuracy 0.172000\n",
      "learning_rate 0.003896  regularization 0.058143  weight_scale 0.034373  hidden_size 214  accuracy 0.159000\n",
      "learning_rate 0.003981  regularization 0.282609  weight_scale 0.015082  hidden_size 61  accuracy 0.172000\n",
      "learning_rate 0.004219  regularization 0.045206  weight_scale 0.058840  hidden_size 110  accuracy 0.156000\n",
      "learning_rate 0.004271  regularization 0.001653  weight_scale 0.041991  hidden_size 111  accuracy 0.158000\n",
      "learning_rate 0.004656  regularization 0.175791  weight_scale 0.001281  hidden_size 74  accuracy 0.173000\n",
      "learning_rate 0.006843  regularization 0.010976  weight_scale 0.001003  hidden_size 107  accuracy 0.201000\n",
      "learning_rate 0.007250  regularization 0.001809  weight_scale 0.033318  hidden_size 97  accuracy 0.146000\n",
      "learning_rate 0.008719  regularization 0.019862  weight_scale 0.016113  hidden_size 61  accuracy 0.087000\n",
      "learning_rate 0.011517  regularization 0.004451  weight_scale 0.007688  hidden_size 176  accuracy 0.132000\n",
      "learning_rate 0.012390  regularization 0.001446  weight_scale 0.003461  hidden_size 264  accuracy 0.167000\n",
      "learning_rate 0.021680  regularization 0.011886  weight_scale 0.036672  hidden_size 102  accuracy 0.115000\n",
      "learning_rate 0.022868  regularization 0.004838  weight_scale 0.002107  hidden_size 129  accuracy 0.087000\n",
      "learning_rate 0.024567  regularization 0.979642  weight_scale 0.011676  hidden_size 89  accuracy 0.094000\n",
      "learning_rate 0.028502  regularization 0.616909  weight_scale 0.001164  hidden_size 95  accuracy 0.136000\n",
      "learning_rate 0.029383  regularization 0.002028  weight_scale 0.023510  hidden_size 56  accuracy 0.110000\n",
      "learning_rate 0.030746  regularization 0.001311  weight_scale 0.001584  hidden_size 216  accuracy 0.116000\n",
      "learning_rate 0.032887  regularization 0.006274  weight_scale 0.007464  hidden_size 160  accuracy 0.113000\n",
      "learning_rate 0.039293  regularization 0.149666  weight_scale 0.012395  hidden_size 107  accuracy 0.147000\n",
      "learning_rate 0.041915  regularization 0.001596  weight_scale 0.009654  hidden_size 177  accuracy 0.121000\n",
      "learning_rate 0.048999  regularization 0.013204  weight_scale 0.051808  hidden_size 131  accuracy 0.099000\n",
      "learning_rate 0.056743  regularization 0.004654  weight_scale 0.056893  hidden_size 133  accuracy 0.087000\n",
      "learning_rate 0.086532  regularization 0.047365  weight_scale 0.020413  hidden_size 208  accuracy 0.096000\n",
      "learning_rate 0.087900  regularization 0.001369  weight_scale 0.005906  hidden_size 134  accuracy 0.107000\n",
      "learning_rate 0.088499  regularization 0.451819  weight_scale 0.038351  hidden_size 202  accuracy 0.089000\n",
      "learning_rate 0.097333  regularization 0.008972  weight_scale 0.054227  hidden_size 284  accuracy 0.087000\n",
      "best val  0.531\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet()\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 50% accuracy on the validation set.                                        #\n",
    "##############################################################################\n",
    "# data = {\n",
    "#     \"X_train\":X_train,\n",
    "#     \"y_train\":y_train,\n",
    "#     \"X_val\":X_val,\n",
    "#     \"y_val\":y_val\n",
    "# }\n",
    "X_t = data['X_train']\n",
    "input_dim = X_t.shape[1] * X_t.shape[2] * X_t.shape[3]\n",
    "Hidden_size = 100\n",
    "num_classes = 10\n",
    "weight_scale = 1e-3\n",
    "count = 100\n",
    "best_val = -1\n",
    "best_solver = None\n",
    "results={}\n",
    "for i in np.arange(count):\n",
    "    # setting parameters\n",
    "    lr = 10 ** np.random.uniform(-6,-1)\n",
    "    reg = 10 ** np.random.uniform(-3,0)\n",
    "    weight_scale = 10 ** np.random.uniform(-3,-1)\n",
    "    hidden_size = int(10 ** np.random.uniform(1.7,2.5))\n",
    "    print \n",
    "    print i,\"th try:\"\n",
    "    print \"learning rate is \",lr\n",
    "    print \"regularization strengths is \", reg\n",
    "    print \"weight scale is \",weight_scale\n",
    "    print \"hidden size is \", hidden_size\n",
    "    print \"Begining training......\"\n",
    "    print \n",
    "    # setting model\n",
    "    model = TwoLayerNet(input_dim=input_dim, hidden_dim=hidden_size,num_classes=num_classes,weight_scale=weight_scale,\n",
    "                       reg=reg)\n",
    "    # setting solver\n",
    "    solver = Solver(model,data,update_rule='sgd',optim_config={\n",
    "                    'learning_rate': lr,\n",
    "                  },\n",
    "                  lr_decay=0.95,\n",
    "                  num_epochs=10, batch_size=100,\n",
    "                  print_every=100)\n",
    "    solver.train()\n",
    "    val_temp = solver.check_accuracy(data['X_val'],data['y_val'], batch_size=100)\n",
    "    print \"validataion accuracy \", val_temp\n",
    "    results[(lr,reg,weight_scale,hidden_size)] = val_temp\n",
    "    if val_temp > best_val:\n",
    "        best_val = val_temp\n",
    "        best_solver = solver\n",
    "\n",
    "for (lr,reg,ws,hs) in sorted(results):\n",
    "    val = results[(lr,reg,ws,hs)]\n",
    "    print \"learning_rate %f  regularization %f  weight_scale %f  hidden_size %d  accuracy %f\" %(lr,reg,ws,hs,val)\n",
    "print \"best val \", best_val\n",
    "pass\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAALJCAYAAAAnCMuGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZLUg/3/ZVl8WV80Z4AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95bc0249d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(best_solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(best_solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(best_solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(best_solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer network\n",
    "Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
    "\n",
    "Read through the `FullyConnectedNet` class in the file `cs231n/classifiers/fc_net.py`.\n",
    "\n",
    "Implement the initialization, the forward pass, and the backward pass. For the moment don't worry about implementing dropout or batch normalization; we will add those features soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-6 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "Initial loss:  99.3500160449\n",
      "W1 relative error: 2.16e-06\n",
      "W2 relative error: 1.07e-08\n",
      "W3 relative error: 1.06e-09\n",
      "b1 relative error: 2.16e-09\n",
      "b2 relative error: 2.80e-09\n",
      "b3 relative error: 1.59e-10\n",
      "Running check with reg =  3.14\n",
      "Initial loss:  2001.81396333\n",
      "W1 relative error: 7.23e-07\n",
      "W2 relative error: 4.66e-06\n",
      "W3 relative error: 1.87e-07\n",
      "b1 relative error: 2.16e-08\n",
      "b2 relative error: 2.61e-08\n",
      "b3 relative error: 1.21e-08\n"
     ]
    }
   ],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print 'Running check with reg = ', reg\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print 'Initial loss: ', loss\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print '%s relative error: %.2e' % (name, rel_error(grad_num, grads[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. You will need to tweak the learning rate and initialization scale, but you should be able to overfit and achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: inf\n",
      "(Epoch 0 / 20) train acc: 0.220000; val_acc: 0.147000\n",
      "(Epoch 1 / 20) train acc: 0.360000; val_acc: 0.122000\n",
      "(Epoch 2 / 20) train acc: 0.440000; val_acc: 0.170000\n",
      "(Epoch 3 / 20) train acc: 0.660000; val_acc: 0.144000\n",
      "(Epoch 4 / 20) train acc: 0.740000; val_acc: 0.161000\n",
      "(Epoch 5 / 20) train acc: 0.760000; val_acc: 0.163000\n",
      "(Iteration 11 / 40) loss: inf\n",
      "(Epoch 6 / 20) train acc: 0.800000; val_acc: 0.143000\n",
      "(Epoch 7 / 20) train acc: 0.960000; val_acc: 0.152000\n",
      "(Epoch 8 / 20) train acc: 0.980000; val_acc: 0.156000\n",
      "(Epoch 9 / 20) train acc: 0.980000; val_acc: 0.156000\n",
      "(Epoch 10 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Iteration 21 / 40) loss: 0.000000\n",
      "(Epoch 11 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Epoch 12 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Iteration 31 / 40) loss: 0.000000\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.154000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAHwCAYAAADNfOnlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd4b45a5490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Use a three-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-2\n",
    "learning_rate = 1e-4\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: inf\n",
      "(Epoch 0 / 20) train acc: 0.200000; val_acc: 0.107000\n",
      "(Epoch 1 / 20) train acc: 0.220000; val_acc: 0.127000\n",
      "(Epoch 2 / 20) train acc: 0.240000; val_acc: 0.114000\n",
      "(Epoch 3 / 20) train acc: 0.400000; val_acc: 0.116000\n",
      "(Epoch 4 / 20) train acc: 0.520000; val_acc: 0.122000\n",
      "(Epoch 5 / 20) train acc: 0.600000; val_acc: 0.134000\n",
      "(Iteration 11 / 40) loss: inf\n",
      "(Epoch 6 / 20) train acc: 0.720000; val_acc: 0.124000\n",
      "(Epoch 7 / 20) train acc: 0.800000; val_acc: 0.128000\n",
      "(Epoch 8 / 20) train acc: 0.900000; val_acc: 0.123000\n",
      "(Epoch 9 / 20) train acc: 0.880000; val_acc: 0.132000\n",
      "(Epoch 10 / 20) train acc: 0.960000; val_acc: 0.132000\n",
      "(Iteration 21 / 40) loss: inf\n",
      "(Epoch 11 / 20) train acc: 0.900000; val_acc: 0.122000\n",
      "(Epoch 12 / 20) train acc: 0.980000; val_acc: 0.132000\n",
      "(Epoch 13 / 20) train acc: 0.980000; val_acc: 0.132000\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.135000\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.135000\n",
      "(Iteration 31 / 40) loss: 0.000000\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.135000\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.135000\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.135000\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.135000\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.135000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAHwCAYAAADNfOnlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X20ZWddH/Dvr5PwooABEyAkwUk1ItEixDFiUYuCkKSU\noO0qYJEXbSOuYn2hoQHU1RdbKalYtFTNUtpQEKSKktrQGMBXakImgSSGEBgRzSuMUhIw1JDk1z/O\nHnPn9r6cycy95zzcz2etu+7Zez/7nO995s6Z7+x99jnV3QEAYEx/Y9EBAAC4/5Q5AICBKXMAAANT\n5gAABqbMAQAMTJkDABiYMgcstaraVVWfrarHHsmx9yPHT1TVfz3S97vOYz29qj6+wfZfrKpXbUcW\nYPkdtegAwBeWqvrsisUvSvJXSe6Zlr+vu99yKPfX3fckeciRHjuy7v7H84yrqpuSvKC7f2drEwGL\npMwBR1R3/3WZmo4u/ePufvd646vqqO6+ezuyMT9/LjAOp1mBbTWdrvyVqnprVX0myQuq6hur6rKq\n+nRV3VpVP1NVR0/jj6qqrqrd0/Kbp+3vqqrPVNUfVtXJhzp22n5mVX2kqm6vqp+tqvdV1Yvn/Dm+\no6qumzK/t6oet2Lbq6rqlqq6o6o+XFVPndY/uaqumtZ/oqrO3+QxXlFV+6f7euGK9W+uqn853X5k\nVV085fhUVf3etP6tSR6T5F3TqecfmSP3TVV1blVdm+Qvq+qVVfUrqzL956r6qXnmCNgeyhywCN+R\n5JeTfEmSX0lyd5IfTHJskqckOSPJ922w/3cl+bEkj0jyZ0n+zaGOrapHJnl7knOnx/2TJKfPE76q\nHp/kvyX5gSTHJXl3kouq6uiq+uop+2nd/bAkZ06PmyQ/m+T8af1XJPnVDR7mxCQPzqyQvTTJz1XV\nw9YYd26Sj005Hp3kR5Oku5+f5JYkZ3b3Q7r7dRvlXnF/z5syHzON/bsHHreqHpDkuUneNM88AdtD\nmQMW4Q+6+390973d/bnuvqK7L+/uu7v7Y0kuSPJ3Ntj/V7t7b3d/Pslbkjzxfox9VpIPdvc7p20/\nneTP58z/vCQXdfd7p31fk1kx/YbMiumDknz1dKryT6afKUk+n+SUqvrS7v5Md1++wWP83yQ/0d2f\n7+6LMnvt4VeuMe7zmRW+x3b3Xd39e/cz9wGv7+6bpj+Xm5L8YZK/P207K8nN3X31Bo8BbDNlDliE\nG1cuVNVXVdX/rKrbquqOJP86s6Nl67ltxe07s/FFD+uNfczKHN3dSW6aI/uBff90xb73Tvue0N03\nJHl5Zj/DJ6fTyY+ehr4kyalJbqiq91fVWRs8xp9PF3SslX2l10xZ3lNVf1xV596f3CvG3LhqnwuT\nvGC6/YLMjtYBS0SZAxahVy3/QpI/SvIV0ynIH09SW5zh1sxOZSZJqqpycKnZyC1JvmzFvn9juq+b\nk6S739zdT0lycpJdSX5yWn9Ddz8vySOT/FSSX6uqBx3OD9Hdd3T3D3f37iTPSfIvqurAUc3V87xh\n7nX2eUeSr5tOH5+Z2dFNYIkoc8AyeGiS2zN70f3js/Hr5Y6U30xyWlX9vao6KrPX7B03575vT/Ls\nqnrq9Hqzc5N8JsnlVfX4qvrWqnpgks9NX/cmSVV9d1UdOx0Ruz2z4nTv4fwQU/4vn8ro7Zm9DcyB\n+/xEkr85T+717r+770zy60nemuR93X3L4eQFjjxlDlgGL0/yosyKxS9kdlHEluruT2T2Yv7XJfmL\nJF+e5AOZvTZts32vyyzvzyXZn9kFG8+eXof2wCSvzez1d7cleXiSV0+7npXk+ukq3v+Q5Lndfddh\n/iiPS/LeJJ9N8r7MXvP2+9O2f5fkX01Xrv7QJrk3cmGSvxWnWGEp1exlIgA7W1Xtyuw05D9YUYZI\nUlV/M8k1SR7V3X+56DzAwRyZA3asqjqjqo6ZTon+WGZXhr5/wbGWyvS6uh9J8suKHCwnnwAB7GTf\nlNn73R2V5Lok39Hdm55m3Smq6ksyuzji40meudg0wHqcZgUAGJjTrAAAA1PmAAAGtqNeM3fsscf2\n7t27Fx0DAGBTV1555Z9396bvf7mjytzu3buzd+/eRccAANhUVf3p5qOcZgUAGJoyBwAwMGUOAGBg\nyhwAwMCUOQCAgSlzAAADU+YAAAamzAEADEyZAwAYmDIHADAwZQ4AYGDKHADAwJQ5AICBKXMAAANT\n5gAABqbMAQAMTJkDABiYMgcAMDBlDgBgYMocAMDAlDkAgIEpcwAAA1PmAAAGpswBAAxMmQMAGJgy\nBwAwMGUOAGBgyhwAwMCUOQCAgSlzAAADU+YAAAamzAEADEyZAwAYmDIHADAwZQ4AYGDKHADAwJQ5\nAICBKXMAAANT5gAABqbMAQAMTJkDABiYMgcAMDBlDgBgYMocAMDAlDkAgIEpcwAAA1PmAAAGpswB\nAAxMmQMAGJgyBwAwMGUOAGBgyhwAwMCUOQCAgSlzAAADW2iZq6ozquqGqtpXVeetsb2q6mem7ddU\n1Wmrtu+qqg9U1W9uX2oAgOWxsDJXVbuSvCHJmUlOTfL8qjp11bAzk5wyfZ2T5OdWbf/BJNdvcVQA\ngKW1yCNzpyfZ190f6+67krwtydmrxpyd5E09c1mSY6rq+CSpqhOT/N0kv7idoQEAlskiy9wJSW5c\nsXzTtG7eMf8xySuS3LvRg1TVOVW1t6r27t+///ASAwAsmSEvgKiqZyX5ZHdfudnY7r6gu/d0957j\njjtuG9IBAGyfRZa5m5OctGL5xGndPGOekuTZVfXxzE7PfltVvXnrogIALKdFlrkrkpxSVSdX1QOS\nPC/JRavGXJTkhdNVrU9Ocnt339rdr+zuE7t797Tfe7v7BduaHgBgCRy1qAfu7rur6mVJLkmyK8kb\nu/u6qnrptP3nk1yc5Kwk+5LcmeQli8oLALCMqrsXnWHb7Nmzp/fu3bvoGAAAm6qqK7t7z2bjhrwA\nAgCAGWUOAGBgyhwAwMCUOQCAgSlzAAADU+YAAAamzAEADEyZAwAYmDIHADAwZQ4AYGDKHADAwJQ5\nAICBKXMAAANT5gAABqbMAQAMTJkDABiYMgcAMDBlDgBgYMocAMDAlDkAgIEpcwAAA1PmAAAGpswB\nAAxMmQMAGJgyBwAwMGUOAGBgyhwAwMCUOQCAgSlzAAADU+YAAAamzAEADEyZAwAYmDIHADAwZQ4A\nYGDKHADAwJQ5AICBKXMAAANT5gAABqbMAQAMTJkDABiYMgcAMDBlDgBgYMocAMDAlDkAgIEpcwAA\nA1PmAAAGpswBAAxMmQMAGJgyBwAwMGUOAGBgyhwAwMCUOQCAgSlzAAADU+YAAAamzAEADEyZAwAY\nmDIHADAwZQ4AYGDKHADAwJQ5AICBKXMAAANT5gAABqbMAQAMTJkDABiYMgcAMDBlDgBgYMocAMDA\nlDkAgIEpcwAAA1PmAAAGttAyV1VnVNUNVbWvqs5bY3tV1c9M26+pqtOm9SdV1W9X1Yeq6rqq+sHt\nTw8AsHgLK3NVtSvJG5KcmeTUJM+vqlNXDTszySnT1zlJfm5af3eSl3f3qUmenOSfrrEvAMAXvEUe\nmTs9yb7u/lh335XkbUnOXjXm7CRv6pnLkhxTVcd3963dfVWSdPdnklyf5ITtDA8AsAwWWeZOSHLj\niuWb8v8Xsk3HVNXuJE9KcvlaD1JV51TV3qrau3///sOMDACwXIa+AKKqHpLk15L8UHffsdaY7r6g\nu/d0957jjjtuewMCAGyxRZa5m5OctGL5xGndXGOq6ujMitxbuvsdW5gTAGBpLbLMXZHklKo6uaoe\nkOR5SS5aNeaiJC+crmp9cpLbu/vWqqokv5Tk+u5+3fbGBgBYHkct6oG7++6qelmSS5LsSvLG7r6u\nql46bf/5JBcnOSvJviR3JnnJtPtTknx3kmur6oPTuld198Xb+TMAACxadfeiM2ybPXv29N69excd\nAwBgU1V1ZXfv2Wzc0BdAAADsdMocAMDAlDkAgIEpcwAAA1PmAAAGpswBAAxMmQMAGJgyBwAwMGUO\nAGBgyhwAwMCUOQCAgSlzAAADU+YAAAamzAEADEyZAwAYmDIHADAwZQ4AYGDKHADAwJQ5AICBKXMA\nAANT5gAABqbMAQAMTJkDABiYMgcAMDBlDgBgYMocAMDAlDkAgIEpcwAAA1PmAAAGpswBAAxMmQMA\nGJgyBwAwMGUOAGBgyhwAwMCUOQCAgSlzAAADU+YAAAamzAEADEyZAwAYmDIHADAwZQ4AYGDKHADA\nwA6pzNXMF29VGAAADs2mZa6q3lRVD6uqL0pybZJ9VfUjWx8NAIDNzHNk7gndfUeS5yS5NMmXJXnx\nVoYCAGA+85S5o6vqqCRnJ3lnd9+V5N6tjQUAwDzmKXO/mOTPkjw8ye9W1WOTfHZLUwEAMJdNy1x3\n/3R3P6a7n9HdneTGJN+29dEAANjMPBdAvKyqHjbd/oUklyf55q0OBgDA5uY5zXpOd99RVc9I8qgk\n/yTJa7c2FgAA85inzPX0/awk/627r55zPwAAttg8pezqqro4ybOSvKuqHpL7Ch4AAAt01BxjXpLk\n65Ls6+47q+rYJN+7tbEAAJjHpmWuu++ZCtx3VlWS/G53v2vLkwEAsKl5rmb9t0lekeRj09e5VfUT\nWx0MAIDNzXOa9e8lOa27706SqnpjkquS/OhWBgMAYHPzXpX60HVuAwCwQPMcmXttkquq6j1JKslT\nk/zYVoYCAGA+81wA8eaq+u0k3zCt+vHuvnlrYwEAMI91y1xVPWHVqn3T9y+tqi/t7mu2LhYAAPPY\n6MjcGzbY1km+5QhnAQDgEK1b5rr7m7czCAAAh85nrAIADEyZAwAYmDIHADCwTd+aZI2rWpPk9iQ3\ndve9Rz4SAADzmudNg38pyROTXJfZmwY/PsmHkjy0qs7p7vdsYT4AADYwz2nWjyf5uu5+Ynd/bZKv\nS/KRJM9M8lNbmA0AgE3MU+Yev/INgrv72iSndve+DfYBAGAbzHOa9cNV9bNJ3jYtP3da98Akd29Z\nMgAANjVPmXthkh9Ict60/L4kr8ysyD3tcB68qs5I8voku5L8Yne/ZtX2mrafleTOJC/u7qvm2Xe7\n/cYHbs75l9yQWz79uTzmmAfn3Gc+Ls950gk7NseyWJb5WIYcy5BBDjmWPYMccoySY6Xq7sU8cNWu\nzF579+1JbkpyRZLnd/eHVow5K7MieVaSb0jy+u7+hnn2XcuePXt67969R/xn+Y0P3JxXvuPafO7z\n9/z1ugcfvSs/+Z1/a1v/gJclx7JYlvlYhhzLkEEOOZY9gxxyLFuOqrqyu/dsNm7T18xV1ZOr6l1V\n9aGq+siBryOQ8fQk+7r7Y919V2ancc9eNebsJG/qmcuSHFNVx8+577Y5/5IbDvqDTZLPff6enH/J\nDTsyx7JYlvlYhhzLkEEOOZY9gxxyjJJjtXlOs/6XJK9IcmWSezYZeyhOSHLjiuWbMjv6ttmYE+bc\nN0lSVeckOSdJHvvYxx5e4nXc8unPHdL6rbIsOZbFsszHMuRYhgxyyLHsGeSQY5Qcq81zNesd3f0/\nuvuW7v7Ega8tT3aEdPcF3b2nu/ccd9xxW/IYjznmwYe0fqssS45lsSzzsQw5liGDHHIsewY55Bgl\nx2rzlLn3VtVPVtXXV9UTDnwdgce+OclJK5ZPnNbNM2aefbfNuc98XB589K6D1j346F0595mP25E5\nlsWyzMcy5FiGDHLIsewZ5JBjlByrzXOa9ZtWfU+STvIth/nYVyQ5papOzqyIPS/Jd60ac1GSl1XV\n2zI7jXp7d99aVfvn2HfbHHjR46KvblmWHMtiWeZjGXIsQwY55Fj2DHLIMUqO1RZ2NWvy11er/sfM\n3l7kjd39b6vqpUnS3T8/vTXJf0pyRmZvTfKS7t673r6bPd5WXc0KAHCkzXs167plrqqe391vrap/\nttb27v6Zw8y47ZQ5AGAU85a5jU6zPnz6vjVXDQAAcNjWLXPd/Z+n7z+2fXEAADgUm14AUVXHJvme\nJLtXju/uc7YuFgAA85jnatZ3JrksyR/kyL5pMAAAh2meMvfF3f3yLU8CAMAhm+dNg99VVc/Y8iQA\nAByyecrcS5P8r6r6bFV9qqr+T1V9aquDAQCwuXlOsx675SkAALhf1i1zVXVKd380yVevM+SarYkE\nAMC8Njoyd16S703yhjW2HYnPZgUA4DBt9KbB3zt9/+btiwMAwKGY5zVzqaqvSnJqkgcdWNfdv7xV\noQAAmM88nwDxo0mekeSrklyS5JmZvYGwMgcAsGDzvDXJc5N8a5Jbu/u7k3xtki/e0lQAAMxlnjL3\nue6+J8ndVfXQJLcl+bKtjQUAwDzmec3cB6rqmCRvTLI3yR1J3r+lqQAAmMuGZa6qKsm/7O5PJ3lD\nVV2S5GHdfdW2pAMAYEMblrnu7qq6NMnXTMv7tiUVAABzmec1cx+sqidteRIAAA7ZRh/ndVR3353k\nSUmuqKo/TvKXSSqzg3anbVNGAADWsdFp1vcnOS3Js7cpCwAAh2ijMldJ0t1/vE1ZAAA4RBuVueOq\n6kfW29jdr9uCPAAAHIKNytyuJA/JdIQOAIDls1GZu7W7//W2JQEA4JBt9NYkjsgBACy5jcrc07Yt\nBQAA98u6Za67P7WdQQAAOHTzfAIEAABLSpkDABiYMgcAMDBlDgBgYMocAMDAlDkAgIEpcwAAA1Pm\nAAAGpswBAAxMmQMAGJgyBwAwMGUOAGBgyhwAwMCUOQCAgSlzAAADU+YAAAamzAEADEyZAwAYmDIH\nADAwZQ4AYGDKHADAwJQ5AICBKXMAAANT5gAABqbMAQAMTJkDABiYMgcAMDBlDgBgYMocAMDAlDkA\ngIEpcwAAA1PmAAAGpswBAAxMmQMAGJgyBwAwMGUOAGBgyhwAwMCUOQCAgSlzAAADU+YAAAamzAEA\nDEyZAwAY2ELKXFU9oqouraqPTt8fvs64M6rqhqraV1XnrVh/flV9uKquqapfr6pjti89AMDyWNSR\nufOSvKe7T0nynmn5IFW1K8kbkpyZ5NQkz6+qU6fNlyb5mu5+QpKPJHnltqQGAFgyiypzZye5cLp9\nYZLnrDHm9CT7uvtj3X1XkrdN+6W7f6u7757GXZbkxC3OCwCwlBZV5h7V3bdOt29L8qg1xpyQ5MYV\nyzdN61b7niTvWu+BquqcqtpbVXv3799/f/MCACylo7bqjqvq3UkevcamV69c6O6uqr6fj/HqJHcn\nect6Y7r7giQXJMmePXvu1+MAACyrLStz3f309bZV1Seq6vjuvrWqjk/yyTWG3ZzkpBXLJ07rDtzH\ni5M8K8nTultJAwB2pEWdZr0oyYum2y9K8s41xlyR5JSqOrmqHpDkedN+qaozkrwiybO7+85tyAsA\nsJQWVeZek+Tbq+qjSZ4+LaeqHlNVFyfJdIHDy5JckuT6JG/v7uum/f9TkocmubSqPlhVP7/dPwAA\nwDLYstOsG+nuv0jytDXW35LkrBXLFye5eI1xX7GlAQEABuETIAAABqbMAQAMTJkDABiYMgcAMDBl\nDgBgYMocAMDAlDkAgIEpcwAAA1PmAAAGpswBAAxMmQMAGJgyBwAwMGUOAGBgyhwAwMCUOQCAgSlz\nAAADU+YAAAamzAEADEyZAwAYmDIHADAwZQ4AYGDKHADAwJQ5AICBKXMAAANT5gAABqbMAQAMTJkD\nABiYMgcAMDBlDgBgYMocAMDAlDkAgIEpcwAAA1PmAAAGpswBAAxMmQMAGJgyBwAwMGUOAGBgyhwA\nwMCUOQCAgSlzAAADU+YAAAamzAEADEyZAwAYmDIHADAwZQ4AYGDKHADAwJQ5AICBKXMAAANT5gAA\nBqbMAQAMTJkDABiYMgcAMDBlDgBgYMocAMDAlDkAgIEpcwAAA1PmAAAGpswBAAxMmQMAGJgyBwAw\nMGUOAGBgyhwAwMCUOQCAgSlzAAADU+YAAAamzAEADEyZAwAYmDIHADAwZQ4AYGDKHADAwBZS5qrq\nEVV1aVV9dPr+8HXGnVFVN1TVvqo6b43tL6+qrqpjtz41AMDyWdSRufOSvKe7T0nynmn5IFW1K8kb\nkpyZ5NQkz6+qU1dsPynJM5L82bYkBgBYQosqc2cnuXC6fWGS56wx5vQk+7r7Y919V5K3Tfsd8NNJ\nXpGktzIoAMAyW1SZe1R33zrdvi3Jo9YYc0KSG1cs3zStS1WdneTm7r56sweqqnOqam9V7d2/f/9h\nxgYAWC5HbdUdV9W7kzx6jU2vXrnQ3V1Vcx9dq6ovSvKqzE6xbqq7L0hyQZLs2bPHUTwA4AvKlpW5\n7n76etuq6hNVdXx331pVxyf55BrDbk5y0orlE6d1X57k5CRXV9WB9VdV1endfdsR+wEAAAawqNOs\nFyV50XT7RUneucaYK5KcUlUnV9UDkjwvyUXdfW13P7K7d3f37sxOv56myAEAO9Giytxrknx7VX00\nydOn5VTVY6rq4iTp7ruTvCzJJUmuT/L27r5uQXkBAJbSlp1m3Uh3/0WSp62x/pYkZ61YvjjJxZvc\n1+4jnQ8AYBQ+AQIAYGDKHADAwJQ5AICBKXMAAANT5gAABqbMAQAMTJkDABiYMgcAMDBlDgBgYMoc\nAMDAlDkAgIEpcwAAA1PmAAAGpswBAAxMmQMAGJgyBwAwMGUOAGBgyhwAwMCUOQCAgSlzAAADU+YA\nAAamzAEADEyZAwAYmDIHADAwZQ4AYGDKHADAwJQ5AICBKXMAAANT5gAABqbMAQAMTJkDABiYMgcA\nMDBlDgBgYMocAMDAlDkAgIEpcwAAA1PmAAAGpswBAAxMmQMAGJgyBwAwMGUOAGBgyhwAwMCUOQCA\ngSlzAAADU+YAAAamzAEADEyZAwAYmDIHADAwZQ4AYGDKHADAwJQ5AICBKXMAAANT5gAABqbMAQAM\nrLp70Rm2TVXtT/KnW/wwxyb58y1+jJGYj4OZj/uYi4OZj/uYi4OZj4PtpPn4su4+brNBO6rMbYeq\n2tvdexadY1mYj4OZj/uYi4OZj/uYi4OZj4OZj/+f06wAAANT5gAABqbMHXkXLDrAkjEfBzMf9zEX\nBzMf9zEXBzMfBzMfq3jNHADAwByZAwAYmDJ3GKrqpKr67ar6UFVdV1U/uGr7y6uqq+rYRWXcLhvN\nRVX9QFV9eFr/2kXm3C7rzUdVPbGqLquqD1bV3qo6fdFZt0NVPaiq3l9VV0/z8a+m9Y+oqkur6qPT\n94cvOutW22Auzp/+nlxTVb9eVccsOut2WG8+VmzfSc+j687FDn0eXe/vyo58Ht1Qd/u6n19Jjk9y\n2nT7oUk+kuTUafmkJJdk9r52xy4666LmIsm3Jnl3kgdO2x656KwLno/fSnLmtP6sJL+z6KzbNB+V\n5CHT7aOTXJ7kyUlem+S8af15Sf79orMucC6ekeSoaf2/3wlzsdF8TMs77Xl0vd+Nnfo8ut587Mjn\n0Y2+HJk7DN19a3dfNd3+TJLrk5wwbf7pJK9IsiNelLjBXHx/ktd0919N2z65uJTbZ4P56CQPm4Z9\nSZJbFpNwe/XMZ6fFo6evTnJ2kgun9Rcmec4C4m2r9eaiu3+ru++e1l+W5MSFBNxmG/xuJDvveXS9\nudipz6PrzceOfB7diDJ3hFTV7iRPSnJ5VZ2d5ObuvnqhoRZk5Vwk+cok31xVl1fV71bV1y8y2yKs\nmo8fSnJ+Vd2Y5D8keeXikm2vqtpVVR9M8skkl3b35Uke1d23TkNuS/KohQXcRuvMxUrfk+Rd259s\nMdaaj536PLrO78aOfR5dZz527PPoepS5I6CqHpLk1zL7Bbs7yauS/PhCQy3Iyrno7juSHJXkEZkd\nGj83ydurqhYYcVutMR/fn+SHu/ukJD+c5JcWmW87dfc93f3EzI44nV5VX7Nq+4H/cX/B22guqurV\nmT2PvGVR+bbbGvPxhOzQ59F1fjd27PPoOvOxY59H16PMHaaqOjqzf6zf0t3vSPLlSU5OcnVVfTyz\nX8CrqurRi0u5PdaYiyS5Kck7psPl709yb2afq/cFb535eFGSA7f/e5Id98Ld7v50kt9OckaST1TV\n8Ukyfd8Rp48OWDUXqaoXJ3lWkn80ldsdZcV8nJ0d+jx6wKrfjR37PHrAqvnY8c+jqylzh2H6n9Ev\nJbm+u1+XJN19bXc/srt3d/fuzP4Sntbdty0w6pZbay4mv5HZi3dTVV+Z5AHZAR+QvMF83JLk70y3\nvy3JR7c72yJU1XEHrs6sqgcn+fYkH05yUWZPzJm+v3MxCbfPenNRVWdk9vqwZ3f3nYvMuJ3WmY8P\n7NDn0fX+nuzU59H15mNHPo9u5KhFBxjcU5J8d5Jrp3P6SfKq7r54gZkWZc25SPLGJG+sqj9KcleS\nF+2QIw7rzcc/SfL6qjoqyf9Ncs6C8m2345NcWFW7MvtP5Nu7+zer6g8zO2X0vZldsfgPFxlym6w3\nF/uSPDDJpdMZtMu6+6ULzLld1pyPBWdalPV+Nx6Qnfk8ut58fDo783l0XT4BAgBgYE6zAgAMTJkD\nABiYMgcAMDBlDgBgYMocAMDAlDlgx6mqz07fd1fVdx3h+37VquX/fSTvH2A1ZQ7YyXYnOaQyN723\n1UYOKnPd/bcPMRPAIVHmgJ3sNZl9gPkHq+qHpw/1Pr+qrqiqa6rq+5Kkqp5aVb9fVRcl+dC07jeq\n6sqquq6qzpnWvSbJg6f7e8u07sBRwJru+4+q6tqqeu6K+/6dqvrVqvpwVb1lp3zuJnBk+AQIYCc7\nL8k/7+5nJclUym7v7q+vqgcmeV9V/dY09rQkX9PdfzItf093f2r6mKErqurXuvu8qnrZ9MHgq31n\nkicm+drMPlfziqr6vWnbk5J8dWYfU/S+zD5B5A+O/I8LfCFyZA7gPs9I8sLpI9guT/KlSU6Ztr1/\nRZFLkn9WVVcnuSzJSSvGreebkry1u+/p7k8k+d0kX7/ivm/q7nuTfDCz078Ac3FkDuA+leQHuvuS\ng1ZWPTXJX65afnqSb+zuO6vqd5I86DAe969W3L4nnpuBQ+DIHLCTfSbJQ1csX5Lk+6vq6CSpqq+s\nqi9eY7/wBVWhAAAAp0lEQVQvSfJ/piL3VUmevGLb5w/sv8rvJ3nu9Lq845J8S5L3H5GfAtjR/O8P\n2MmuSXLPdLr0vyZ5fWanOK+aLkLYn+Q5a+z3v5K8tKquT3JDZqdaD7ggyTVVdVV3/6MV6389yTcm\nuTpJJ3lFd982lUGA+626e9EZAAC4n5xmBQAYmDIHADAwZQ4AYGDKHADAwJQ5AICBKXMAAANT5gAA\nBqbMAQAM7P8BoJ56JGsyFYkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd4b43cd910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "# learning_rate = 1e-3\n",
    "# weight_scale = 1e-5\n",
    "# # hidden layer = 4  lr = 1e-6\n",
    "# # hidden layer = 5 lr = 3e-8\n",
    "# #              = 6  lr = 1e-8  w = 1\n",
    "# #             = 7 lr = 3e-9  weight = 5\n",
    "learning_rate = 3e-8\n",
    "weight_scale = 5\n",
    "model = FullyConnectedNet([100, 100, 100, 100,100],\n",
    "                weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inline question: \n",
    "Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net?\n",
    "\n",
    "# Answer:\n",
    "[when the number of layers increase, the learing rate will be decrease about 1e-2, the weight will be bigger]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochstic gradient descent.\n",
    "\n",
    "Open the file `cs231n/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  8.88234703351e-09\n",
      "velocity error:  4.26928774328e-09\n"
     ]
    }
   ],
   "source": [
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print 'next_w error: ', rel_error(next_w, expected_next_w)\n",
    "print 'velocity error: ', rel_error(expected_velocity, config['velocity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with  sgd\n",
      "(Iteration 1 / 200) loss: inf\n",
      "(Epoch 0 / 5) train acc: 0.094000; val_acc: 0.105000\n",
      "(Iteration 11 / 200) loss: nan\n",
      "(Iteration 21 / 200) loss: nan\n",
      "(Iteration 31 / 200) loss: nan\n",
      "(Epoch 1 / 5) train acc: 0.103000; val_acc: 0.087000\n",
      "(Iteration 41 / 200) loss: nan\n",
      "(Iteration 51 / 200) loss: nan\n",
      "(Iteration 61 / 200) loss: nan\n",
      "(Iteration 71 / 200) loss: nan\n",
      "(Epoch 2 / 5) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 81 / 200) loss: nan\n",
      "(Iteration 91 / 200) loss: nan\n",
      "(Iteration 101 / 200) loss: nan\n",
      "(Iteration 111 / 200) loss: nan\n",
      "(Epoch 3 / 5) train acc: 0.104000; val_acc: 0.087000\n",
      "(Iteration 121 / 200) loss: nan\n",
      "(Iteration 131 / 200) loss: nan\n",
      "(Iteration 141 / 200) loss: nan\n",
      "(Iteration 151 / 200) loss: nan\n",
      "(Epoch 4 / 5) train acc: 0.089000; val_acc: 0.087000\n",
      "(Iteration 161 / 200) loss: nan\n",
      "(Iteration 171 / 200) loss: nan\n",
      "(Iteration 181 / 200) loss: nan\n",
      "(Iteration 191 / 200) loss: nan\n",
      "(Epoch 5 / 5) train acc: 0.091000; val_acc: 0.087000\n",
      "\n",
      "running with  sgd_momentum\n",
      "(Iteration 1 / 200) loss: inf\n",
      "(Epoch 0 / 5) train acc: 0.111000; val_acc: 0.105000\n",
      "(Iteration 11 / 200) loss: nan\n",
      "(Iteration 21 / 200) loss: nan\n",
      "(Iteration 31 / 200) loss: nan\n",
      "(Epoch 1 / 5) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 41 / 200) loss: nan\n",
      "(Iteration 51 / 200) loss: nan\n",
      "(Iteration 61 / 200) loss: nan\n",
      "(Iteration 71 / 200) loss: nan\n",
      "(Epoch 2 / 5) train acc: 0.101000; val_acc: 0.087000\n",
      "(Iteration 81 / 200) loss: nan\n",
      "(Iteration 91 / 200) loss: nan\n",
      "(Iteration 101 / 200) loss: nan\n",
      "(Iteration 111 / 200) loss: nan\n",
      "(Epoch 3 / 5) train acc: 0.100000; val_acc: 0.087000\n",
      "(Iteration 121 / 200) loss: nan\n",
      "(Iteration 131 / 200) loss: nan\n",
      "(Iteration 141 / 200) loss: nan\n",
      "(Iteration 151 / 200) loss: nan\n",
      "(Epoch 4 / 5) train acc: 0.095000; val_acc: 0.087000\n",
      "(Iteration 161 / 200) loss: nan\n",
      "(Iteration 171 / 200) loss: nan\n",
      "(Iteration 181 / 200) loss: nan\n",
      "(Iteration 191 / 200) loss: nan\n",
      "(Epoch 5 / 5) train acc: 0.099000; val_acc: 0.087000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4EAAANsCAYAAADlcK2QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd4b448cfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print 'running with ', update_rule\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-2,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.iteritems():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  9.52468751104e-08\n",
      "cache error:  2.64779558072e-09\n"
     ]
    }
   ],
   "source": [
    "# Test RMSProp implementation; you should see errors less than 1e-7\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "print 'next_w error: ', rel_error(expected_next_w, next_w)\n",
    "print 'cache error: ', rel_error(expected_cache, config['cache'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  0.207207038071\n",
      "v error:  4.20831403811e-09\n",
      "m error:  4.21496319311e-09\n"
     ]
    }
   ],
   "source": [
    "# Test Adam implementation; you should see errors around 1e-7 or less\n",
    "from cs231n.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print 'next_w error: ', rel_error(expected_next_w, next_w)\n",
    "print 'v error: ', rel_error(expected_v, config['v'])\n",
    "print 'm error: ', rel_error(expected_m, config['m'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with  adam\n",
      "(Iteration 1 / 200) loss: inf\n",
      "(Epoch 0 / 5) train acc: 0.095000; val_acc: 0.117000\n",
      "(Iteration 11 / 200) loss: inf\n",
      "(Iteration 21 / 200) loss: inf\n",
      "(Iteration 31 / 200) loss: inf\n",
      "(Epoch 1 / 5) train acc: 0.253000; val_acc: 0.179000\n",
      "(Iteration 41 / 200) loss: inf\n",
      "(Iteration 51 / 200) loss: inf\n",
      "(Iteration 61 / 200) loss: inf\n",
      "(Iteration 71 / 200) loss: inf\n",
      "(Epoch 2 / 5) train acc: 0.299000; val_acc: 0.170000\n",
      "(Iteration 81 / 200) loss: inf\n",
      "(Iteration 91 / 200) loss: inf\n",
      "(Iteration 101 / 200) loss: inf\n",
      "(Iteration 111 / 200) loss: inf\n",
      "(Epoch 3 / 5) train acc: 0.319000; val_acc: 0.195000\n",
      "(Iteration 121 / 200) loss: inf\n",
      "(Iteration 131 / 200) loss: inf\n",
      "(Iteration 141 / 200) loss: inf\n",
      "(Iteration 151 / 200) loss: inf\n",
      "(Epoch 4 / 5) train acc: 0.354000; val_acc: 0.182000\n",
      "(Iteration 161 / 200) loss: inf\n",
      "(Iteration 171 / 200) loss: inf\n",
      "(Iteration 181 / 200) loss: inf\n",
      "(Iteration 191 / 200) loss: inf\n",
      "(Epoch 5 / 5) train acc: 0.400000; val_acc: 0.168000\n",
      "\n",
      "running with  rmsprop\n",
      "(Iteration 1 / 200) loss: inf\n",
      "(Epoch 0 / 5) train acc: 0.091000; val_acc: 0.101000\n",
      "(Iteration 11 / 200) loss: inf\n",
      "(Iteration 21 / 200) loss: inf\n",
      "(Iteration 31 / 200) loss: inf\n",
      "(Epoch 1 / 5) train acc: 0.116000; val_acc: 0.099000\n",
      "(Iteration 41 / 200) loss: inf\n",
      "(Iteration 51 / 200) loss: inf\n",
      "(Iteration 61 / 200) loss: inf\n",
      "(Iteration 71 / 200) loss: inf\n",
      "(Epoch 2 / 5) train acc: 0.121000; val_acc: 0.111000\n",
      "(Iteration 81 / 200) loss: inf\n",
      "(Iteration 91 / 200) loss: inf\n",
      "(Iteration 101 / 200) loss: inf\n",
      "(Iteration 111 / 200) loss: inf\n",
      "(Epoch 3 / 5) train acc: 0.127000; val_acc: 0.123000\n",
      "(Iteration 121 / 200) loss: inf\n",
      "(Iteration 131 / 200) loss: inf\n",
      "(Iteration 141 / 200) loss: inf\n",
      "(Iteration 151 / 200) loss: inf\n",
      "(Epoch 4 / 5) train acc: 0.124000; val_acc: 0.129000\n",
      "(Iteration 161 / 200) loss: inf\n",
      "(Iteration 171 / 200) loss: inf\n",
      "(Iteration 181 / 200) loss: inf\n",
      "(Iteration 191 / 200) loss: inf\n",
      "(Epoch 5 / 5) train acc: 0.140000; val_acc: 0.137000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAANsCAYAAAAAwqq5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBlw+A62nowAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd4b44365d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "  print 'running with ', update_rule\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.iteritems():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a good model!\n",
    "Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully-connected net.\n",
    "\n",
    "If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the assignment we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional nets rather than fully-connected nets.\n",
    "\n",
    "You might find it useful to complete the `BatchNormalization.ipynb` and `Dropout.ipynb` notebooks before completing this part, since those techniques can help you train powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# batch normalization and dropout useful. Store your best model in the         #\n",
    "# best_model variable.                                                         #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test you model\n",
    "Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(X_test), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(X_val), axis=1)\n",
    "print 'Validation set accuracy: ', (y_val_pred == y_val).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == y_test).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
